Mon Nov  1 08:46:30 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   61C    P0   195W / 250W |  16902MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   24C    P0    23W / 250W |     12MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   58C    P0   142W / 250W |  18942MiB / 32480MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   58C    P0   163W / 250W |  25198MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   68C    P0   181W / 250W |  21064MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   41C    P0    41W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    2     38294      C   python                                     16891MiB |
|    4     27065      C   python                                     18931MiB |
|    5     43157      C   python                                      2909MiB |
|    5     49354      C   python                                     18931MiB |
|    5     53019      C   python                                      3347MiB |
|    6     47790      C   python                                     21053MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u18112042/CorrSTN/configurations/PEMS08.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 8
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
load file: /data/home/u18112042/CorrSTN/data/PEMS08/PEMS08_r1_d0_w0.npz
ori length: 10699 , percent: 1.0 , scale: 10699
train: torch.Size([10699, 170, 1, 12]) torch.Size([10699, 170, 12]) torch.Size([10699, 170, 12])
val: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
test: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
TemporalPositionalEncoding max_len: 12
w_index: []
d_index: []
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([170, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([170, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 463721
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 7, loss: 0.41
validation cost time: 90.7513s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:157.89s
epoch: 0, total time:248.66s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.3985s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:157.78s
epoch: 1, total time:496.85s
validation batch 1 / 7, loss: 0.08
validation cost time: 90.4137s
epoch: 2, train time every whole data:157.87s
epoch: 2, total time:745.14s
validation batch 1 / 7, loss: 0.12
validation cost time: 90.4265s
epoch: 3, train time every whole data:157.81s
epoch: 3, total time:993.38s
validation batch 1 / 7, loss: 0.10
validation cost time: 90.4140s
epoch: 4, train time every whole data:157.87s
epoch: 4, total time:1241.66s
validation batch 1 / 7, loss: 0.19
validation cost time: 90.4577s
epoch: 5, train time every whole data:157.80s
epoch: 5, total time:1489.92s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4212s
epoch: 6, train time every whole data:157.83s
epoch: 6, total time:1738.18s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4301s
epoch: 7, train time every whole data:157.79s
epoch: 7, total time:1986.40s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4047s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_8.params
epoch: 8, train time every whole data:157.87s
epoch: 8, total time:2234.69s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4666s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_9.params
epoch: 9, train time every whole data:157.82s
epoch: 9, total time:2482.99s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.3831s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_10.params
epoch: 10, train time every whole data:157.87s
epoch: 10, total time:2731.25s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4194s
epoch: 11, train time every whole data:157.86s
epoch: 11, total time:2979.53s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4273s
epoch: 12, train time every whole data:157.86s
epoch: 12, total time:3227.82s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4077s
epoch: 13, train time every whole data:157.81s
epoch: 13, total time:3476.04s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.3975s
epoch: 14, train time every whole data:157.83s
epoch: 14, total time:3724.27s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4370s
epoch: 15, train time every whole data:157.85s
epoch: 15, total time:3972.56s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4059s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_16.params
epoch: 16, train time every whole data:157.85s
epoch: 16, total time:4220.83s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4800s
epoch: 17, train time every whole data:157.75s
epoch: 17, total time:4469.06s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4173s
epoch: 18, train time every whole data:157.79s
epoch: 18, total time:4717.26s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4267s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_19.params
epoch: 19, train time every whole data:157.79s
epoch: 19, total time:4965.50s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4334s
epoch: 20, train time every whole data:157.80s
epoch: 20, total time:5213.74s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4324s
epoch: 21, train time every whole data:157.84s
epoch: 21, total time:5462.01s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4258s
epoch: 22, train time every whole data:157.85s
epoch: 22, total time:5710.29s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4551s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_23.params
epoch: 23, train time every whole data:157.95s
epoch: 23, total time:5958.70s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4440s
epoch: 24, train time every whole data:157.85s
epoch: 24, total time:6207.00s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4284s
epoch: 25, train time every whole data:157.88s
epoch: 25, total time:6455.31s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4572s
epoch: 26, train time every whole data:157.84s
epoch: 26, total time:6703.61s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4518s
epoch: 27, train time every whole data:157.86s
epoch: 27, total time:6951.92s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.3944s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_28.params
epoch: 28, train time every whole data:157.84s
epoch: 28, total time:7200.17s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4264s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_29.params
epoch: 29, train time every whole data:157.86s
epoch: 29, total time:7448.47s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4234s
epoch: 30, train time every whole data:157.80s
epoch: 30, total time:7696.70s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4131s
epoch: 31, train time every whole data:157.83s
epoch: 31, total time:7944.94s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4508s
epoch: 32, train time every whole data:157.84s
epoch: 32, total time:8193.24s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4388s
epoch: 33, train time every whole data:157.77s
epoch: 33, total time:8441.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4080s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_34.params
epoch: 34, train time every whole data:157.82s
epoch: 34, total time:8689.68s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4681s
epoch: 35, train time every whole data:157.79s
epoch: 35, total time:8937.94s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4192s
epoch: 36, train time every whole data:157.81s
epoch: 36, total time:9186.18s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4127s
epoch: 37, train time every whole data:157.71s
epoch: 37, total time:9434.30s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4366s
epoch: 38, train time every whole data:157.77s
epoch: 38, total time:9682.51s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4321s
epoch: 39, train time every whole data:157.73s
epoch: 39, total time:9930.68s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4070s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_40.params
epoch: 40, train time every whole data:157.75s
epoch: 40, total time:10178.84s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4472s
epoch: 41, train time every whole data:157.66s
epoch: 41, total time:10426.95s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4182s
epoch: 42, train time every whole data:157.71s
epoch: 42, total time:10675.08s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4341s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_43.params
epoch: 43, train time every whole data:157.69s
epoch: 43, total time:10923.21s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4799s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_44.params
epoch: 44, train time every whole data:157.73s
epoch: 44, total time:11171.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4277s
epoch: 45, train time every whole data:157.63s
epoch: 45, total time:11419.50s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.3941s
epoch: 46, train time every whole data:157.71s
epoch: 46, total time:11667.60s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4269s
epoch: 47, train time every whole data:157.84s
epoch: 47, total time:11915.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4177s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_48.params
epoch: 48, train time every whole data:157.67s
epoch: 48, total time:12163.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4159s
epoch: 49, train time every whole data:157.89s
epoch: 49, total time:12412.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4319s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_50.params
epoch: 50, train time every whole data:157.68s
epoch: 50, total time:12660.42s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4163s
epoch: 51, train time every whole data:157.69s
epoch: 51, total time:12908.52s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.3909s
epoch: 52, train time every whole data:157.74s
epoch: 52, total time:13156.66s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4233s
epoch: 53, train time every whole data:157.87s
epoch: 53, total time:13404.95s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4515s
epoch: 54, train time every whole data:157.77s
epoch: 54, total time:13653.18s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4263s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_55.params
epoch: 55, train time every whole data:157.76s
epoch: 55, total time:13901.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4374s
epoch: 56, train time every whole data:157.64s
epoch: 56, total time:14149.47s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4153s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_57.params
epoch: 57, train time every whole data:157.74s
epoch: 57, total time:14397.64s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4056s
epoch: 58, train time every whole data:157.79s
epoch: 58, total time:14645.83s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4290s
epoch: 59, train time every whole data:157.74s
epoch: 59, total time:14894.01s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4233s
epoch: 60, train time every whole data:157.75s
epoch: 60, total time:15142.18s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4403s
epoch: 61, train time every whole data:157.81s
epoch: 61, total time:15390.43s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5103s
epoch: 62, train time every whole data:157.68s
epoch: 62, total time:15638.62s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4147s
epoch: 63, train time every whole data:157.75s
epoch: 63, total time:15886.79s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4030s
epoch: 64, train time every whole data:157.77s
epoch: 64, total time:16134.96s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5187s
epoch: 65, train time every whole data:157.80s
epoch: 65, total time:16383.28s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4277s
epoch: 66, train time every whole data:157.74s
epoch: 66, total time:16631.45s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4050s
epoch: 67, train time every whole data:157.85s
epoch: 67, total time:16879.70s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5379s
epoch: 68, train time every whole data:157.71s
epoch: 68, total time:17127.95s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4118s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_69.params
epoch: 69, train time every whole data:157.83s
epoch: 69, total time:17376.21s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4220s
epoch: 70, train time every whole data:157.86s
epoch: 70, total time:17624.50s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4658s
epoch: 71, train time every whole data:157.80s
epoch: 71, total time:17872.76s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4706s
epoch: 72, train time every whole data:157.78s
epoch: 72, total time:18121.01s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4092s
epoch: 73, train time every whole data:157.81s
epoch: 73, total time:18369.23s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4527s
epoch: 74, train time every whole data:157.74s
epoch: 74, total time:18617.42s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4227s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_75.params
epoch: 75, train time every whole data:157.71s
epoch: 75, total time:18865.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4165s
epoch: 76, train time every whole data:157.82s
epoch: 76, total time:19113.81s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5117s
epoch: 77, train time every whole data:157.74s
epoch: 77, total time:19362.07s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4447s
epoch: 78, train time every whole data:157.77s
epoch: 78, total time:19610.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4485s
epoch: 79, train time every whole data:157.83s
epoch: 79, total time:19858.58s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4788s
epoch: 80, train time every whole data:157.82s
epoch: 80, total time:20106.88s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4916s
epoch: 81, train time every whole data:157.79s
epoch: 81, total time:20355.16s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4940s
epoch: 82, train time every whole data:157.75s
epoch: 82, total time:20603.41s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4223s
epoch: 83, train time every whole data:157.76s
epoch: 83, total time:20851.60s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4231s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_84.params
epoch: 84, train time every whole data:157.78s
epoch: 84, total time:21099.81s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4074s
epoch: 85, train time every whole data:157.77s
epoch: 85, total time:21347.99s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4466s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_86.params
epoch: 86, train time every whole data:157.86s
epoch: 86, total time:21596.31s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4174s
epoch: 87, train time every whole data:157.81s
epoch: 87, total time:21844.54s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.3944s
epoch: 88, train time every whole data:157.90s
epoch: 88, total time:22092.84s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4232s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_89.params
epoch: 89, train time every whole data:157.69s
epoch: 89, total time:22340.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4302s
epoch: 90, train time every whole data:157.56s
epoch: 90, total time:22588.95s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4091s
epoch: 91, train time every whole data:157.53s
epoch: 91, total time:22836.89s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4376s
epoch: 92, train time every whole data:157.54s
epoch: 92, total time:23084.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4605s
epoch: 93, train time every whole data:157.52s
epoch: 93, total time:23332.86s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.3935s
epoch: 94, train time every whole data:157.59s
epoch: 94, total time:23580.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4246s
epoch: 95, train time every whole data:157.49s
epoch: 95, total time:23828.77s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5211s
epoch: 96, train time every whole data:157.56s
epoch: 96, total time:24076.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4579s
epoch: 97, train time every whole data:157.54s
epoch: 97, total time:24324.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4879s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_98.params
epoch: 98, train time every whole data:157.54s
epoch: 98, total time:24572.89s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4166s
epoch: 99, train time every whole data:157.51s
epoch: 99, total time:24820.83s
best epoch: 98
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_98.params
predicting testing set batch 1 / 7, time: 12.98s
test time on whole data:90.47s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 98, predict 0 points
MAE: 12.09
RMSE: 19.30
MAPE: 8.03
current epoch: 98, predict 1 points
MAE: 13.10
RMSE: 20.97
MAPE: 8.67
current epoch: 98, predict 2 points
MAE: 13.78
RMSE: 22.08
MAPE: 8.95
current epoch: 98, predict 3 points
MAE: 14.24
RMSE: 22.90
MAPE: 9.21
current epoch: 98, predict 4 points
MAE: 14.64
RMSE: 23.60
MAPE: 9.48
current epoch: 98, predict 5 points
MAE: 15.04
RMSE: 24.21
MAPE: 9.75
current epoch: 98, predict 6 points
MAE: 15.44
RMSE: 24.82
MAPE: 9.98
current epoch: 98, predict 7 points
MAE: 15.82
RMSE: 25.37
MAPE: 10.22
current epoch: 98, predict 8 points
MAE: 16.11
RMSE: 25.82
MAPE: 10.45
current epoch: 98, predict 9 points
MAE: 16.44
RMSE: 26.30
MAPE: 10.67
current epoch: 98, predict 10 points
MAE: 16.81
RMSE: 26.84
MAPE: 10.96
current epoch: 98, predict 11 points
MAE: 17.22
RMSE: 27.47
MAPE: 11.29
all MAE: 15.06
all RMSE: 24.26
all MAPE: 9.80
[12.087601, 19.296862348173992, 8.0283984541893, 13.09875, 20.973903608254698, 8.670686930418015, 13.78183, 22.084038873114192, 8.954852819442749, 14.241535, 22.89774713220645, 9.206025302410126, 14.636549, 23.59927372218992, 9.477133303880692, 15.038171, 24.21359194064294, 9.745597094297409, 15.437593, 24.822146906558693, 9.977013617753983, 15.815361, 25.370445105738316, 10.216633975505829, 16.109058, 25.81791427024315, 10.449157655239105, 16.440962, 26.30144698075089, 10.667022317647934, 16.808548, 26.837156540725513, 10.958165675401688, 17.216671, 27.468983307456295, 11.294004321098328, 15.059381, 24.256301533055282, 9.80372503399849]
fine tune the model ... 
epoch: 100, train time every whole data:414.66s
epoch: 100, total time:25326.79s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4305s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_100.params
epoch: 101, train time every whole data:414.64s
epoch: 101, total time:25831.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4373s
epoch: 102, train time every whole data:414.56s
epoch: 102, total time:26336.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4408s
epoch: 103, train time every whole data:414.61s
epoch: 103, total time:26841.92s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4344s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_103.params
epoch: 104, train time every whole data:414.58s
epoch: 104, total time:27346.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4360s
epoch: 105, train time every whole data:414.59s
epoch: 105, total time:27851.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5239s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_105.params
epoch: 106, train time every whole data:414.60s
epoch: 106, total time:28357.12s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4778s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_106.params
epoch: 107, train time every whole data:414.59s
epoch: 107, total time:28862.20s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4377s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_107.params
epoch: 108, train time every whole data:414.62s
epoch: 108, total time:29367.27s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4290s
epoch: 109, train time every whole data:414.59s
epoch: 109, total time:29872.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4362s
epoch: 110, train time every whole data:414.64s
epoch: 110, total time:30377.37s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4354s
epoch: 111, train time every whole data:414.64s
epoch: 111, total time:30882.46s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5461s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_111.params
epoch: 112, train time every whole data:414.72s
epoch: 112, total time:31387.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4461s
epoch: 113, train time every whole data:414.60s
epoch: 113, total time:31892.78s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4565s
epoch: 114, train time every whole data:414.60s
epoch: 114, total time:32397.83s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4418s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_114.params
epoch: 115, train time every whole data:414.72s
epoch: 115, total time:32903.01s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4414s
epoch: 116, train time every whole data:414.69s
epoch: 116, total time:33408.14s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4550s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_116.params
epoch: 117, train time every whole data:414.61s
epoch: 117, total time:33913.22s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4947s
epoch: 118, train time every whole data:414.63s
epoch: 118, total time:34418.35s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4482s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_118.params
epoch: 119, train time every whole data:414.65s
epoch: 119, total time:34923.46s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4737s
epoch: 120, train time every whole data:414.64s
epoch: 120, total time:35428.58s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5664s
epoch: 121, train time every whole data:414.76s
epoch: 121, total time:35933.91s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4318s
epoch: 122, train time every whole data:414.66s
epoch: 122, total time:36439.00s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5714s
epoch: 123, train time every whole data:414.63s
epoch: 123, total time:36944.21s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5775s
epoch: 124, train time every whole data:414.73s
epoch: 124, total time:37449.51s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5454s
epoch: 125, train time every whole data:414.73s
epoch: 125, total time:37954.79s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4832s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_125.params
epoch: 126, train time every whole data:414.75s
epoch: 126, total time:38460.04s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5366s
epoch: 127, train time every whole data:414.76s
epoch: 127, total time:38965.33s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5857s
epoch: 128, train time every whole data:414.77s
epoch: 128, total time:39470.69s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5758s
epoch: 129, train time every whole data:414.71s
epoch: 129, total time:39975.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4433s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_129.params
epoch: 130, train time every whole data:414.80s
epoch: 130, total time:40481.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4375s
epoch: 131, train time every whole data:414.75s
epoch: 131, total time:40986.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4470s
epoch: 132, train time every whole data:414.83s
epoch: 132, total time:41491.71s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5027s
epoch: 133, train time every whole data:414.81s
epoch: 133, total time:41997.02s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4290s
epoch: 134, train time every whole data:414.80s
epoch: 134, total time:42502.25s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4723s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_134.params
epoch: 135, train time every whole data:414.86s
epoch: 135, total time:43007.60s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4527s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_135.params
epoch: 136, train time every whole data:414.83s
epoch: 136, total time:43512.89s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4964s
epoch: 137, train time every whole data:414.84s
epoch: 137, total time:44018.23s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4317s
epoch: 138, train time every whole data:414.82s
epoch: 138, total time:44523.49s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4641s
epoch: 139, train time every whole data:414.79s
epoch: 139, total time:45028.74s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4500s
epoch: 140, train time every whole data:414.83s
epoch: 140, total time:45534.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4813s
epoch: 141, train time every whole data:414.73s
epoch: 141, total time:46039.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4538s
epoch: 142, train time every whole data:414.83s
epoch: 142, total time:46544.52s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4773s
epoch: 143, train time every whole data:414.77s
epoch: 143, total time:47049.76s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5335s
epoch: 144, train time every whole data:414.75s
epoch: 144, total time:47555.05s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5305s
epoch: 145, train time every whole data:414.78s
epoch: 145, total time:48060.37s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5111s
epoch: 146, train time every whole data:414.74s
epoch: 146, total time:48565.62s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5315s
epoch: 147, train time every whole data:414.80s
epoch: 147, total time:49070.95s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5341s
epoch: 148, train time every whole data:414.75s
epoch: 148, total time:49576.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5297s
epoch: 149, train time every whole data:414.77s
epoch: 149, total time:50081.55s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5072s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_149.params
best epoch: 149
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_149.params
predicting testing set batch 1 / 7, time: 13.00s
test time on whole data:90.60s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 149, predict 0 points
MAE: 11.90
RMSE: 19.24
MAPE: 7.79
current epoch: 149, predict 1 points
MAE: 12.70
RMSE: 20.77
MAPE: 8.37
current epoch: 149, predict 2 points
MAE: 13.24
RMSE: 21.75
MAPE: 8.71
current epoch: 149, predict 3 points
MAE: 13.58
RMSE: 22.42
MAPE: 8.96
current epoch: 149, predict 4 points
MAE: 13.88
RMSE: 23.04
MAPE: 9.14
current epoch: 149, predict 5 points
MAE: 14.15
RMSE: 23.57
MAPE: 9.33
current epoch: 149, predict 6 points
MAE: 14.43
RMSE: 24.05
MAPE: 9.50
current epoch: 149, predict 7 points
MAE: 14.69
RMSE: 24.55
MAPE: 9.65
current epoch: 149, predict 8 points
MAE: 14.90
RMSE: 24.95
MAPE: 9.81
current epoch: 149, predict 9 points
MAE: 15.13
RMSE: 25.34
MAPE: 9.95
current epoch: 149, predict 10 points
MAE: 15.36
RMSE: 25.74
MAPE: 10.08
current epoch: 149, predict 11 points
MAE: 15.69
RMSE: 26.21
MAPE: 10.26
all MAE: 14.14
all RMSE: 23.56
all MAPE: 9.30
[11.904371, 19.24114310883752, 7.793667912483215, 12.696409, 20.769615369751598, 8.36556926369667, 13.239235, 21.75171172323284, 8.70571956038475, 13.577086, 22.42059896739248, 8.960247039794922, 13.877847, 23.043046557018155, 9.143320471048355, 14.149221, 23.574350468252568, 9.328161925077438, 14.426014, 24.051897522109613, 9.503640979528427, 14.687142, 24.546196183325687, 9.651271253824234, 14.904365, 24.947816631671397, 9.814398735761642, 15.128068, 25.344815646908565, 9.950003772974014, 15.363017, 25.73580986057535, 10.075131058692932, 15.6850815, 26.205426255536754, 10.26121899485588, 14.136482, 23.556617872807394, 9.296026080846786]
