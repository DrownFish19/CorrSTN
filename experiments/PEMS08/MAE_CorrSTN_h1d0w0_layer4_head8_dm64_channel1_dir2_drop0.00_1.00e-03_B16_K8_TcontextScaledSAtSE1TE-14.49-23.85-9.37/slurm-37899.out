Mon Nov  1 01:00:28 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   28C    P0    23W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   50C    P0    68W / 250W |   3724MiB / 32480MiB |     46%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   57C    P0   110W / 250W |   3724MiB / 32480MiB |     44%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   61C    P0   105W / 250W |   6564MiB / 32480MiB |     98%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   45C    P0    60W / 250W |   2808MiB / 32480MiB |     46%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   53C    P0    93W / 250W |   2808MiB / 32480MiB |     47%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   30C    P0    24W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   28C    P0    23W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    1     19680      C   python                                      3713MiB |
|    2     23887      C   python                                      3713MiB |
|    3      4214      C   python                                      2909MiB |
|    3      4833      C   python                                      3643MiB |
|    4     23888      C   python                                      2797MiB |
|    5     43157      C   python                                      2797MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u18112042/CorrSTN/configurations/PEMS08.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 8
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
load file: /data/home/u18112042/CorrSTN/data/PEMS08/PEMS08_r1_d0_w0.npz
ori length: 10699 , percent: 1.0 , scale: 10699
train: torch.Size([10699, 170, 1, 12]) torch.Size([10699, 170, 12]) torch.Size([10699, 170, 12])
val: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
test: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
TemporalPositionalEncoding max_len: 12
w_index: []
d_index: []
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([170, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([170, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 463721
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 7, loss: 0.86
validation cost time: 90.8028s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:158.19s
epoch: 0, total time:249.00s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4583s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:158.22s
epoch: 1, total time:497.70s
validation batch 1 / 7, loss: 0.09
validation cost time: 90.4871s
epoch: 2, train time every whole data:158.23s
epoch: 2, total time:746.42s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4837s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_3.params
epoch: 3, train time every whole data:158.22s
epoch: 3, total time:995.13s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4877s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_4.params
epoch: 4, train time every whole data:158.23s
epoch: 4, total time:1243.86s
validation batch 1 / 7, loss: 0.10
validation cost time: 90.5162s
epoch: 5, train time every whole data:158.22s
epoch: 5, total time:1492.60s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5178s
epoch: 6, train time every whole data:158.23s
epoch: 6, total time:1741.35s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4991s
epoch: 7, train time every whole data:158.23s
epoch: 7, total time:1990.08s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4815s
epoch: 8, train time every whole data:158.22s
epoch: 8, total time:2238.79s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4830s
epoch: 9, train time every whole data:158.23s
epoch: 9, total time:2487.51s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4575s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_10.params
epoch: 10, train time every whole data:158.22s
epoch: 10, total time:2736.20s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4719s
epoch: 11, train time every whole data:158.23s
epoch: 11, total time:2984.91s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4827s
epoch: 12, train time every whole data:158.23s
epoch: 12, total time:3233.62s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4612s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_13.params
epoch: 13, train time every whole data:158.24s
epoch: 13, total time:3482.34s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4719s
epoch: 14, train time every whole data:158.24s
epoch: 14, total time:3731.05s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5216s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_15.params
epoch: 15, train time every whole data:158.24s
epoch: 15, total time:3979.83s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4647s
epoch: 16, train time every whole data:158.23s
epoch: 16, total time:4228.52s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4946s
epoch: 17, train time every whole data:158.21s
epoch: 17, total time:4477.23s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4867s
epoch: 18, train time every whole data:158.12s
epoch: 18, total time:4725.84s
validation batch 1 / 7, loss: 0.09
validation cost time: 90.4750s
epoch: 19, train time every whole data:158.12s
epoch: 19, total time:4974.44s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5022s
epoch: 20, train time every whole data:158.12s
epoch: 20, total time:5223.06s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4831s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_21.params
epoch: 21, train time every whole data:158.13s
epoch: 21, total time:5471.68s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4590s
epoch: 22, train time every whole data:158.12s
epoch: 22, total time:5720.27s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5256s
epoch: 23, train time every whole data:158.14s
epoch: 23, total time:5968.93s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4866s
epoch: 24, train time every whole data:158.14s
epoch: 24, total time:6217.56s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4794s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_25.params
epoch: 25, train time every whole data:158.14s
epoch: 25, total time:6466.19s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5044s
epoch: 26, train time every whole data:158.15s
epoch: 26, total time:6714.84s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4863s
epoch: 27, train time every whole data:158.14s
epoch: 27, total time:6963.47s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4621s
epoch: 28, train time every whole data:158.14s
epoch: 28, total time:7212.07s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4949s
epoch: 29, train time every whole data:158.12s
epoch: 29, total time:7460.69s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4863s
epoch: 30, train time every whole data:158.12s
epoch: 30, total time:7709.31s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4757s
epoch: 31, train time every whole data:158.13s
epoch: 31, total time:7957.91s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5062s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_32.params
epoch: 32, train time every whole data:158.12s
epoch: 32, total time:8206.55s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4916s
epoch: 33, train time every whole data:158.13s
epoch: 33, total time:8455.17s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4624s
epoch: 34, train time every whole data:158.14s
epoch: 34, total time:8703.78s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5325s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_35.params
epoch: 35, train time every whole data:158.13s
epoch: 35, total time:8952.45s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4921s
epoch: 36, train time every whole data:158.08s
epoch: 36, total time:9201.02s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4779s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_37.params
epoch: 37, train time every whole data:158.08s
epoch: 37, total time:9449.60s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5025s
epoch: 38, train time every whole data:158.06s
epoch: 38, total time:9698.17s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4885s
epoch: 39, train time every whole data:158.04s
epoch: 39, total time:9946.70s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4599s
epoch: 40, train time every whole data:158.09s
epoch: 40, total time:10195.26s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4913s
epoch: 41, train time every whole data:158.05s
epoch: 41, total time:10443.80s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4872s
epoch: 42, train time every whole data:158.04s
epoch: 42, total time:10692.33s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4768s
epoch: 43, train time every whole data:158.09s
epoch: 43, total time:10940.90s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5015s
epoch: 44, train time every whole data:158.07s
epoch: 44, total time:11189.47s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4876s
epoch: 45, train time every whole data:158.04s
epoch: 45, total time:11438.00s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4615s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_46.params
epoch: 46, train time every whole data:158.10s
epoch: 46, total time:11686.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4944s
epoch: 47, train time every whole data:158.06s
epoch: 47, total time:11935.13s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4838s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_48.params
epoch: 48, train time every whole data:158.02s
epoch: 48, total time:12183.65s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4814s
epoch: 49, train time every whole data:158.08s
epoch: 49, total time:12432.21s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5362s
epoch: 50, train time every whole data:158.09s
epoch: 50, total time:12680.84s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4910s
epoch: 51, train time every whole data:158.04s
epoch: 51, total time:12929.37s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4625s
epoch: 52, train time every whole data:158.08s
epoch: 52, total time:13177.91s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4931s
epoch: 53, train time every whole data:158.04s
epoch: 53, total time:13426.45s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4867s
epoch: 54, train time every whole data:158.02s
epoch: 54, total time:13674.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4762s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_55.params
epoch: 55, train time every whole data:158.09s
epoch: 55, total time:13923.53s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4997s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_56.params
epoch: 56, train time every whole data:158.07s
epoch: 56, total time:14172.11s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4863s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_57.params
epoch: 57, train time every whole data:158.02s
epoch: 57, total time:14420.67s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4615s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_58.params
epoch: 58, train time every whole data:158.12s
epoch: 58, total time:14669.27s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4850s
epoch: 59, train time every whole data:157.97s
epoch: 59, total time:14917.73s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4789s
epoch: 60, train time every whole data:157.96s
epoch: 60, total time:15166.16s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4729s
epoch: 61, train time every whole data:157.97s
epoch: 61, total time:15414.61s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4999s
epoch: 62, train time every whole data:158.02s
epoch: 62, total time:15663.13s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4864s
epoch: 63, train time every whole data:157.92s
epoch: 63, total time:15911.53s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4577s
epoch: 64, train time every whole data:158.02s
epoch: 64, total time:16160.01s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4893s
epoch: 65, train time every whole data:157.95s
epoch: 65, total time:16408.46s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4841s
epoch: 66, train time every whole data:157.97s
epoch: 66, total time:16656.91s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4730s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_67.params
epoch: 67, train time every whole data:157.98s
epoch: 67, total time:16905.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4941s
epoch: 68, train time every whole data:158.02s
epoch: 68, total time:17153.89s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4840s
epoch: 69, train time every whole data:157.94s
epoch: 69, total time:17402.32s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4553s
epoch: 70, train time every whole data:158.00s
epoch: 70, total time:17650.78s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5224s
epoch: 71, train time every whole data:158.00s
epoch: 71, total time:17899.30s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4810s
epoch: 72, train time every whole data:157.97s
epoch: 72, total time:18147.75s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4772s
epoch: 73, train time every whole data:157.98s
epoch: 73, total time:18396.21s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4967s
epoch: 74, train time every whole data:158.02s
epoch: 74, total time:18644.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4842s
epoch: 75, train time every whole data:157.97s
epoch: 75, total time:18893.19s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4539s
epoch: 76, train time every whole data:158.00s
epoch: 76, total time:19141.65s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4872s
epoch: 77, train time every whole data:158.00s
epoch: 77, total time:19390.13s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4814s
epoch: 78, train time every whole data:157.95s
epoch: 78, total time:19638.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4729s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_79.params
epoch: 79, train time every whole data:158.00s
epoch: 79, total time:19887.05s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4970s
epoch: 80, train time every whole data:157.98s
epoch: 80, total time:20135.53s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4815s
epoch: 81, train time every whole data:157.98s
epoch: 81, total time:20383.99s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4584s
epoch: 82, train time every whole data:157.94s
epoch: 82, total time:20632.39s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4914s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_83.params
epoch: 83, train time every whole data:158.03s
epoch: 83, total time:20880.93s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4828s
epoch: 84, train time every whole data:157.96s
epoch: 84, total time:21129.37s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4729s
epoch: 85, train time every whole data:157.99s
epoch: 85, total time:21377.84s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5011s
epoch: 86, train time every whole data:157.99s
epoch: 86, total time:21626.34s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4822s
epoch: 87, train time every whole data:158.00s
epoch: 87, total time:21874.83s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4896s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_88.params
epoch: 88, train time every whole data:157.96s
epoch: 88, total time:22123.28s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4875s
epoch: 89, train time every whole data:158.02s
epoch: 89, total time:22371.79s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4845s
epoch: 90, train time every whole data:157.96s
epoch: 90, total time:22620.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4742s
epoch: 91, train time every whole data:157.97s
epoch: 91, total time:22868.68s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4996s
epoch: 92, train time every whole data:158.00s
epoch: 92, total time:23117.19s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4818s
epoch: 93, train time every whole data:157.99s
epoch: 93, total time:23365.66s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4602s
epoch: 94, train time every whole data:157.97s
epoch: 94, total time:23614.09s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4857s
epoch: 95, train time every whole data:157.98s
epoch: 95, total time:23862.56s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5166s
epoch: 96, train time every whole data:158.01s
epoch: 96, total time:24111.09s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4721s
epoch: 97, train time every whole data:157.93s
epoch: 97, total time:24359.50s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5014s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_98.params
epoch: 98, train time every whole data:158.02s
epoch: 98, total time:24608.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4784s
epoch: 99, train time every whole data:157.97s
epoch: 99, total time:24856.48s
best epoch: 98
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_98.params
predicting testing set batch 1 / 7, time: 12.99s
test time on whole data:90.49s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 98, predict 0 points
MAE: 12.17
RMSE: 19.40
MAPE: 7.92
current epoch: 98, predict 1 points
MAE: 13.13
RMSE: 21.08
MAPE: 8.57
current epoch: 98, predict 2 points
MAE: 13.81
RMSE: 22.14
MAPE: 9.00
current epoch: 98, predict 3 points
MAE: 14.28
RMSE: 22.97
MAPE: 9.38
current epoch: 98, predict 4 points
MAE: 14.64
RMSE: 23.61
MAPE: 9.62
current epoch: 98, predict 5 points
MAE: 15.03
RMSE: 24.27
MAPE: 9.88
current epoch: 98, predict 6 points
MAE: 15.45
RMSE: 24.86
MAPE: 10.15
current epoch: 98, predict 7 points
MAE: 15.78
RMSE: 25.37
MAPE: 10.36
current epoch: 98, predict 8 points
MAE: 16.05
RMSE: 25.80
MAPE: 10.52
current epoch: 98, predict 9 points
MAE: 16.31
RMSE: 26.18
MAPE: 10.65
current epoch: 98, predict 10 points
MAE: 16.56
RMSE: 26.58
MAPE: 10.79
current epoch: 98, predict 11 points
MAE: 16.88
RMSE: 27.05
MAPE: 11.00
all MAE: 15.01
all RMSE: 24.21
all MAPE: 9.82
[12.172063, 19.402046866437104, 7.917851954698563, 13.130213, 21.07778620465305, 8.573479950428009, 13.805437, 22.136938979515946, 9.004861861467361, 14.276118, 22.966849942668727, 9.381359070539474, 14.643001, 23.61380950866926, 9.624245762825012, 15.034896, 24.268219386019418, 9.883523732423782, 15.450849, 24.857792562378076, 10.147273540496826, 15.780682, 25.373084085228854, 10.360269248485565, 16.053556, 25.79517379687405, 10.517966747283936, 16.3132, 26.184121435175772, 10.648523271083832, 16.558836, 26.57980479516768, 10.790316015481949, 16.876965, 27.054077247304612, 11.00347563624382, 15.007988, 24.214159091083076, 9.821093827486038]
fine tune the model ... 
epoch: 100, train time every whole data:415.63s
epoch: 100, total time:25363.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4924s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_100.params
epoch: 101, train time every whole data:415.63s
epoch: 101, total time:25869.58s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5074s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_101.params
epoch: 102, train time every whole data:415.66s
epoch: 102, total time:26375.76s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4806s
epoch: 103, train time every whole data:415.62s
epoch: 103, total time:26881.86s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4917s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_103.params
epoch: 104, train time every whole data:415.60s
epoch: 104, total time:27387.97s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5037s
epoch: 105, train time every whole data:415.61s
epoch: 105, total time:27894.08s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4834s
epoch: 106, train time every whole data:415.56s
epoch: 106, total time:28400.12s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5122s
epoch: 107, train time every whole data:415.54s
epoch: 107, total time:28906.18s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4917s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_107.params
epoch: 108, train time every whole data:415.53s
epoch: 108, total time:29412.21s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4929s
epoch: 109, train time every whole data:415.46s
epoch: 109, total time:29918.17s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4825s
epoch: 110, train time every whole data:415.46s
epoch: 110, total time:30424.11s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4998s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_110.params
epoch: 111, train time every whole data:415.48s
epoch: 111, total time:30930.11s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5012s
epoch: 112, train time every whole data:415.47s
epoch: 112, total time:31436.08s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4785s
epoch: 113, train time every whole data:415.47s
epoch: 113, total time:31942.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4922s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_113.params
epoch: 114, train time every whole data:415.44s
epoch: 114, total time:32447.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5036s
epoch: 115, train time every whole data:415.42s
epoch: 115, total time:32953.91s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4994s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_115.params
epoch: 116, train time every whole data:415.43s
epoch: 116, total time:33459.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4947s
epoch: 117, train time every whole data:415.42s
epoch: 117, total time:33965.77s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4901s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_117.params
epoch: 118, train time every whole data:415.47s
epoch: 118, total time:34471.74s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5234s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_118.params
epoch: 119, train time every whole data:415.45s
epoch: 119, total time:34977.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4758s
epoch: 120, train time every whole data:415.42s
epoch: 120, total time:35483.62s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4831s
epoch: 121, train time every whole data:415.42s
epoch: 121, total time:35989.53s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4899s
epoch: 122, train time every whole data:415.42s
epoch: 122, total time:36495.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4864s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_122.params
epoch: 123, train time every whole data:415.46s
epoch: 123, total time:37001.40s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4829s
epoch: 124, train time every whole data:415.44s
epoch: 124, total time:37507.33s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4996s
epoch: 125, train time every whole data:415.48s
epoch: 125, total time:38013.31s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4853s
epoch: 126, train time every whole data:415.47s
epoch: 126, total time:38519.27s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5071s
epoch: 127, train time every whole data:415.46s
epoch: 127, total time:39025.23s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4890s
epoch: 128, train time every whole data:415.47s
epoch: 128, total time:39531.19s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4964s
epoch: 129, train time every whole data:415.55s
epoch: 129, total time:40037.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5022s
epoch: 130, train time every whole data:415.50s
epoch: 130, total time:40543.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4689s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_130.params
epoch: 131, train time every whole data:415.50s
epoch: 131, total time:41049.22s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4996s
epoch: 132, train time every whole data:415.51s
epoch: 132, total time:41555.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5034s
epoch: 133, train time every whole data:415.48s
epoch: 133, total time:42061.23s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4930s
epoch: 134, train time every whole data:415.52s
epoch: 134, total time:42567.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4938s
epoch: 135, train time every whole data:415.52s
epoch: 135, total time:43073.26s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4760s
epoch: 136, train time every whole data:415.53s
epoch: 136, total time:43579.26s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4821s
epoch: 137, train time every whole data:415.56s
epoch: 137, total time:44085.31s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4934s
epoch: 138, train time every whole data:415.55s
epoch: 138, total time:44591.36s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4932s
epoch: 139, train time every whole data:415.55s
epoch: 139, total time:45097.40s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4879s
epoch: 140, train time every whole data:415.57s
epoch: 140, total time:45603.46s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5080s
epoch: 141, train time every whole data:415.54s
epoch: 141, total time:46109.51s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5037s
epoch: 142, train time every whole data:415.54s
epoch: 142, total time:46615.55s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4797s
epoch: 143, train time every whole data:415.54s
epoch: 143, total time:47121.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4908s
epoch: 144, train time every whole data:415.47s
epoch: 144, total time:47627.54s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5043s
epoch: 145, train time every whole data:415.49s
epoch: 145, total time:48133.53s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4959s
epoch: 146, train time every whole data:415.49s
epoch: 146, total time:48639.52s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4914s
epoch: 147, train time every whole data:415.48s
epoch: 147, total time:49145.49s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4817s
epoch: 148, train time every whole data:415.47s
epoch: 148, total time:49651.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4879s
epoch: 149, train time every whole data:415.49s
epoch: 149, total time:50157.43s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4835s
best epoch: 130
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_130.params
predicting testing set batch 1 / 7, time: 12.99s
test time on whole data:90.52s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 130, predict 0 points
MAE: 12.07
RMSE: 19.32
MAPE: 7.82
current epoch: 130, predict 1 points
MAE: 12.93
RMSE: 20.96
MAPE: 8.38
current epoch: 130, predict 2 points
MAE: 13.52
RMSE: 21.98
MAPE: 8.72
current epoch: 130, predict 3 points
MAE: 13.91
RMSE: 22.79
MAPE: 8.94
current epoch: 130, predict 4 points
MAE: 14.20
RMSE: 23.36
MAPE: 9.15
current epoch: 130, predict 5 points
MAE: 14.50
RMSE: 23.91
MAPE: 9.35
current epoch: 130, predict 6 points
MAE: 14.82
RMSE: 24.42
MAPE: 9.56
current epoch: 130, predict 7 points
MAE: 15.08
RMSE: 24.87
MAPE: 9.75
current epoch: 130, predict 8 points
MAE: 15.33
RMSE: 25.29
MAPE: 9.92
current epoch: 130, predict 9 points
MAE: 15.56
RMSE: 25.65
MAPE: 10.10
current epoch: 130, predict 10 points
MAE: 15.80
RMSE: 26.04
MAPE: 10.27
current epoch: 130, predict 11 points
MAE: 16.16
RMSE: 26.53
MAPE: 10.53
all MAE: 14.49
all RMSE: 23.85
all MAPE: 9.37
[12.068052, 19.320363835607093, 7.818052917718887, 12.932387, 20.955288473085602, 8.376391232013702, 13.523835, 21.981618658276524, 8.71996209025383, 13.909135, 22.787037641445547, 8.94109383225441, 14.201238, 23.362323444774947, 9.148525446653366, 14.499565, 23.90760438361153, 9.353692084550858, 14.815171, 24.41726728965654, 9.558437764644623, 15.083508, 24.869645505678704, 9.754156321287155, 15.329346, 25.290541932320643, 9.923909604549408, 15.556572, 25.653648476888844, 10.103631764650345, 15.804244, 26.041188407066674, 10.273314267396927, 16.15798, 26.5312695542478, 10.528304427862167, 14.490085, 23.85142303635938, 9.374958276748657]
