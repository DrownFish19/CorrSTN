Mon Nov  1 12:01:01 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   58C    P0   158W / 250W |  18942MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   54C    P0   155W / 250W |  18943MiB / 32480MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   55C    P0   162W / 250W |  19012MiB / 32480MiB |     94%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   53C    P0   143W / 250W |  22401MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   63C    P0   154W / 250W |  21064MiB / 32480MiB |     84%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   26C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    2     38294      C   python                                     18931MiB |
|    3     45484      C   python                                     18931MiB |
|    4     27065      C   python                                     19001MiB |
|    5     49354      C   python                                     18931MiB |
|    5     53019      C   python                                      3459MiB |
|    6     47790      C   python                                     21053MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u18112042/CorrSTN/configurations/PEMS08.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 8
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
load file: /data/home/u18112042/CorrSTN/data/PEMS08/PEMS08_r1_d0_w0.npz
ori length: 10699 , percent: 1.0 , scale: 10699
train: torch.Size([10699, 170, 1, 12]) torch.Size([10699, 170, 12]) torch.Size([10699, 170, 12])
val: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
test: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
TemporalPositionalEncoding max_len: 12
w_index: []
d_index: []
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([170, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([170, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 463721
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 7, loss: 0.39
validation cost time: 90.8676s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:158.30s
epoch: 0, total time:249.18s
validation batch 1 / 7, loss: 0.18
validation cost time: 90.4315s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:158.18s
epoch: 1, total time:497.81s
validation batch 1 / 7, loss: 0.11
validation cost time: 90.4374s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_2.params
epoch: 2, train time every whole data:158.29s
epoch: 2, total time:746.55s
validation batch 1 / 7, loss: 0.09
validation cost time: 90.4494s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_3.params
epoch: 3, train time every whole data:158.25s
epoch: 3, total time:995.27s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4779s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_4.params
epoch: 4, train time every whole data:158.43s
epoch: 4, total time:1244.19s
validation batch 1 / 7, loss: 0.10
validation cost time: 90.4520s
epoch: 5, train time every whole data:158.24s
epoch: 5, total time:1492.88s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4675s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_6.params
epoch: 6, train time every whole data:158.26s
epoch: 6, total time:1741.63s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.5034s
epoch: 7, train time every whole data:158.26s
epoch: 7, total time:1990.40s
validation batch 1 / 7, loss: 0.12
validation cost time: 90.5396s
epoch: 8, train time every whole data:158.37s
epoch: 8, total time:2239.31s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4512s
epoch: 9, train time every whole data:158.19s
epoch: 9, total time:2487.96s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4830s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_10.params
epoch: 10, train time every whole data:158.26s
epoch: 10, total time:2736.71s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4566s
epoch: 11, train time every whole data:158.29s
epoch: 11, total time:2985.46s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4863s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_12.params
epoch: 12, train time every whole data:158.28s
epoch: 12, total time:3234.24s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4082s
epoch: 13, train time every whole data:158.25s
epoch: 13, total time:3482.89s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4973s
epoch: 14, train time every whole data:158.29s
epoch: 14, total time:3731.69s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4415s
epoch: 15, train time every whole data:158.26s
epoch: 15, total time:3980.39s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4852s
epoch: 16, train time every whole data:158.27s
epoch: 16, total time:4229.15s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4371s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_17.params
epoch: 17, train time every whole data:158.29s
epoch: 17, total time:4477.89s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.5204s
epoch: 18, train time every whole data:158.25s
epoch: 18, total time:4726.66s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4436s
epoch: 19, train time every whole data:158.27s
epoch: 19, total time:4975.37s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5850s
epoch: 20, train time every whole data:158.30s
epoch: 20, total time:5224.26s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4962s
epoch: 21, train time every whole data:158.27s
epoch: 21, total time:5473.03s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4193s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_22.params
epoch: 22, train time every whole data:158.28s
epoch: 22, total time:5721.75s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5114s
epoch: 23, train time every whole data:158.32s
epoch: 23, total time:5970.58s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4507s
epoch: 24, train time every whole data:158.28s
epoch: 24, total time:6219.32s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4368s
epoch: 25, train time every whole data:158.31s
epoch: 25, total time:6468.07s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5374s
epoch: 26, train time every whole data:158.32s
epoch: 26, total time:6716.93s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4809s
epoch: 27, train time every whole data:158.29s
epoch: 27, total time:6965.70s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4472s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_28.params
epoch: 28, train time every whole data:158.29s
epoch: 28, total time:7214.45s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5071s
epoch: 29, train time every whole data:158.33s
epoch: 29, total time:7463.29s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4335s
epoch: 30, train time every whole data:158.26s
epoch: 30, total time:7711.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4705s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_31.params
epoch: 31, train time every whole data:158.28s
epoch: 31, total time:7960.75s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5050s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_32.params
epoch: 32, train time every whole data:158.30s
epoch: 32, total time:8209.57s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4828s
epoch: 33, train time every whole data:158.29s
epoch: 33, total time:8458.35s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4294s
epoch: 34, train time every whole data:158.23s
epoch: 34, total time:8707.01s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4817s
epoch: 35, train time every whole data:158.31s
epoch: 35, total time:8955.80s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4933s
epoch: 36, train time every whole data:158.23s
epoch: 36, total time:9204.53s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4214s
epoch: 37, train time every whole data:158.28s
epoch: 37, total time:9453.24s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4644s
epoch: 38, train time every whole data:158.25s
epoch: 38, total time:9701.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4977s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_39.params
epoch: 39, train time every whole data:158.30s
epoch: 39, total time:9950.77s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4700s
epoch: 40, train time every whole data:158.23s
epoch: 40, total time:10199.46s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4912s
epoch: 41, train time every whole data:158.31s
epoch: 41, total time:10448.27s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4321s
epoch: 42, train time every whole data:158.18s
epoch: 42, total time:10696.89s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5130s
epoch: 43, train time every whole data:158.28s
epoch: 43, total time:10945.69s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4665s
epoch: 44, train time every whole data:158.27s
epoch: 44, total time:11194.43s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4325s
epoch: 45, train time every whole data:158.28s
epoch: 45, total time:11443.15s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4284s
epoch: 46, train time every whole data:158.26s
epoch: 46, total time:11691.84s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4611s
epoch: 47, train time every whole data:158.31s
epoch: 47, total time:11940.61s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4297s
epoch: 48, train time every whole data:158.25s
epoch: 48, total time:12189.30s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4383s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_49.params
epoch: 49, train time every whole data:158.28s
epoch: 49, total time:12438.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4601s
epoch: 50, train time every whole data:158.28s
epoch: 50, total time:12686.76s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4833s
epoch: 51, train time every whole data:158.26s
epoch: 51, total time:12935.51s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4078s
epoch: 52, train time every whole data:158.24s
epoch: 52, total time:13184.16s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4623s
epoch: 53, train time every whole data:158.30s
epoch: 53, total time:13432.92s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4512s
epoch: 54, train time every whole data:158.20s
epoch: 54, total time:13681.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4409s
epoch: 55, train time every whole data:158.25s
epoch: 55, total time:13930.27s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4557s
epoch: 56, train time every whole data:158.24s
epoch: 56, total time:14178.97s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4381s
epoch: 57, train time every whole data:158.27s
epoch: 57, total time:14427.68s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4221s
epoch: 58, train time every whole data:158.23s
epoch: 58, total time:14676.33s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4544s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_59.params
epoch: 59, train time every whole data:158.30s
epoch: 59, total time:14925.10s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4392s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_60.params
epoch: 60, train time every whole data:158.22s
epoch: 60, total time:15173.77s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4611s
epoch: 61, train time every whole data:158.26s
epoch: 61, total time:15422.49s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4838s
epoch: 62, train time every whole data:158.24s
epoch: 62, total time:15671.22s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4626s
epoch: 63, train time every whole data:158.26s
epoch: 63, total time:15919.94s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4749s
epoch: 64, train time every whole data:158.20s
epoch: 64, total time:16168.62s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4905s
epoch: 65, train time every whole data:158.29s
epoch: 65, total time:16417.40s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4638s
epoch: 66, train time every whole data:158.18s
epoch: 66, total time:16666.05s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4527s
epoch: 67, train time every whole data:158.25s
epoch: 67, total time:16914.75s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4901s
epoch: 68, train time every whole data:158.23s
epoch: 68, total time:17163.48s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4605s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_69.params
epoch: 69, train time every whole data:158.27s
epoch: 69, total time:17412.22s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4754s
epoch: 70, train time every whole data:158.22s
epoch: 70, total time:17660.92s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4507s
epoch: 71, train time every whole data:158.27s
epoch: 71, total time:17909.65s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4515s
epoch: 72, train time every whole data:158.20s
epoch: 72, total time:18158.30s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4610s
epoch: 73, train time every whole data:158.26s
epoch: 73, total time:18407.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4621s
epoch: 74, train time every whole data:158.26s
epoch: 74, total time:18655.75s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4348s
epoch: 75, train time every whole data:158.24s
epoch: 75, total time:18904.43s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4481s
epoch: 76, train time every whole data:158.20s
epoch: 76, total time:19153.08s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4920s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_77.params
epoch: 77, train time every whole data:158.29s
epoch: 77, total time:19401.88s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4498s
epoch: 78, train time every whole data:158.24s
epoch: 78, total time:19650.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4378s
epoch: 79, train time every whole data:158.25s
epoch: 79, total time:19899.26s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4661s
epoch: 80, train time every whole data:158.25s
epoch: 80, total time:20147.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4520s
epoch: 81, train time every whole data:158.29s
epoch: 81, total time:20396.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4248s
epoch: 82, train time every whole data:158.23s
epoch: 82, total time:20645.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4664s
epoch: 83, train time every whole data:158.27s
epoch: 83, total time:20894.11s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4735s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_84.params
epoch: 84, train time every whole data:158.27s
epoch: 84, total time:21142.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4681s
epoch: 85, train time every whole data:158.23s
epoch: 85, total time:21391.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4803s
epoch: 86, train time every whole data:158.28s
epoch: 86, total time:21640.32s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4602s
epoch: 87, train time every whole data:158.26s
epoch: 87, total time:21889.05s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4660s
epoch: 88, train time every whole data:158.24s
epoch: 88, total time:22137.76s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4907s
epoch: 89, train time every whole data:158.26s
epoch: 89, total time:22386.51s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4775s
epoch: 90, train time every whole data:158.27s
epoch: 90, total time:22635.27s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4286s
epoch: 91, train time every whole data:158.22s
epoch: 91, total time:22883.92s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5230s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_92.params
epoch: 92, train time every whole data:158.27s
epoch: 92, total time:23132.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5187s
epoch: 93, train time every whole data:158.25s
epoch: 93, total time:23381.50s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4567s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_94.params
epoch: 94, train time every whole data:158.25s
epoch: 94, total time:23630.22s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4463s
epoch: 95, train time every whole data:158.28s
epoch: 95, total time:23878.94s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4880s
epoch: 96, train time every whole data:158.28s
epoch: 96, total time:24127.71s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4277s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_97.params
epoch: 97, train time every whole data:158.26s
epoch: 97, total time:24376.41s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5293s
epoch: 98, train time every whole data:158.29s
epoch: 98, total time:24625.23s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4514s
epoch: 99, train time every whole data:158.26s
epoch: 99, total time:24873.95s
best epoch: 97
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_97.params
predicting testing set batch 1 / 7, time: 12.98s
test time on whole data:90.45s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 97, predict 0 points
MAE: 12.07
RMSE: 19.33
MAPE: 7.85
current epoch: 97, predict 1 points
MAE: 13.05
RMSE: 20.98
MAPE: 8.46
current epoch: 97, predict 2 points
MAE: 13.70
RMSE: 22.00
MAPE: 8.89
current epoch: 97, predict 3 points
MAE: 14.20
RMSE: 22.86
MAPE: 9.21
current epoch: 97, predict 4 points
MAE: 14.58
RMSE: 23.52
MAPE: 9.45
current epoch: 97, predict 5 points
MAE: 14.99
RMSE: 24.17
MAPE: 9.71
current epoch: 97, predict 6 points
MAE: 15.40
RMSE: 24.76
MAPE: 9.95
current epoch: 97, predict 7 points
MAE: 15.82
RMSE: 25.33
MAPE: 10.27
current epoch: 97, predict 8 points
MAE: 16.15
RMSE: 25.82
MAPE: 10.49
current epoch: 97, predict 9 points
MAE: 16.47
RMSE: 26.23
MAPE: 10.69
current epoch: 97, predict 10 points
MAE: 16.80
RMSE: 26.69
MAPE: 10.97
current epoch: 97, predict 11 points
MAE: 17.23
RMSE: 27.31
MAPE: 11.26
all MAE: 15.04
all RMSE: 24.20
all MAPE: 9.77
[12.0664425, 19.32630362906706, 7.848040014505386, 13.051602, 20.983410049714347, 8.462197333574295, 13.700095, 22.001728336762262, 8.888546377420425, 14.197096, 22.86029480591362, 9.206276386976242, 14.583604, 23.522260795461126, 9.446626156568527, 14.985774, 24.16677133822519, 9.709779173135757, 15.400979, 24.761611218511586, 9.947003424167633, 15.818535, 25.332434521173273, 10.27117744088173, 16.150003, 25.817136482050298, 10.491720587015152, 16.468273, 26.232826646163318, 10.693790018558502, 16.800552, 26.688251279311206, 10.97107082605362, 17.22984, 27.311702201591856, 11.256660521030426, 15.037727, 24.196337916194828, 9.766063839197159]
fine tune the model ... 
epoch: 100, train time every whole data:415.59s
epoch: 100, total time:25380.82s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4365s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_100.params
epoch: 101, train time every whole data:415.59s
epoch: 101, total time:25886.86s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4518s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_101.params
epoch: 102, train time every whole data:415.64s
epoch: 102, total time:26392.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4734s
epoch: 103, train time every whole data:415.56s
epoch: 103, total time:26898.99s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4348s
epoch: 104, train time every whole data:415.59s
epoch: 104, total time:27405.02s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4510s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_104.params
epoch: 105, train time every whole data:415.62s
epoch: 105, total time:27911.11s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4483s
epoch: 106, train time every whole data:415.52s
epoch: 106, total time:28417.07s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4646s
epoch: 107, train time every whole data:415.54s
epoch: 107, total time:28923.08s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4493s
epoch: 108, train time every whole data:415.45s
epoch: 108, total time:29428.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4671s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_108.params
epoch: 109, train time every whole data:415.48s
epoch: 109, total time:29934.94s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4429s
epoch: 110, train time every whole data:415.64s
epoch: 110, total time:30441.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4314s
epoch: 111, train time every whole data:415.52s
epoch: 111, total time:30946.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4651s
epoch: 112, train time every whole data:415.59s
epoch: 112, total time:31453.04s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4818s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_112.params
epoch: 113, train time every whole data:415.54s
epoch: 113, total time:31959.07s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4617s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_113.params
epoch: 114, train time every whole data:415.59s
epoch: 114, total time:32465.13s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4413s
epoch: 115, train time every whole data:415.58s
epoch: 115, total time:32971.16s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4418s
epoch: 116, train time every whole data:415.54s
epoch: 116, total time:33477.14s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4253s
epoch: 117, train time every whole data:415.63s
epoch: 117, total time:33983.20s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4722s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_117.params
epoch: 118, train time every whole data:415.55s
epoch: 118, total time:34489.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4415s
epoch: 119, train time every whole data:415.55s
epoch: 119, total time:34995.23s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4490s
epoch: 120, train time every whole data:415.58s
epoch: 120, total time:35501.26s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4529s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_120.params
epoch: 121, train time every whole data:415.56s
epoch: 121, total time:36007.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4679s
epoch: 122, train time every whole data:415.59s
epoch: 122, total time:36513.35s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4430s
epoch: 123, train time every whole data:415.55s
epoch: 123, total time:37019.34s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5119s
epoch: 124, train time every whole data:415.58s
epoch: 124, total time:37525.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4695s
epoch: 125, train time every whole data:415.61s
epoch: 125, total time:38031.52s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4851s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_125.params
epoch: 126, train time every whole data:415.54s
epoch: 126, total time:38537.56s
validation batch 1 / 7, loss: 0.03
validation cost time: 111.3069s
epoch: 127, train time every whole data:745.51s
epoch: 127, total time:39394.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 157.7783s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_127.params
epoch: 128, train time every whole data:745.60s
epoch: 128, total time:40297.78s
validation batch 1 / 7, loss: 0.03
validation cost time: 159.9903s
epoch: 129, train time every whole data:742.90s
epoch: 129, total time:41200.67s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.2296s
epoch: 130, train time every whole data:744.92s
epoch: 130, total time:42105.82s
validation batch 1 / 7, loss: 0.03
validation cost time: 158.5584s
epoch: 131, train time every whole data:745.74s
epoch: 131, total time:43010.12s
validation batch 1 / 7, loss: 0.03
validation cost time: 157.5019s
epoch: 132, train time every whole data:745.22s
epoch: 132, total time:43912.84s
validation batch 1 / 7, loss: 0.03
validation cost time: 157.7149s
epoch: 133, train time every whole data:745.29s
epoch: 133, total time:44815.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.1018s
epoch: 134, train time every whole data:742.38s
epoch: 134, total time:45718.33s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.5096s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_134.params
epoch: 135, train time every whole data:745.51s
epoch: 135, total time:46624.37s
validation batch 1 / 7, loss: 0.03
validation cost time: 157.8465s
epoch: 136, train time every whole data:745.37s
epoch: 136, total time:47527.58s
validation batch 1 / 7, loss: 0.03
validation cost time: 157.6265s
epoch: 137, train time every whole data:745.78s
epoch: 137, total time:48430.99s
validation batch 1 / 7, loss: 0.03
validation cost time: 158.7814s
epoch: 138, train time every whole data:744.94s
epoch: 138, total time:49334.72s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.2845s
epoch: 139, train time every whole data:745.51s
epoch: 139, total time:50240.52s
validation batch 1 / 7, loss: 0.03
validation cost time: 158.3265s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_139.params
epoch: 140, train time every whole data:746.35s
epoch: 140, total time:51145.21s
validation batch 1 / 7, loss: 0.03
validation cost time: 157.4489s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_140.params
epoch: 141, train time every whole data:746.25s
epoch: 141, total time:52048.92s
validation batch 1 / 7, loss: 0.03
validation cost time: 158.3299s
epoch: 142, train time every whole data:745.61s
epoch: 142, total time:52952.86s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.2838s
epoch: 143, train time every whole data:745.03s
epoch: 143, total time:53858.18s
validation batch 1 / 7, loss: 0.03
validation cost time: 158.7578s
epoch: 144, train time every whole data:746.05s
epoch: 144, total time:54762.99s
validation batch 1 / 7, loss: 0.03
validation cost time: 157.5964s
epoch: 145, train time every whole data:746.20s
epoch: 145, total time:55666.78s
validation batch 1 / 7, loss: 0.03
validation cost time: 157.6838s
epoch: 146, train time every whole data:746.05s
epoch: 146, total time:56570.52s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.2945s
epoch: 147, train time every whole data:744.15s
epoch: 147, total time:57474.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 159.5734s
epoch: 148, train time every whole data:706.19s
epoch: 148, total time:58340.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 143.7076s
epoch: 149, train time every whole data:645.43s
epoch: 149, total time:59129.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 143.6754s
best epoch: 140
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_140.params
predicting testing set batch 1 / 7, time: 20.15s
test time on whole data:143.73s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 140, predict 0 points
MAE: 11.89
RMSE: 19.26
MAPE: 7.77
current epoch: 140, predict 1 points
MAE: 12.69
RMSE: 20.79
MAPE: 8.29
current epoch: 140, predict 2 points
MAE: 13.20
RMSE: 21.69
MAPE: 8.62
current epoch: 140, predict 3 points
MAE: 13.53
RMSE: 22.40
MAPE: 8.81
current epoch: 140, predict 4 points
MAE: 13.81
RMSE: 23.02
MAPE: 8.99
current epoch: 140, predict 5 points
MAE: 14.11
RMSE: 23.57
MAPE: 9.20
current epoch: 140, predict 6 points
MAE: 14.40
RMSE: 24.10
MAPE: 9.43
current epoch: 140, predict 7 points
MAE: 14.69
RMSE: 24.60
MAPE: 9.62
current epoch: 140, predict 8 points
MAE: 14.92
RMSE: 25.03
MAPE: 9.81
current epoch: 140, predict 9 points
MAE: 15.15
RMSE: 25.37
MAPE: 10.01
current epoch: 140, predict 10 points
MAE: 15.38
RMSE: 25.73
MAPE: 10.19
current epoch: 140, predict 11 points
MAE: 15.69
RMSE: 26.19
MAPE: 10.40
all MAE: 14.12
all RMSE: 23.57
all MAPE: 9.26
[11.890665, 19.255925046957948, 7.769249379634857, 12.686836, 20.794695045636413, 8.286480605602264, 13.196409, 21.688166275563372, 8.622722327709198, 13.52779, 22.398458618537305, 8.814138919115067, 13.814806, 23.01761649140311, 8.988140523433685, 14.110663, 23.56957836733747, 9.199132770299911, 14.403567, 24.097251944351065, 9.42729040980339, 14.69024, 24.59870924625988, 9.623914211988449, 14.924356, 25.027905568163433, 9.809750318527222, 15.1521, 25.366742373673915, 10.012300312519073, 15.3756695, 25.732456193644097, 10.189135372638702, 15.6865635, 26.19066256634342, 10.39673238992691, 14.121634, 23.56617283211521, 9.261584281921387]
