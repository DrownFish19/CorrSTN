Mon Nov  1 08:16:46 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   24C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   24C    P0    23W / 250W |     12MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   55C    P0   137W / 250W |  18942MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   55C    P0   112W / 250W |  25198MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   65C    P0   154W / 250W |  21064MiB / 32480MiB |     97%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   42C    P0    63W / 250W |   3726MiB / 32480MiB |     43%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    4     27065      C   python                                     18931MiB |
|    5     43157      C   python                                      2909MiB |
|    5     49354      C   python                                     18931MiB |
|    5     53019      C   python                                      3347MiB |
|    6     47790      C   python                                     21053MiB |
|    7     17340      C   python                                      3715MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u18112042/CorrSTN/configurations/PEMS08.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 8
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
load file: /data/home/u18112042/CorrSTN/data/PEMS08/PEMS08_r1_d0_w0.npz
ori length: 10699 , percent: 1.0 , scale: 10699
train: torch.Size([10699, 170, 1, 12]) torch.Size([10699, 170, 12]) torch.Size([10699, 170, 12])
val: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
test: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
TemporalPositionalEncoding max_len: 12
w_index: []
d_index: []
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([170, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([170, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 463721
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 7, loss: 1.10
validation cost time: 90.8660s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:158.28s
epoch: 0, total time:249.16s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.5034s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:158.33s
epoch: 1, total time:498.02s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.5185s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_2.params
epoch: 2, train time every whole data:158.27s
epoch: 2, total time:746.82s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.5002s
epoch: 3, train time every whole data:158.28s
epoch: 3, total time:995.61s
validation batch 1 / 7, loss: 0.10
validation cost time: 90.5241s
epoch: 4, train time every whole data:158.22s
epoch: 4, total time:1244.35s
validation batch 1 / 7, loss: 0.08
validation cost time: 90.5641s
epoch: 5, train time every whole data:158.33s
epoch: 5, total time:1493.25s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4975s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_6.params
epoch: 6, train time every whole data:158.28s
epoch: 6, total time:1742.04s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5150s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_7.params
epoch: 7, train time every whole data:157.99s
epoch: 7, total time:1990.56s
validation batch 1 / 7, loss: 0.06
validation cost time: 90.4982s
epoch: 8, train time every whole data:157.94s
epoch: 8, total time:2239.00s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4464s
epoch: 9, train time every whole data:157.98s
epoch: 9, total time:2487.42s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4749s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_10.params
epoch: 10, train time every whole data:157.99s
epoch: 10, total time:2735.90s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4874s
epoch: 11, train time every whole data:158.02s
epoch: 11, total time:2984.41s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.4901s
epoch: 12, train time every whole data:158.00s
epoch: 12, total time:3232.91s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4747s
epoch: 13, train time every whole data:157.99s
epoch: 13, total time:3481.37s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4908s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_14.params
epoch: 14, train time every whole data:158.01s
epoch: 14, total time:3729.88s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5410s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_15.params
epoch: 15, train time every whole data:157.98s
epoch: 15, total time:3978.42s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4876s
epoch: 16, train time every whole data:158.02s
epoch: 16, total time:4226.93s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5110s
epoch: 17, train time every whole data:157.96s
epoch: 17, total time:4475.40s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5045s
epoch: 18, train time every whole data:158.01s
epoch: 18, total time:4723.91s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.4976s
epoch: 19, train time every whole data:157.94s
epoch: 19, total time:4972.35s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.5246s
epoch: 20, train time every whole data:158.02s
epoch: 20, total time:5220.90s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5077s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_21.params
epoch: 21, train time every whole data:157.95s
epoch: 21, total time:5469.37s
validation batch 1 / 7, loss: 0.07
validation cost time: 90.5176s
epoch: 22, train time every whole data:158.03s
epoch: 22, total time:5717.92s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5151s
epoch: 23, train time every whole data:157.93s
epoch: 23, total time:5966.37s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.5077s
epoch: 24, train time every whole data:158.01s
epoch: 24, total time:6214.89s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4980s
epoch: 25, train time every whole data:157.95s
epoch: 25, total time:6463.34s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5271s
epoch: 26, train time every whole data:158.01s
epoch: 26, total time:6711.88s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5094s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_27.params
epoch: 27, train time every whole data:157.94s
epoch: 27, total time:6960.35s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4879s
epoch: 28, train time every whole data:158.01s
epoch: 28, total time:7208.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5192s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_29.params
epoch: 29, train time every whole data:157.97s
epoch: 29, total time:7457.35s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5130s
epoch: 30, train time every whole data:158.00s
epoch: 30, total time:7705.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5353s
epoch: 31, train time every whole data:158.00s
epoch: 31, total time:7954.40s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5290s
epoch: 32, train time every whole data:158.02s
epoch: 32, total time:8202.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5157s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_33.params
epoch: 33, train time every whole data:157.95s
epoch: 33, total time:8451.43s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4863s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_34.params
epoch: 34, train time every whole data:158.01s
epoch: 34, total time:8699.94s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.5200s
epoch: 35, train time every whole data:157.91s
epoch: 35, total time:8948.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5118s
epoch: 36, train time every whole data:158.01s
epoch: 36, total time:9196.89s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.5049s
epoch: 37, train time every whole data:157.93s
epoch: 37, total time:9445.33s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5341s
epoch: 38, train time every whole data:157.99s
epoch: 38, total time:9693.86s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5142s
epoch: 39, train time every whole data:157.93s
epoch: 39, total time:9942.30s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4880s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_40.params
epoch: 40, train time every whole data:157.92s
epoch: 40, total time:10190.72s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5152s
epoch: 41, train time every whole data:157.82s
epoch: 41, total time:10439.06s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5112s
epoch: 42, train time every whole data:157.92s
epoch: 42, total time:10687.50s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4959s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_43.params
epoch: 43, train time every whole data:157.85s
epoch: 43, total time:10935.86s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5609s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_44.params
epoch: 44, train time every whole data:157.91s
epoch: 44, total time:11184.34s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5038s
epoch: 45, train time every whole data:157.88s
epoch: 45, total time:11432.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4851s
epoch: 46, train time every whole data:157.90s
epoch: 46, total time:11681.12s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5142s
epoch: 47, train time every whole data:157.85s
epoch: 47, total time:11929.48s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5105s
epoch: 48, train time every whole data:157.87s
epoch: 48, total time:12177.87s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4935s
epoch: 49, train time every whole data:157.85s
epoch: 49, total time:12426.22s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5237s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_50.params
epoch: 50, train time every whole data:157.87s
epoch: 50, total time:12674.63s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5047s
epoch: 51, train time every whole data:157.89s
epoch: 51, total time:12923.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4845s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_52.params
epoch: 52, train time every whole data:157.87s
epoch: 52, total time:13171.39s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5138s
epoch: 53, train time every whole data:157.90s
epoch: 53, total time:13419.80s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5023s
epoch: 54, train time every whole data:157.90s
epoch: 54, total time:13668.21s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5287s
epoch: 55, train time every whole data:157.89s
epoch: 55, total time:13916.63s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4664s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_56.params
epoch: 56, train time every whole data:157.82s
epoch: 56, total time:14164.93s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4955s
epoch: 57, train time every whole data:157.88s
epoch: 57, total time:14413.31s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4844s
epoch: 58, train time every whole data:157.94s
epoch: 58, total time:14661.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5136s
epoch: 59, train time every whole data:157.90s
epoch: 59, total time:14910.15s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5010s
epoch: 60, train time every whole data:157.92s
epoch: 60, total time:15158.58s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4953s
epoch: 61, train time every whole data:157.89s
epoch: 61, total time:15406.97s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5192s
epoch: 62, train time every whole data:157.95s
epoch: 62, total time:15655.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5032s
epoch: 63, train time every whole data:157.89s
epoch: 63, total time:15903.83s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4793s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_64.params
epoch: 64, train time every whole data:157.96s
epoch: 64, total time:16152.28s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5146s
epoch: 65, train time every whole data:157.96s
epoch: 65, total time:16400.76s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5003s
epoch: 66, train time every whole data:157.90s
epoch: 66, total time:16649.16s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4966s
epoch: 67, train time every whole data:157.93s
epoch: 67, total time:16897.59s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5466s
epoch: 68, train time every whole data:157.95s
epoch: 68, total time:17146.09s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5043s
epoch: 69, train time every whole data:157.90s
epoch: 69, total time:17394.49s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4758s
epoch: 70, train time every whole data:157.90s
epoch: 70, total time:17642.87s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4825s
epoch: 71, train time every whole data:157.95s
epoch: 71, total time:17891.30s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5059s
epoch: 72, train time every whole data:157.90s
epoch: 72, total time:18139.70s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5008s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_73.params
epoch: 73, train time every whole data:157.92s
epoch: 73, total time:18388.14s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5231s
epoch: 74, train time every whole data:157.95s
epoch: 74, total time:18636.62s
validation batch 1 / 7, loss: 0.05
validation cost time: 90.5040s
epoch: 75, train time every whole data:157.90s
epoch: 75, total time:18885.02s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4747s
epoch: 76, train time every whole data:157.91s
epoch: 76, total time:19133.41s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5402s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_77.params
epoch: 77, train time every whole data:157.94s
epoch: 77, total time:19381.91s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4984s
epoch: 78, train time every whole data:157.88s
epoch: 78, total time:19630.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4951s
epoch: 79, train time every whole data:157.93s
epoch: 79, total time:19878.71s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5173s
epoch: 80, train time every whole data:157.92s
epoch: 80, total time:20127.15s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5055s
epoch: 81, train time every whole data:157.91s
epoch: 81, total time:20375.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4787s
epoch: 82, train time every whole data:157.90s
epoch: 82, total time:20623.95s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5124s
epoch: 83, train time every whole data:157.94s
epoch: 83, total time:20872.40s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4981s
epoch: 84, train time every whole data:157.84s
epoch: 84, total time:21120.74s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.4955s
epoch: 85, train time every whole data:157.93s
epoch: 85, total time:21369.17s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5199s
epoch: 86, train time every whole data:157.91s
epoch: 86, total time:21617.60s
validation batch 1 / 7, loss: 0.04
validation cost time: 90.5093s
epoch: 87, train time every whole data:157.90s
epoch: 87, total time:21866.01s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4780s
epoch: 88, train time every whole data:157.86s
epoch: 88, total time:22114.35s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5130s
epoch: 89, train time every whole data:157.95s
epoch: 89, total time:22362.81s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5043s
epoch: 90, train time every whole data:157.82s
epoch: 90, total time:22611.14s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4971s
epoch: 91, train time every whole data:157.93s
epoch: 91, total time:22859.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5185s
epoch: 92, train time every whole data:157.88s
epoch: 92, total time:23107.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5030s
epoch: 93, train time every whole data:157.91s
epoch: 93, total time:23356.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4788s
epoch: 94, train time every whole data:157.85s
epoch: 94, total time:23604.71s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5152s
epoch: 95, train time every whole data:157.95s
epoch: 95, total time:23853.18s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5037s
epoch: 96, train time every whole data:157.83s
epoch: 96, total time:24101.51s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4727s
epoch: 97, train time every whole data:157.93s
epoch: 97, total time:24349.92s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5505s
epoch: 98, train time every whole data:157.91s
epoch: 98, total time:24598.37s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5046s
epoch: 99, train time every whole data:157.91s
epoch: 99, total time:24846.79s
best epoch: 77
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_77.params
predicting testing set batch 1 / 7, time: 12.99s
test time on whole data:90.51s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 77, predict 0 points
MAE: 12.22
RMSE: 19.40
MAPE: 7.89
current epoch: 77, predict 1 points
MAE: 13.17
RMSE: 21.04
MAPE: 8.52
current epoch: 77, predict 2 points
MAE: 13.79
RMSE: 22.06
MAPE: 8.88
current epoch: 77, predict 3 points
MAE: 14.25
RMSE: 22.89
MAPE: 9.15
current epoch: 77, predict 4 points
MAE: 14.65
RMSE: 23.58
MAPE: 9.38
current epoch: 77, predict 5 points
MAE: 15.06
RMSE: 24.24
MAPE: 9.66
current epoch: 77, predict 6 points
MAE: 15.49
RMSE: 24.88
MAPE: 9.92
current epoch: 77, predict 7 points
MAE: 15.90
RMSE: 25.45
MAPE: 10.16
current epoch: 77, predict 8 points
MAE: 16.23
RMSE: 25.93
MAPE: 10.36
current epoch: 77, predict 9 points
MAE: 16.56
RMSE: 26.37
MAPE: 10.57
current epoch: 77, predict 10 points
MAE: 16.84
RMSE: 26.79
MAPE: 10.76
current epoch: 77, predict 11 points
MAE: 17.24
RMSE: 27.35
MAPE: 11.02
all MAE: 15.12
all RMSE: 24.28
all MAPE: 9.69
[12.215025, 19.402863187001955, 7.88596123456955, 13.168474, 21.037025146202424, 8.517967164516449, 13.792586, 22.06429743806829, 8.877041935920715, 14.253137, 22.88874912322168, 9.154441207647324, 14.651413, 23.58408069662007, 9.376516938209534, 15.062847, 24.24384788263303, 9.65925082564354, 15.493218, 24.876577662752116, 9.924377501010895, 15.895732, 25.453343211925276, 10.158833116292953, 16.22681, 25.925134712538743, 10.355237126350403, 16.560076, 26.37178895034249, 10.570568591356277, 16.843061, 26.791746274870388, 10.763029009103775, 17.244362, 27.349892167291994, 11.020243912935257, 15.11723, 24.279350844728246, 9.688617289066315]
fine tune the model ... 
epoch: 100, train time every whole data:415.63s
epoch: 100, total time:25353.75s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4813s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_100.params
epoch: 101, train time every whole data:415.68s
epoch: 101, total time:25859.93s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4920s
epoch: 102, train time every whole data:415.58s
epoch: 102, total time:26366.00s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4989s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_102.params
epoch: 103, train time every whole data:415.86s
epoch: 103, total time:26872.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4788s
epoch: 104, train time every whole data:415.75s
epoch: 104, total time:27378.61s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5079s
epoch: 105, train time every whole data:415.66s
epoch: 105, total time:27884.78s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5090s
epoch: 106, train time every whole data:415.88s
epoch: 106, total time:28391.17s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5465s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_106.params
epoch: 107, train time every whole data:415.96s
epoch: 107, total time:28897.69s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4926s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_107.params
epoch: 108, train time every whole data:415.56s
epoch: 108, total time:29403.75s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4957s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_108.params
epoch: 109, train time every whole data:415.83s
epoch: 109, total time:29910.09s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4985s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_109.params
epoch: 110, train time every whole data:415.80s
epoch: 110, total time:30416.40s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5032s
epoch: 111, train time every whole data:415.70s
epoch: 111, total time:30922.60s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5180s
epoch: 112, train time every whole data:415.56s
epoch: 112, total time:31428.68s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5090s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_112.params
epoch: 113, train time every whole data:415.82s
epoch: 113, total time:31935.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5255s
epoch: 114, train time every whole data:415.77s
epoch: 114, total time:32441.32s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4771s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_114.params
epoch: 115, train time every whole data:415.69s
epoch: 115, total time:32947.50s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5002s
epoch: 116, train time every whole data:415.71s
epoch: 116, total time:33453.71s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4962s
epoch: 117, train time every whole data:415.78s
epoch: 117, total time:33959.99s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5507s
epoch: 118, train time every whole data:415.75s
epoch: 118, total time:34466.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4913s
epoch: 119, train time every whole data:415.60s
epoch: 119, total time:34972.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5113s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_119.params
epoch: 120, train time every whole data:415.99s
epoch: 120, total time:35478.90s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5060s
epoch: 121, train time every whole data:415.78s
epoch: 121, total time:35985.19s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4993s
epoch: 122, train time every whole data:415.87s
epoch: 122, total time:36491.56s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4903s
epoch: 123, train time every whole data:415.90s
epoch: 123, total time:36997.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5133s
epoch: 124, train time every whole data:415.84s
epoch: 124, total time:37504.31s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5082s
epoch: 125, train time every whole data:416.02s
epoch: 125, total time:38010.84s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5092s
epoch: 126, train time every whole data:415.67s
epoch: 126, total time:38517.02s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5125s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_126.params
epoch: 127, train time every whole data:415.54s
epoch: 127, total time:39023.08s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5045s
epoch: 128, train time every whole data:415.30s
epoch: 128, total time:39528.89s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5017s
epoch: 129, train time every whole data:415.78s
epoch: 129, total time:40035.18s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5059s
epoch: 130, train time every whole data:415.72s
epoch: 130, total time:40541.40s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.4905s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_130.params
epoch: 131, train time every whole data:415.79s
epoch: 131, total time:41047.70s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5457s
epoch: 132, train time every whole data:446.85s
epoch: 132, total time:41585.10s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.3455s
epoch: 133, train time every whole data:698.77s
epoch: 133, total time:42435.21s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.2441s
epoch: 134, train time every whole data:698.60s
epoch: 134, total time:43285.05s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.3161s
epoch: 135, train time every whole data:698.34s
epoch: 135, total time:44134.71s
validation batch 1 / 7, loss: 0.03
validation cost time: 153.5366s
epoch: 136, train time every whole data:697.72s
epoch: 136, total time:44985.97s
validation batch 1 / 7, loss: 0.03
validation cost time: 154.6660s
epoch: 137, train time every whole data:698.61s
epoch: 137, total time:45839.25s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.1682s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_137.params
epoch: 138, train time every whole data:698.65s
epoch: 138, total time:46689.08s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.2297s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_138.params
epoch: 139, train time every whole data:699.02s
epoch: 139, total time:47539.34s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.1849s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_139.params
epoch: 140, train time every whole data:699.03s
epoch: 140, total time:48389.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.3510s
epoch: 141, train time every whole data:698.66s
epoch: 141, total time:49239.58s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.2899s
epoch: 142, train time every whole data:698.98s
epoch: 142, total time:50089.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.1734s
epoch: 143, train time every whole data:698.55s
epoch: 143, total time:50939.58s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.3886s
epoch: 144, train time every whole data:699.18s
epoch: 144, total time:51790.15s
validation batch 1 / 7, loss: 0.03
validation cost time: 155.4697s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_144.params
epoch: 145, train time every whole data:698.55s
epoch: 145, total time:52644.19s
validation batch 1 / 7, loss: 0.03
validation cost time: 152.0859s
epoch: 146, train time every whole data:698.86s
epoch: 146, total time:53495.13s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.3299s
epoch: 147, train time every whole data:698.92s
epoch: 147, total time:54345.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.1610s
epoch: 148, train time every whole data:698.75s
epoch: 148, total time:55195.30s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.1671s
epoch: 149, train time every whole data:698.83s
epoch: 149, total time:56045.30s
validation batch 1 / 7, loss: 0.03
validation cost time: 151.2538s
best epoch: 144
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_144.params
predicting testing set batch 1 / 7, time: 21.03s
test time on whole data:151.34s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 144, predict 0 points
MAE: 12.11
RMSE: 19.39
MAPE: 7.86
current epoch: 144, predict 1 points
MAE: 12.90
RMSE: 20.87
MAPE: 8.37
current epoch: 144, predict 2 points
MAE: 13.42
RMSE: 21.76
MAPE: 8.67
current epoch: 144, predict 3 points
MAE: 13.76
RMSE: 22.46
MAPE: 8.89
current epoch: 144, predict 4 points
MAE: 14.06
RMSE: 23.05
MAPE: 9.11
current epoch: 144, predict 5 points
MAE: 14.35
RMSE: 23.62
MAPE: 9.35
current epoch: 144, predict 6 points
MAE: 14.64
RMSE: 24.14
MAPE: 9.54
current epoch: 144, predict 7 points
MAE: 14.91
RMSE: 24.63
MAPE: 9.72
current epoch: 144, predict 8 points
MAE: 15.12
RMSE: 25.02
MAPE: 9.87
current epoch: 144, predict 9 points
MAE: 15.34
RMSE: 25.39
MAPE: 10.03
current epoch: 144, predict 10 points
MAE: 15.57
RMSE: 25.74
MAPE: 10.26
current epoch: 144, predict 11 points
MAE: 15.90
RMSE: 26.21
MAPE: 10.53
all MAE: 14.34
all RMSE: 23.61
all MAPE: 9.35
[12.11069, 19.39037790686643, 7.8630149364471436, 12.901262, 20.874577500870377, 8.36944580078125, 13.4179, 21.76390245225272, 8.66599977016449, 13.756742, 22.460966711937527, 8.894748985767365, 14.062891, 23.04842023421494, 9.113801270723343, 14.354773, 23.61691743353692, 9.345882385969162, 14.640588, 24.14457770026676, 9.536422789096832, 14.9104, 24.62697659222948, 9.71742644906044, 15.124846, 25.01945897969656, 9.86865684390068, 15.336282, 25.38632776135673, 10.034986585378647, 15.572464, 25.740839541718525, 10.255438089370728, 15.901726, 26.206668802922927, 10.5325385928154, 14.340876, 23.608179310804417, 9.349863976240158]
