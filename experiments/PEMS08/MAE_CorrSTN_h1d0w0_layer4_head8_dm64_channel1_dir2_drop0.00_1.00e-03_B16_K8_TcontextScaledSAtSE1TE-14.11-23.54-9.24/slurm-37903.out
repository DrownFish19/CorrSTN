Mon Nov  1 05:04:55 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   26C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   41C    P0    40W / 250W |     12MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   52C    P0   139W / 250W |  21851MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   49C    P0    94W / 250W |   6267MiB / 32480MiB |     97%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   62C    P0   137W / 250W |  18942MiB / 32480MiB |     90%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   46C    P0    83W / 250W |   3654MiB / 32480MiB |     58%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    4     23888      C   python                                      2909MiB |
|    4     27065      C   python                                     18931MiB |
|    5     43157      C   python                                      2909MiB |
|    5     53019      C   python                                      3347MiB |
|    6     47790      C   python                                     18931MiB |
|    7     17340      C   python                                      3643MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u18112042/CorrSTN/configurations/PEMS08.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 8
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
load file: /data/home/u18112042/CorrSTN/data/PEMS08/PEMS08_r1_d0_w0.npz
ori length: 10699 , percent: 1.0 , scale: 10699
train: torch.Size([10699, 170, 1, 12]) torch.Size([10699, 170, 12]) torch.Size([10699, 170, 12])
val: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
test: torch.Size([3567, 170, 1, 12]) torch.Size([3567, 170, 12]) torch.Size([3567, 170, 12])
TemporalPositionalEncoding max_len: 12
w_index: []
d_index: []
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(170, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([3])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([170, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([170, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([3])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 463721
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 7, loss: 0.30
validation cost time: 179.7714s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:341.62s
epoch: 0, total time:521.41s
validation batch 1 / 7, loss: 0.06
validation cost time: 183.0837s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:340.41s
epoch: 1, total time:1044.91s
validation batch 1 / 7, loss: 0.04
validation cost time: 179.2774s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_2.params
epoch: 2, train time every whole data:345.51s
epoch: 2, total time:1569.71s
validation batch 1 / 7, loss: 0.09
validation cost time: 184.9150s
epoch: 3, train time every whole data:345.47s
epoch: 3, total time:2100.10s
validation batch 1 / 7, loss: 0.05
validation cost time: 179.3034s
epoch: 4, train time every whole data:345.23s
epoch: 4, total time:2624.64s
validation batch 1 / 7, loss: 0.09
validation cost time: 179.5719s
epoch: 5, train time every whole data:340.44s
epoch: 5, total time:3144.65s
validation batch 1 / 7, loss: 0.08
validation cost time: 182.8348s
epoch: 6, train time every whole data:341.66s
epoch: 6, total time:3669.14s
validation batch 1 / 7, loss: 0.06
validation cost time: 184.9374s
epoch: 7, train time every whole data:345.65s
epoch: 7, total time:4199.73s
validation batch 1 / 7, loss: 0.05
validation cost time: 179.3387s
epoch: 8, train time every whole data:345.47s
epoch: 8, total time:4724.55s
validation batch 1 / 7, loss: 0.08
validation cost time: 179.3001s
epoch: 9, train time every whole data:340.38s
epoch: 9, total time:5244.23s
validation batch 1 / 7, loss: 0.08
validation cost time: 188.5761s
epoch: 10, train time every whole data:336.09s
epoch: 10, total time:5768.89s
validation batch 1 / 7, loss: 0.06
validation cost time: 184.8841s
epoch: 11, train time every whole data:345.48s
epoch: 11, total time:6299.26s
validation batch 1 / 7, loss: 0.05
validation cost time: 179.2965s
epoch: 12, train time every whole data:345.42s
epoch: 12, total time:6823.98s
validation batch 1 / 7, loss: 0.07
validation cost time: 179.3402s
epoch: 13, train time every whole data:340.37s
epoch: 13, total time:7343.69s
validation batch 1 / 7, loss: 0.04
validation cost time: 189.6122s
epoch: 14, train time every whole data:334.87s
epoch: 14, total time:7868.17s
validation batch 1 / 7, loss: 0.04
validation cost time: 184.8353s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_15.params
epoch: 15, train time every whole data:345.41s
epoch: 15, total time:8398.43s
validation batch 1 / 7, loss: 0.05
validation cost time: 179.3291s
epoch: 16, train time every whole data:345.54s
epoch: 16, total time:8923.30s
validation batch 1 / 7, loss: 0.06
validation cost time: 179.2746s
epoch: 17, train time every whole data:343.36s
epoch: 17, total time:9445.94s
validation batch 1 / 7, loss: 0.04
validation cost time: 186.9174s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_18.params
epoch: 18, train time every whole data:334.62s
epoch: 18, total time:9967.49s
validation batch 1 / 7, loss: 0.05
validation cost time: 184.8682s
epoch: 19, train time every whole data:345.32s
epoch: 19, total time:10497.68s
validation batch 1 / 7, loss: 0.04
validation cost time: 179.2584s
epoch: 20, train time every whole data:345.52s
epoch: 20, total time:11022.45s
validation batch 1 / 7, loss: 0.06
validation cost time: 179.3212s
epoch: 21, train time every whole data:345.41s
epoch: 21, total time:11547.18s
validation batch 1 / 7, loss: 0.04
validation cost time: 184.7481s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_22.params
epoch: 22, train time every whole data:334.50s
epoch: 22, total time:12066.44s
validation batch 1 / 7, loss: 0.04
validation cost time: 185.9444s
epoch: 23, train time every whole data:343.93s
epoch: 23, total time:12596.31s
validation batch 1 / 7, loss: 0.04
validation cost time: 179.1972s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_24.params
epoch: 24, train time every whole data:345.28s
epoch: 24, total time:13120.80s
validation batch 1 / 7, loss: 0.03
validation cost time: 179.1563s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_25.params
epoch: 25, train time every whole data:345.17s
epoch: 25, total time:13645.14s
validation batch 1 / 7, loss: 0.05
validation cost time: 184.7308s
epoch: 26, train time every whole data:334.61s
epoch: 26, total time:14164.48s
validation batch 1 / 7, loss: 0.04
validation cost time: 188.5407s
epoch: 27, train time every whole data:341.14s
epoch: 27, total time:14694.17s
validation batch 1 / 7, loss: 0.04
validation cost time: 179.2069s
epoch: 28, train time every whole data:345.34s
epoch: 28, total time:15218.72s
validation batch 1 / 7, loss: 0.04
validation cost time: 179.1800s
epoch: 29, train time every whole data:345.16s
epoch: 29, total time:15743.05s
validation batch 1 / 7, loss: 0.04
validation cost time: 184.6931s
epoch: 30, train time every whole data:334.53s
epoch: 30, total time:16262.28s
validation batch 1 / 7, loss: 0.03
validation cost time: 189.4810s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_31.params
epoch: 31, train time every whole data:340.13s
epoch: 31, total time:16791.91s
validation batch 1 / 7, loss: 0.06
validation cost time: 179.1299s
epoch: 32, train time every whole data:345.29s
epoch: 32, total time:17316.34s
validation batch 1 / 7, loss: 0.03
validation cost time: 179.1677s
epoch: 33, train time every whole data:345.32s
epoch: 33, total time:17840.83s
validation batch 1 / 7, loss: 0.03
validation cost time: 184.7270s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_34.params
epoch: 34, train time every whole data:338.75s
epoch: 34, total time:18364.32s
validation batch 1 / 7, loss: 0.04
validation cost time: 185.4120s
epoch: 35, train time every whole data:340.01s
epoch: 35, total time:18889.75s
validation batch 1 / 7, loss: 0.03
validation cost time: 179.1789s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_36.params
epoch: 36, train time every whole data:321.30s
epoch: 36, total time:19390.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.8923s
epoch: 37, train time every whole data:302.95s
epoch: 37, total time:19854.09s
validation batch 1 / 7, loss: 0.03
validation cost time: 172.3942s
epoch: 38, train time every whole data:290.88s
epoch: 38, total time:20317.37s
validation batch 1 / 7, loss: 0.03
validation cost time: 172.4277s
epoch: 39, train time every whole data:303.13s
epoch: 39, total time:20792.93s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.8960s
epoch: 40, train time every whole data:302.97s
epoch: 40, total time:21256.80s
validation batch 1 / 7, loss: 0.04
validation cost time: 160.9081s
epoch: 41, train time every whole data:303.02s
epoch: 41, total time:21720.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.8947s
epoch: 42, train time every whole data:302.94s
epoch: 42, total time:22184.57s
validation batch 1 / 7, loss: 0.04
validation cost time: 171.3449s
epoch: 43, train time every whole data:297.80s
epoch: 43, total time:22653.71s
validation batch 1 / 7, loss: 0.03
validation cost time: 166.7965s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_44.params
epoch: 44, train time every whole data:303.17s
epoch: 44, total time:23123.69s
validation batch 1 / 7, loss: 0.03
validation cost time: 160.9287s
epoch: 45, train time every whole data:303.03s
epoch: 45, total time:23587.65s
validation batch 1 / 7, loss: 0.04
validation cost time: 160.8730s
epoch: 46, train time every whole data:303.17s
epoch: 46, total time:24051.70s
validation batch 1 / 7, loss: 0.04
validation cost time: 166.5158s
epoch: 47, train time every whole data:297.35s
epoch: 47, total time:24515.56s
validation batch 1 / 7, loss: 0.03
validation cost time: 166.5358s
epoch: 48, train time every whole data:301.90s
epoch: 48, total time:24984.00s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1858s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_49.params
epoch: 49, train time every whole data:232.37s
epoch: 49, total time:25356.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1646s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_50.params
epoch: 50, train time every whole data:238.66s
epoch: 50, total time:25735.40s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2619s
epoch: 51, train time every whole data:238.38s
epoch: 51, total time:26108.05s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2683s
epoch: 52, train time every whole data:238.64s
epoch: 52, total time:26480.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2697s
epoch: 53, train time every whole data:238.42s
epoch: 53, total time:26853.64s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2562s
epoch: 54, train time every whole data:238.41s
epoch: 54, total time:27226.31s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1935s
epoch: 55, train time every whole data:232.46s
epoch: 55, total time:27598.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1815s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_56.params
epoch: 56, train time every whole data:232.35s
epoch: 56, total time:27971.51s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1991s
epoch: 57, train time every whole data:238.58s
epoch: 57, total time:28350.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2511s
epoch: 58, train time every whole data:238.66s
epoch: 58, total time:28723.20s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2468s
epoch: 59, train time every whole data:238.34s
epoch: 59, total time:29095.79s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2779s
epoch: 60, train time every whole data:238.48s
epoch: 60, total time:29468.55s
validation batch 1 / 7, loss: 0.04
validation cost time: 134.2133s
epoch: 61, train time every whole data:238.58s
epoch: 61, total time:29841.34s
validation batch 1 / 7, loss: 0.04
validation cost time: 138.5126s
epoch: 62, train time every whole data:234.06s
epoch: 62, total time:30213.92s
validation batch 1 / 7, loss: 0.04
validation cost time: 140.1983s
epoch: 63, train time every whole data:232.77s
epoch: 63, total time:30586.89s
validation batch 1 / 7, loss: 0.04
validation cost time: 140.1787s
epoch: 64, train time every whole data:238.76s
epoch: 64, total time:30965.82s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2392s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_65.params
epoch: 65, train time every whole data:238.40s
epoch: 65, total time:31338.48s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2489s
epoch: 66, train time every whole data:238.56s
epoch: 66, total time:31711.28s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2187s
epoch: 67, train time every whole data:238.43s
epoch: 67, total time:32083.94s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2129s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_68.params
epoch: 68, train time every whole data:238.56s
epoch: 68, total time:32456.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 136.9887s
epoch: 69, train time every whole data:235.84s
epoch: 69, total time:32829.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.2424s
epoch: 70, train time every whole data:232.31s
epoch: 70, total time:33202.12s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1736s
epoch: 71, train time every whole data:238.42s
epoch: 71, total time:33580.72s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2184s
epoch: 72, train time every whole data:238.55s
epoch: 72, total time:33953.49s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2362s
epoch: 73, train time every whole data:238.55s
epoch: 73, total time:34326.27s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2479s
epoch: 74, train time every whole data:238.59s
epoch: 74, total time:34699.11s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2597s
epoch: 75, train time every whole data:238.48s
epoch: 75, total time:35071.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 135.1414s
epoch: 76, train time every whole data:237.71s
epoch: 76, total time:35444.71s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1957s
epoch: 77, train time every whole data:232.33s
epoch: 77, total time:35817.23s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1784s
epoch: 78, train time every whole data:238.54s
epoch: 78, total time:36195.95s
validation batch 1 / 7, loss: 0.04
validation cost time: 134.2257s
epoch: 79, train time every whole data:238.63s
epoch: 79, total time:36568.81s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2389s
epoch: 80, train time every whole data:238.64s
epoch: 80, total time:36941.69s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2255s
epoch: 81, train time every whole data:238.74s
epoch: 81, total time:37314.65s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2790s
epoch: 82, train time every whole data:238.48s
epoch: 82, total time:37687.42s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2586s
epoch: 83, train time every whole data:238.59s
epoch: 83, total time:38060.27s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1739s
epoch: 84, train time every whole data:232.42s
epoch: 84, total time:38432.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1658s
epoch: 85, train time every whole data:238.45s
epoch: 85, total time:38811.49s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2560s
epoch: 86, train time every whole data:238.73s
epoch: 86, total time:39184.48s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2442s
epoch: 87, train time every whole data:238.59s
epoch: 87, total time:39557.31s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2749s
epoch: 88, train time every whole data:238.73s
epoch: 88, total time:39930.32s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2462s
epoch: 89, train time every whole data:238.47s
epoch: 89, total time:40303.04s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2774s
epoch: 90, train time every whole data:238.36s
epoch: 90, total time:40675.68s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1232s
epoch: 91, train time every whole data:232.60s
epoch: 91, total time:41048.40s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.2014s
epoch: 92, train time every whole data:238.53s
epoch: 92, total time:41427.13s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2439s
epoch: 93, train time every whole data:238.66s
epoch: 93, total time:41800.04s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2536s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_94.params
epoch: 94, train time every whole data:238.43s
epoch: 94, total time:42172.73s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2511s
epoch: 95, train time every whole data:238.39s
epoch: 95, total time:42545.38s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2268s
epoch: 96, train time every whole data:238.37s
epoch: 96, total time:42917.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2339s
epoch: 97, train time every whole data:238.20s
epoch: 97, total time:43290.41s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1486s
epoch: 98, train time every whole data:232.38s
epoch: 98, total time:43662.95s
validation batch 1 / 7, loss: 0.04
validation cost time: 140.1936s
epoch: 99, train time every whole data:234.90s
epoch: 99, total time:44038.04s
best epoch: 94
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_94.params
predicting testing set batch 1 / 7, time: 22.43s
test time on whole data:137.44s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 94, predict 0 points
MAE: 12.05
RMSE: 19.29
MAPE: 7.84
current epoch: 94, predict 1 points
MAE: 12.98
RMSE: 20.93
MAPE: 8.43
current epoch: 94, predict 2 points
MAE: 13.59
RMSE: 21.91
MAPE: 8.79
current epoch: 94, predict 3 points
MAE: 14.06
RMSE: 22.73
MAPE: 9.12
current epoch: 94, predict 4 points
MAE: 14.47
RMSE: 23.41
MAPE: 9.37
current epoch: 94, predict 5 points
MAE: 14.90
RMSE: 24.07
MAPE: 9.67
current epoch: 94, predict 6 points
MAE: 15.32
RMSE: 24.65
MAPE: 9.95
current epoch: 94, predict 7 points
MAE: 15.64
RMSE: 25.18
MAPE: 10.14
current epoch: 94, predict 8 points
MAE: 16.00
RMSE: 25.65
MAPE: 10.39
current epoch: 94, predict 9 points
MAE: 16.37
RMSE: 26.16
MAPE: 10.74
current epoch: 94, predict 10 points
MAE: 16.73
RMSE: 26.65
MAPE: 10.98
current epoch: 94, predict 11 points
MAE: 17.16
RMSE: 27.22
MAPE: 11.26
all MAE: 14.94
all RMSE: 24.10
all MAPE: 9.72
[12.048488, 19.294854556422955, 7.837648689746857, 12.979514, 20.926050682533692, 8.426021784543991, 13.589146, 21.905734547252404, 8.787204325199127, 14.059647, 22.7319492369942, 9.118922799825668, 14.469646, 23.408710182373223, 9.373379498720169, 14.898954, 24.067714532387573, 9.6722312271595, 15.315283, 24.650080422662924, 9.951239079236984, 15.642681, 25.176841582112345, 10.143926739692688, 16.00187, 25.650743309390055, 10.387992858886719, 16.374382, 26.159565668729336, 10.7427217066288, 16.732924, 26.645154032176986, 10.981883853673935, 17.160955, 27.224108793666232, 11.255458742380142, 14.939458, 24.098612056036416, 9.72321480512619]
fine tune the model ... 
epoch: 100, train time every whole data:610.60s
epoch: 100, total time:44787.25s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2819s
epoch: 101, train time every whole data:610.50s
epoch: 101, total time:45532.04s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2742s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_101.params
epoch: 102, train time every whole data:610.37s
epoch: 102, total time:46276.70s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.2018s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_102.params
epoch: 103, train time every whole data:609.38s
epoch: 103, total time:47026.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 135.1916s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_103.params
epoch: 104, train time every whole data:610.49s
epoch: 104, total time:47771.98s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2546s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_104.params
epoch: 105, train time every whole data:610.44s
epoch: 105, total time:48516.69s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2421s
epoch: 106, train time every whole data:610.21s
epoch: 106, total time:49261.15s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.1932s
epoch: 107, train time every whole data:610.29s
epoch: 107, total time:50011.64s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.3351s
epoch: 108, train time every whole data:610.28s
epoch: 108, total time:50756.26s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2743s
epoch: 109, train time every whole data:610.33s
epoch: 109, total time:51500.86s
validation batch 1 / 7, loss: 0.03
validation cost time: 134.2578s
epoch: 110, train time every whole data:610.32s
epoch: 110, total time:52245.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 140.2470s
epoch: 111, train time every whole data:611.18s
epoch: 111, total time:52996.87s
validation batch 1 / 7, loss: 0.03
validation cost time: 99.1522s
epoch: 112, train time every whole data:416.62s
epoch: 112, total time:53512.64s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5425s
epoch: 113, train time every whole data:416.84s
epoch: 113, total time:54020.03s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5460s
epoch: 114, train time every whole data:416.87s
epoch: 114, total time:54527.44s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5309s
epoch: 115, train time every whole data:416.80s
epoch: 115, total time:55034.78s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5753s
epoch: 116, train time every whole data:416.88s
epoch: 116, total time:55542.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5341s
epoch: 117, train time every whole data:416.88s
epoch: 117, total time:56049.66s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5535s
epoch: 118, train time every whole data:416.94s
epoch: 118, total time:56557.15s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5551s
epoch: 119, train time every whole data:416.91s
epoch: 119, total time:57064.62s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5397s
epoch: 120, train time every whole data:416.83s
epoch: 120, total time:57572.00s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5487s
epoch: 121, train time every whole data:416.83s
epoch: 121, total time:58079.37s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5550s
epoch: 122, train time every whole data:416.31s
epoch: 122, total time:58586.24s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5406s
epoch: 123, train time every whole data:416.18s
epoch: 123, total time:59092.97s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5301s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_123.params
epoch: 124, train time every whole data:416.18s
epoch: 124, total time:59599.69s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5345s
epoch: 125, train time every whole data:416.10s
epoch: 125, total time:60106.33s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5402s
epoch: 126, train time every whole data:416.19s
epoch: 126, total time:60613.06s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5343s
epoch: 127, train time every whole data:416.25s
epoch: 127, total time:61119.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5257s
epoch: 128, train time every whole data:416.29s
epoch: 128, total time:61626.66s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5244s
epoch: 129, train time every whole data:416.28s
epoch: 129, total time:62133.47s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5255s
save parameters to file: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_129.params
epoch: 130, train time every whole data:416.16s
epoch: 130, total time:62640.17s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5750s
epoch: 131, train time every whole data:416.24s
epoch: 131, total time:63146.99s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5253s
epoch: 132, train time every whole data:416.13s
epoch: 132, total time:63653.64s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5277s
epoch: 133, train time every whole data:416.10s
epoch: 133, total time:64160.27s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5281s
epoch: 134, train time every whole data:416.05s
epoch: 134, total time:64666.85s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5484s
epoch: 135, train time every whole data:416.65s
epoch: 135, total time:65174.05s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5321s
epoch: 136, train time every whole data:416.78s
epoch: 136, total time:65681.37s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5274s
epoch: 137, train time every whole data:416.94s
epoch: 137, total time:66188.84s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5323s
epoch: 138, train time every whole data:416.94s
epoch: 138, total time:66696.31s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5304s
epoch: 139, train time every whole data:416.85s
epoch: 139, total time:67203.69s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5397s
epoch: 140, train time every whole data:416.74s
epoch: 140, total time:67710.97s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5523s
epoch: 141, train time every whole data:416.56s
epoch: 141, total time:68218.09s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5802s
epoch: 142, train time every whole data:416.31s
epoch: 142, total time:68724.97s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5381s
epoch: 143, train time every whole data:416.05s
epoch: 143, total time:69231.57s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5383s
epoch: 144, train time every whole data:416.17s
epoch: 144, total time:69738.28s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5282s
epoch: 145, train time every whole data:416.11s
epoch: 145, total time:70244.92s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5424s
epoch: 146, train time every whole data:416.13s
epoch: 146, total time:70751.60s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5309s
epoch: 147, train time every whole data:416.16s
epoch: 147, total time:71258.29s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5302s
epoch: 148, train time every whole data:416.13s
epoch: 148, total time:71764.96s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5689s
epoch: 149, train time every whole data:416.19s
epoch: 149, total time:72271.72s
validation batch 1 / 7, loss: 0.03
validation cost time: 90.5354s
best epoch: 129
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS08/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K8_TcontextScaledSAtSE1TE/epoch_129.params
predicting testing set batch 1 / 7, time: 13.00s
test time on whole data:90.55s
input: (3567, 170, 12, 1)
prediction: (3567, 170, 12, 1)
data_target_tensor: (3567, 170, 12)
current epoch: 129, predict 0 points
MAE: 11.90
RMSE: 19.20
MAPE: 7.79
current epoch: 129, predict 1 points
MAE: 12.71
RMSE: 20.74
MAPE: 8.28
current epoch: 129, predict 2 points
MAE: 13.22
RMSE: 21.66
MAPE: 8.59
current epoch: 129, predict 3 points
MAE: 13.53
RMSE: 22.34
MAPE: 8.80
current epoch: 129, predict 4 points
MAE: 13.82
RMSE: 22.88
MAPE: 8.98
current epoch: 129, predict 5 points
MAE: 14.10
RMSE: 23.44
MAPE: 9.16
current epoch: 129, predict 6 points
MAE: 14.39
RMSE: 23.96
MAPE: 9.38
current epoch: 129, predict 7 points
MAE: 14.65
RMSE: 24.49
MAPE: 9.57
current epoch: 129, predict 8 points
MAE: 14.88
RMSE: 24.99
MAPE: 9.76
current epoch: 129, predict 9 points
MAE: 15.11
RMSE: 25.45
MAPE: 9.95
current epoch: 129, predict 10 points
MAE: 15.37
RMSE: 25.87
MAPE: 10.18
current epoch: 129, predict 11 points
MAE: 15.68
RMSE: 26.34
MAPE: 10.41
all MAE: 14.11
all RMSE: 23.54
all MAPE: 9.24
[11.89654, 19.201782874935645, 7.789646089076996, 12.705763, 20.74263476448101, 8.279884606599808, 13.216445, 21.660856123160997, 8.586293458938599, 13.534021, 22.33992400591022, 8.80415141582489, 13.817658, 22.880602541372244, 8.983621001243591, 14.104782, 23.435966095638932, 9.159307181835175, 14.39176, 23.95966128000026, 9.38098281621933, 14.649595, 24.487153091037477, 9.567374736070633, 14.882468, 24.987947534214495, 9.759271889925003, 15.107609, 25.45017895588789, 9.946277737617493, 15.366021, 25.869490327123884, 10.17659604549408, 15.681419, 26.33654469412761, 10.414781421422958, 14.112836, 23.53833517172041, 9.237346798181534]
