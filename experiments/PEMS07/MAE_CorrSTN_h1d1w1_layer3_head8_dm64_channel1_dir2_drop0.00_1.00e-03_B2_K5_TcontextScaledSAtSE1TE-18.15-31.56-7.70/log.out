Mon Aug 16 20:50:35 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   25C    P0    23W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   27C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   26C    P0    24W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   54C    P0   126W / 250W |   3936MiB / 32480MiB |     72%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   54C    P0    26W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   64C    P0    98W / 250W |  11489MiB / 32480MiB |     78%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   35C    P0    41W / 250W |  11006MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   31C    P0    34W / 250W |  11369MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    3      3099      C   python                                      3925MiB |
|    5      2451      C   python                                      3921MiB |
|    5     42253      C   python                                      7557MiB |
|    6     54094      C   python                                     10995MiB |
|    7      1942      C   python                                     11357MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u18112042/IC-STGNN/configurations/PEMS07_rdw-smooth.conf
total training epoch, fine tune epoch: 50 , 25
batch_size: 2
folder_dir: MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE
load file: /data/home/u18112042/IC-STGNN/data/PEMS07/PEMS07_r1_d1_w1.npz
ori length: 15718 , percent: 1.0 , scale: 15718
train: torch.Size([15718, 883, 1, 36]) torch.Size([15718, 883, 12]) torch.Size([15718, 883, 12])
val: torch.Size([5239, 883, 1, 36]) torch.Size([5239, 883, 12]) torch.Size([5239, 883, 12])
test: torch.Size([5240, 883, 1, 36]) torch.Size([5240, 883, 12]) torch.Size([5240, 883, 12])
TemporalPositionalEncoding max_len: 2016
w_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
d_index: [1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739]
h_index: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.coefficient 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.coefficient 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.coefficient 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.coefficient 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.coefficient 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.coefficient 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 2016, 64])
src_embed.2.embedding.weight 	 torch.Size([883, 64])
src_embed.2.gcn_smooth_layers.0.coefficient 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([883, 64])
trg_embed.2.gcn_smooth_layers.0.coefficient 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 575305
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]}]
validation batch 1 / 2620, loss: 1.06
validation batch 101 / 2620, loss: 1.44
validation batch 201 / 2620, loss: 1.39
validation batch 301 / 2620, loss: 1.06
validation batch 401 / 2620, loss: 1.28
validation batch 501 / 2620, loss: 1.53
validation batch 601 / 2620, loss: 1.07
validation batch 701 / 2620, loss: 1.11
validation batch 801 / 2620, loss: 1.52
validation batch 901 / 2620, loss: 1.17
validation batch 1001 / 2620, loss: 1.06
validation batch 1101 / 2620, loss: 1.51
validation batch 1201 / 2620, loss: 1.28
validation batch 1301 / 2620, loss: 1.07
validation batch 1401 / 2620, loss: 1.07
validation batch 1501 / 2620, loss: 1.47
validation batch 1601 / 2620, loss: 1.05
validation batch 1701 / 2620, loss: 1.10
validation batch 1801 / 2620, loss: 1.52
validation batch 1901 / 2620, loss: 1.08
validation batch 2001 / 2620, loss: 1.06
validation batch 2101 / 2620, loss: 1.55
validation batch 2201 / 2620, loss: 1.20
validation batch 2301 / 2620, loss: 1.09
validation batch 2401 / 2620, loss: 1.13
validation batch 2501 / 2620, loss: 1.36
validation batch 2601 / 2620, loss: 1.06
validation cost time: 668.2762s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:1471.13s
epoch: 0, total time:2139.42s
validation batch 1 / 2620, loss: 0.06
validation batch 101 / 2620, loss: 0.06
validation batch 201 / 2620, loss: 0.07
validation batch 301 / 2620, loss: 0.06
validation batch 401 / 2620, loss: 0.07
validation batch 501 / 2620, loss: 0.07
validation batch 601 / 2620, loss: 0.06
validation batch 701 / 2620, loss: 0.05
validation batch 801 / 2620, loss: 0.05
validation batch 901 / 2620, loss: 0.07
validation batch 1001 / 2620, loss: 0.05
validation batch 1101 / 2620, loss: 0.05
validation batch 1201 / 2620, loss: 0.08
validation batch 1301 / 2620, loss: 0.05
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.07
validation batch 1601 / 2620, loss: 0.05
validation batch 1701 / 2620, loss: 0.05
validation batch 1801 / 2620, loss: 0.06
validation batch 1901 / 2620, loss: 0.06
validation batch 2001 / 2620, loss: 0.05
validation batch 2101 / 2620, loss: 0.07
validation batch 2201 / 2620, loss: 0.06
validation batch 2301 / 2620, loss: 0.05
validation batch 2401 / 2620, loss: 0.06
validation batch 2501 / 2620, loss: 0.07
validation batch 2601 / 2620, loss: 0.06
validation cost time: 667.7825s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:1471.73s
epoch: 1, total time:4278.95s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.04
validation batch 201 / 2620, loss: 0.04
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.04
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.05
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.04
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.04
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.05
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.04
validation cost time: 668.0070s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_2.params
epoch: 2, train time every whole data:1471.39s
epoch: 2, total time:6418.36s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 667.8469s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_3.params
epoch: 3, train time every whole data:1471.48s
epoch: 3, total time:8557.70s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.04
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.03
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.07
validation batch 1501 / 2620, loss: 0.04
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.04
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.03
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.04
validation cost time: 667.4425s
epoch: 4, train time every whole data:1471.08s
epoch: 4, total time:10696.22s
validation batch 1 / 2620, loss: 0.05
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.04
validation batch 301 / 2620, loss: 0.05
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.03
validation batch 601 / 2620, loss: 0.06
validation batch 701 / 2620, loss: 0.05
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.05
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.05
validation batch 1401 / 2620, loss: 0.10
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.05
validation batch 1701 / 2620, loss: 0.05
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.05
validation batch 2001 / 2620, loss: 0.05
validation batch 2101 / 2620, loss: 0.03
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.05
validation batch 2401 / 2620, loss: 0.05
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.05
validation cost time: 667.1648s
epoch: 5, train time every whole data:1470.45s
epoch: 5, total time:12833.83s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.4082s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_6.params
epoch: 6, train time every whole data:1471.31s
epoch: 6, total time:14972.57s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.2606s
epoch: 7, train time every whole data:1471.49s
epoch: 7, total time:17111.32s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.2656s
epoch: 8, train time every whole data:1470.98s
epoch: 8, total time:19249.57s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.2363s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_9.params
epoch: 9, train time every whole data:1471.03s
epoch: 9, total time:21387.85s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 667.2211s
epoch: 10, train time every whole data:1471.48s
epoch: 10, total time:23526.56s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.0386s
epoch: 11, train time every whole data:1471.25s
epoch: 11, total time:25664.85s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.2922s
epoch: 12, train time every whole data:1471.96s
epoch: 12, total time:27804.11s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.03
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.03
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.04
validation cost time: 667.1062s
epoch: 13, train time every whole data:1471.79s
epoch: 13, total time:29943.00s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.03
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.03
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.5682s
epoch: 14, train time every whole data:1471.33s
epoch: 14, total time:32081.90s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.0901s
epoch: 15, train time every whole data:1471.22s
epoch: 15, total time:34220.21s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 667.2672s
epoch: 16, train time every whole data:1470.41s
epoch: 16, total time:36357.89s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 666.9545s
epoch: 17, train time every whole data:1470.32s
epoch: 17, total time:38495.18s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.0793s
epoch: 18, train time every whole data:1470.78s
epoch: 18, total time:40633.03s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.0670s
epoch: 19, train time every whole data:1471.04s
epoch: 19, total time:42771.14s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 667.2198s
epoch: 20, train time every whole data:1470.95s
epoch: 20, total time:44909.32s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.3289s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_21.params
epoch: 21, train time every whole data:1471.26s
epoch: 21, total time:47047.92s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.04
validation cost time: 667.6730s
epoch: 22, train time every whole data:1472.41s
epoch: 22, total time:49188.01s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.7147s
epoch: 23, train time every whole data:1472.66s
epoch: 23, total time:51328.38s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.8985s
epoch: 24, train time every whole data:1472.64s
epoch: 24, total time:53468.92s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.8837s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_25.params
epoch: 25, train time every whole data:1471.81s
epoch: 25, total time:55608.62s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.9995s
epoch: 26, train time every whole data:1472.19s
epoch: 26, total time:57748.81s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.8940s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_27.params
epoch: 27, train time every whole data:1472.77s
epoch: 27, total time:59889.48s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 718.3566s
epoch: 28, train time every whole data:1763.08s
epoch: 28, total time:62370.92s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 822.7661s
epoch: 29, train time every whole data:1761.85s
epoch: 29, total time:64955.54s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 822.6074s
epoch: 30, train time every whole data:1761.71s
epoch: 30, total time:67539.86s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 822.6613s
epoch: 31, train time every whole data:1761.61s
epoch: 31, total time:70124.13s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 822.7096s
epoch: 32, train time every whole data:1770.18s
epoch: 32, total time:72717.02s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 832.4627s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_33.params
epoch: 33, train time every whole data:1788.37s
epoch: 33, total time:75337.87s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 832.2018s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_34.params
epoch: 34, train time every whole data:1787.99s
epoch: 34, total time:77958.07s
validation batch 1 / 2620, loss: 0.05
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.05
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.05
validation batch 1301 / 2620, loss: 0.05
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.05
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.05
validation cost time: 832.3079s
epoch: 35, train time every whole data:1788.01s
epoch: 35, total time:80578.39s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 832.0577s
epoch: 36, train time every whole data:1758.24s
epoch: 36, total time:83168.69s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 716.2770s
epoch: 37, train time every whole data:1482.95s
epoch: 37, total time:85367.92s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 668.5743s
epoch: 38, train time every whole data:1475.16s
epoch: 38, total time:87511.65s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 668.4103s
epoch: 39, train time every whole data:1475.89s
epoch: 39, total time:89655.96s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 668.5658s
epoch: 40, train time every whole data:1474.37s
epoch: 40, total time:91798.90s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.4459s
epoch: 41, train time every whole data:1472.05s
epoch: 41, total time:93938.40s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.5851s
epoch: 42, train time every whole data:1472.12s
epoch: 42, total time:96078.10s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.4715s
epoch: 43, train time every whole data:1472.15s
epoch: 43, total time:98217.72s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.5992s
epoch: 44, train time every whole data:1471.52s
epoch: 44, total time:100356.85s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 668.2457s
epoch: 45, train time every whole data:1762.74s
epoch: 45, total time:102787.83s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.3437s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_46.params
epoch: 46, train time every whole data:1764.38s
epoch: 46, total time:105376.57s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.6387s
epoch: 47, train time every whole data:1764.66s
epoch: 47, total time:107965.87s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 822.9513s
epoch: 48, train time every whole data:1764.57s
epoch: 48, total time:110553.39s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.5888s
epoch: 49, train time every whole data:1764.52s
epoch: 49, total time:113142.51s
best epoch: 46
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_46.params
predicting testing set batch 1 / 2620, time: 0.32s
predicting testing set batch 101 / 2620, time: 31.88s
predicting testing set batch 201 / 2620, time: 63.47s
predicting testing set batch 301 / 2620, time: 95.06s
predicting testing set batch 401 / 2620, time: 126.64s
predicting testing set batch 501 / 2620, time: 158.23s
predicting testing set batch 601 / 2620, time: 189.81s
predicting testing set batch 701 / 2620, time: 221.39s
predicting testing set batch 801 / 2620, time: 252.97s
predicting testing set batch 901 / 2620, time: 284.55s
predicting testing set batch 1001 / 2620, time: 316.13s
predicting testing set batch 1101 / 2620, time: 347.72s
predicting testing set batch 1201 / 2620, time: 379.30s
predicting testing set batch 1301 / 2620, time: 410.86s
predicting testing set batch 1401 / 2620, time: 442.37s
predicting testing set batch 1501 / 2620, time: 473.94s
predicting testing set batch 1601 / 2620, time: 505.53s
predicting testing set batch 1701 / 2620, time: 537.12s
predicting testing set batch 1801 / 2620, time: 568.69s
predicting testing set batch 1901 / 2620, time: 600.23s
predicting testing set batch 2001 / 2620, time: 631.82s
predicting testing set batch 2101 / 2620, time: 663.41s
predicting testing set batch 2201 / 2620, time: 695.00s
predicting testing set batch 2301 / 2620, time: 726.60s
predicting testing set batch 2401 / 2620, time: 758.19s
predicting testing set batch 2501 / 2620, time: 789.75s
predicting testing set batch 2601 / 2620, time: 821.34s
test time on whole data:827.33s
input: (5240, 883, 36, 1)
prediction: (5240, 883, 12, 1)
data_target_tensor: (5240, 883, 12)
current epoch: 46, predict 0 points
MAE: 15.44
RMSE: 25.89
MAPE: 6.60
current epoch: 46, predict 1 points
MAE: 16.64
RMSE: 28.40
MAPE: 7.12
current epoch: 46, predict 2 points
MAE: 17.33
RMSE: 29.84
MAPE: 7.41
current epoch: 46, predict 3 points
MAE: 17.82
RMSE: 30.86
MAPE: 7.62
current epoch: 46, predict 4 points
MAE: 18.25
RMSE: 31.71
MAPE: 7.77
current epoch: 46, predict 5 points
MAE: 18.64
RMSE: 32.44
MAPE: 7.92
current epoch: 46, predict 6 points
MAE: 18.98
RMSE: 33.06
MAPE: 8.06
current epoch: 46, predict 7 points
MAE: 19.28
RMSE: 33.64
MAPE: 8.21
current epoch: 46, predict 8 points
MAE: 19.56
RMSE: 34.16
MAPE: 8.32
current epoch: 46, predict 9 points
MAE: 19.84
RMSE: 34.65
MAPE: 8.45
current epoch: 46, predict 10 points
MAE: 20.14
RMSE: 35.17
MAPE: 8.62
current epoch: 46, predict 11 points
MAE: 20.60
RMSE: 35.78
MAPE: 8.86
all MAE: 18.54
all RMSE: 32.26
all MAPE: 7.91
[15.436932, 25.890662718741385, 6.599324941635132, 16.638308, 28.39734888731913, 7.115247845649719, 17.32548, 29.839726904879885, 7.405059039592743, 17.816164, 30.86213397476854, 7.622227817773819, 18.246014, 31.710649750788576, 7.77081772685051, 18.63788, 32.43659868991064, 7.924812287092209, 18.976479, 33.06108589249036, 8.062567561864853, 19.284887, 33.63883602419234, 8.210410922765732, 19.556683, 34.15559954778645, 8.31732377409935, 19.836239, 34.64737790798082, 8.445120602846146, 20.144808, 35.16962245674525, 8.6235873401165, 20.595842, 35.77627217012171, 8.86329561471939, 18.54132, 32.25652489710415, 7.913299649953842]
fine tune the model ... 
epoch: 50, train time every whole data:3729.04s
epoch: 50, total time:117706.15s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 823.0508s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_50.params
epoch: 51, train time every whole data:3728.71s
epoch: 51, total time:122257.92s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.1693s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_51.params
epoch: 52, train time every whole data:3725.20s
epoch: 52, total time:126807.31s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.1742s
epoch: 53, train time every whole data:3724.86s
epoch: 53, total time:131356.34s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.5394s
epoch: 54, train time every whole data:3724.48s
epoch: 54, total time:135905.35s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.3648s
epoch: 55, train time every whole data:3724.99s
epoch: 55, total time:140454.71s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 823.9444s
epoch: 56, train time every whole data:3725.04s
epoch: 56, total time:145003.70s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 823.9625s
epoch: 57, train time every whole data:3724.74s
epoch: 57, total time:149552.41s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.0754s
epoch: 58, train time every whole data:3724.56s
epoch: 58, total time:154101.04s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 824.0806s
epoch: 59, train time every whole data:3728.82s
epoch: 59, total time:158653.94s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 827.5912s
epoch: 60, train time every whole data:3765.14s
epoch: 60, total time:163246.68s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 827.8008s
epoch: 61, train time every whole data:3764.85s
epoch: 61, total time:167839.33s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 671.2059s
epoch: 62, train time every whole data:3844.75s
epoch: 62, total time:172355.29s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 772.6173s
epoch: 63, train time every whole data:3129.37s
epoch: 63, total time:176257.28s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 671.5757s
epoch: 64, train time every whole data:3129.20s
epoch: 64, total time:180058.06s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 671.0894s
epoch: 65, train time every whole data:3083.40s
epoch: 65, total time:183812.56s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.9447s
epoch: 66, train time every whole data:3083.64s
epoch: 66, total time:187564.14s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.6105s
epoch: 67, train time every whole data:3082.72s
epoch: 67, total time:191314.47s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 668.2572s
epoch: 68, train time every whole data:3082.93s
epoch: 68, total time:195065.66s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 668.0389s
epoch: 69, train time every whole data:3082.89s
epoch: 69, total time:198816.59s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.8514s
epoch: 70, train time every whole data:3083.12s
epoch: 70, total time:202567.55s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 668.0201s
epoch: 71, train time every whole data:3082.66s
epoch: 71, total time:206318.24s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.5432s
epoch: 72, train time every whole data:3082.75s
epoch: 72, total time:210068.54s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.7205s
epoch: 73, train time every whole data:3083.57s
epoch: 73, total time:213819.83s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.5073s
epoch: 74, train time every whole data:3083.12s
epoch: 74, total time:217570.46s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 667.8891s
best epoch: 51
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE1TE/epoch_51.params
predicting testing set batch 1 / 2620, time: 0.26s
predicting testing set batch 101 / 2620, time: 25.77s
predicting testing set batch 201 / 2620, time: 51.29s
predicting testing set batch 301 / 2620, time: 76.82s
predicting testing set batch 401 / 2620, time: 102.35s
predicting testing set batch 501 / 2620, time: 127.88s
predicting testing set batch 601 / 2620, time: 153.42s
predicting testing set batch 701 / 2620, time: 178.95s
predicting testing set batch 801 / 2620, time: 204.49s
predicting testing set batch 901 / 2620, time: 230.03s
predicting testing set batch 1001 / 2620, time: 255.56s
predicting testing set batch 1101 / 2620, time: 281.09s
predicting testing set batch 1201 / 2620, time: 306.62s
predicting testing set batch 1301 / 2620, time: 332.15s
predicting testing set batch 1401 / 2620, time: 357.69s
predicting testing set batch 1501 / 2620, time: 383.24s
predicting testing set batch 1601 / 2620, time: 408.79s
predicting testing set batch 1701 / 2620, time: 434.32s
predicting testing set batch 1801 / 2620, time: 459.86s
predicting testing set batch 1901 / 2620, time: 485.39s
predicting testing set batch 2001 / 2620, time: 510.93s
predicting testing set batch 2101 / 2620, time: 536.48s
predicting testing set batch 2201 / 2620, time: 562.04s
predicting testing set batch 2301 / 2620, time: 587.60s
predicting testing set batch 2401 / 2620, time: 613.17s
predicting testing set batch 2501 / 2620, time: 638.74s
predicting testing set batch 2601 / 2620, time: 664.31s
test time on whole data:669.17s
input: (5240, 883, 36, 1)
prediction: (5240, 883, 12, 1)
data_target_tensor: (5240, 883, 12)
current epoch: 51, predict 0 points
MAE: 15.34
RMSE: 25.78
MAPE: 6.47
current epoch: 51, predict 1 points
MAE: 16.51
RMSE: 28.20
MAPE: 7.03
current epoch: 51, predict 2 points
MAE: 17.16
RMSE: 29.54
MAPE: 7.28
current epoch: 51, predict 3 points
MAE: 17.61
RMSE: 30.45
MAPE: 7.46
current epoch: 51, predict 4 points
MAE: 17.95
RMSE: 31.18
MAPE: 7.59
current epoch: 51, predict 5 points
MAE: 18.26
RMSE: 31.79
MAPE: 7.73
current epoch: 51, predict 6 points
MAE: 18.53
RMSE: 32.30
MAPE: 7.82
current epoch: 51, predict 7 points
MAE: 18.79
RMSE: 32.78
MAPE: 7.96
current epoch: 51, predict 8 points
MAE: 19.02
RMSE: 33.21
MAPE: 8.07
current epoch: 51, predict 9 points
MAE: 19.26
RMSE: 33.63
MAPE: 8.18
current epoch: 51, predict 10 points
MAE: 19.52
RMSE: 34.06
MAPE: 8.30
current epoch: 51, predict 11 points
MAE: 19.88
RMSE: 34.59
MAPE: 8.47
all MAE: 18.15
all RMSE: 31.56
all MAPE: 7.70
[15.3421335, 25.776176110564045, 6.4683698117733, 16.5136, 28.197890572077263, 7.032541185617447, 17.162203, 29.543267024228104, 7.280924171209335, 17.60608, 30.451902393364524, 7.45571106672287, 17.954597, 31.17611871958998, 7.589942961931229, 18.263784, 31.786791053033628, 7.730200886726379, 18.531809, 32.30062606724423, 7.821275293827057, 18.786629, 32.77588544939995, 7.957054674625397, 19.02308, 33.21275044304804, 8.067768812179565, 19.257458, 33.628270780745254, 8.180877566337585, 19.519655, 34.06181729533728, 8.300407230854034, 19.875097, 34.591939739220685, 8.466754853725433, 18.15299, 31.557285112194677, 7.6959580183029175]
