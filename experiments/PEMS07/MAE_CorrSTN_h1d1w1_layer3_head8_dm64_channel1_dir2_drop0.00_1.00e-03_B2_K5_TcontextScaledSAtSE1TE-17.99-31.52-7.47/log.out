Tue Aug 24 18:12:52 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   30C    P0    36W / 250W |   3380MiB / 32480MiB |     11%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   28C    P0    34W / 250W |   3380MiB / 32480MiB |     13%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   27C    P0    34W / 250W |   3380MiB / 32480MiB |      8%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   25C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   51C    P0   134W / 250W |   4654MiB / 32480MiB |     90%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   69C    P0   125W / 250W |   7465MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   29C    P0    24W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   38C    P0    58W / 250W |   1476MiB / 32480MiB |     32%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     53574      C   python                                      3369MiB |
|    1     53708      C   python                                      3369MiB |
|    2     53833      C   python                                      3369MiB |
|    4      4829      C   python                                      4643MiB |
|    5     25909      C   python                                      1895MiB |
|    5     39145      C   python                                      5559MiB |
|    7     16340      C   python                                      1465MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u9272/ICASTGNN/configurations/PEMS07_rdw.conf
total training epoch, fine tune epoch: 60 , 40
batch_size: 2
folder_dir: MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE
load file: /data/home/u9272/ICASTGNN/data/PEMS07/PEMS07_r1_d1_w1.npz
ori length: 15718 , percent: 1.0 , scale: 15718
train: torch.Size([15718, 883, 1, 36]) torch.Size([15718, 883, 12]) torch.Size([15718, 883, 12])
val: torch.Size([5239, 883, 1, 36]) torch.Size([5239, 883, 12]) torch.Size([5239, 883, 12])
test: torch.Size([5240, 883, 1, 36]) torch.Size([5240, 883, 12]) torch.Size([5240, 883, 12])
TemporalPositionalEncoding max_len: 2016
w_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
d_index: [1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739]
h_index: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 2016, 64])
src_embed.2.embedding.weight 	 torch.Size([883, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([883, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 575313
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]}]
validation batch 1 / 2620, loss: 1.28
validation batch 101 / 2620, loss: 1.58
validation batch 201 / 2620, loss: 1.57
validation batch 301 / 2620, loss: 1.28
validation batch 401 / 2620, loss: 1.46
validation batch 501 / 2620, loss: 1.67
validation batch 601 / 2620, loss: 1.29
validation batch 701 / 2620, loss: 1.32
validation batch 801 / 2620, loss: 1.65
validation batch 901 / 2620, loss: 1.38
validation batch 1001 / 2620, loss: 1.28
validation batch 1101 / 2620, loss: 1.64
validation batch 1201 / 2620, loss: 1.47
validation batch 1301 / 2620, loss: 1.29
validation batch 1401 / 2620, loss: 1.29
validation batch 1501 / 2620, loss: 1.63
validation batch 1601 / 2620, loss: 1.27
validation batch 1701 / 2620, loss: 1.32
validation batch 1801 / 2620, loss: 1.66
validation batch 1901 / 2620, loss: 1.30
validation batch 2001 / 2620, loss: 1.29
validation batch 2101 / 2620, loss: 1.68
validation batch 2201 / 2620, loss: 1.40
validation batch 2301 / 2620, loss: 1.31
validation batch 2401 / 2620, loss: 1.33
validation batch 2501 / 2620, loss: 1.54
validation batch 2601 / 2620, loss: 1.29
validation cost time: 716.3646s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:1668.51s
epoch: 0, total time:2384.89s
validation batch 1 / 2620, loss: 0.14
validation batch 101 / 2620, loss: 0.12
validation batch 201 / 2620, loss: 0.13
validation batch 301 / 2620, loss: 0.14
validation batch 401 / 2620, loss: 0.14
validation batch 501 / 2620, loss: 0.11
validation batch 601 / 2620, loss: 0.13
validation batch 701 / 2620, loss: 0.14
validation batch 801 / 2620, loss: 0.09
validation batch 901 / 2620, loss: 0.14
validation batch 1001 / 2620, loss: 0.14
validation batch 1101 / 2620, loss: 0.09
validation batch 1201 / 2620, loss: 0.15
validation batch 1301 / 2620, loss: 0.13
validation batch 1401 / 2620, loss: 0.09
validation batch 1501 / 2620, loss: 0.11
validation batch 1601 / 2620, loss: 0.14
validation batch 1701 / 2620, loss: 0.13
validation batch 1801 / 2620, loss: 0.10
validation batch 1901 / 2620, loss: 0.14
validation batch 2001 / 2620, loss: 0.13
validation batch 2101 / 2620, loss: 0.11
validation batch 2201 / 2620, loss: 0.13
validation batch 2301 / 2620, loss: 0.13
validation batch 2401 / 2620, loss: 0.14
validation batch 2501 / 2620, loss: 0.12
validation batch 2601 / 2620, loss: 0.14
validation cost time: 716.1197s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:1670.26s
epoch: 1, total time:4771.28s
validation batch 1 / 2620, loss: 0.05
validation batch 101 / 2620, loss: 0.05
validation batch 201 / 2620, loss: 0.04
validation batch 301 / 2620, loss: 0.05
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.04
validation batch 601 / 2620, loss: 0.06
validation batch 701 / 2620, loss: 0.05
validation batch 801 / 2620, loss: 0.05
validation batch 901 / 2620, loss: 0.05
validation batch 1001 / 2620, loss: 0.05
validation batch 1101 / 2620, loss: 0.05
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.06
validation batch 1401 / 2620, loss: 0.10
validation batch 1501 / 2620, loss: 0.04
validation batch 1601 / 2620, loss: 0.05
validation batch 1701 / 2620, loss: 0.06
validation batch 1801 / 2620, loss: 0.05
validation batch 1901 / 2620, loss: 0.06
validation batch 2001 / 2620, loss: 0.05
validation batch 2101 / 2620, loss: 0.04
validation batch 2201 / 2620, loss: 0.05
validation batch 2301 / 2620, loss: 0.06
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.05
validation batch 2601 / 2620, loss: 0.05
validation cost time: 716.5288s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_2.params
epoch: 2, train time every whole data:1670.49s
epoch: 2, total time:7158.32s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.05
validation batch 201 / 2620, loss: 0.04
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.04
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.04
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.04
validation batch 1201 / 2620, loss: 0.05
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.04
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.05
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.04
validation cost time: 716.0760s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_3.params
epoch: 3, train time every whole data:1669.83s
epoch: 3, total time:9544.25s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 715.1579s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_4.params
epoch: 4, train time every whole data:1669.38s
epoch: 4, total time:11928.80s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.9119s
epoch: 5, train time every whole data:1669.51s
epoch: 5, total time:14313.23s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 715.0336s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_6.params
epoch: 6, train time every whole data:1669.44s
epoch: 6, total time:16697.72s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.1433s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_7.params
epoch: 7, train time every whole data:1669.67s
epoch: 7, total time:19082.55s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 715.2459s
epoch: 8, train time every whole data:1669.66s
epoch: 8, total time:21467.46s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.1861s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_9.params
epoch: 9, train time every whole data:1669.67s
epoch: 9, total time:23852.33s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.9328s
epoch: 10, train time every whole data:1669.33s
epoch: 10, total time:26236.60s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.1632s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_11.params
epoch: 11, train time every whole data:1669.53s
epoch: 11, total time:28621.31s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 714.8957s
epoch: 12, train time every whole data:1668.67s
epoch: 12, total time:31004.88s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.0513s
epoch: 13, train time every whole data:1668.80s
epoch: 13, total time:33388.73s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.9745s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_14.params
epoch: 14, train time every whole data:1668.90s
epoch: 14, total time:35772.62s
validation batch 1 / 2620, loss: 0.05
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.05
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.05
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.05
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.06
validation batch 1301 / 2620, loss: 0.05
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.05
validation batch 1701 / 2620, loss: 0.05
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.05
validation batch 2001 / 2620, loss: 0.05
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.05
validation batch 2301 / 2620, loss: 0.05
validation batch 2401 / 2620, loss: 0.05
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.05
validation cost time: 715.0368s
epoch: 15, train time every whole data:1668.62s
epoch: 15, total time:38156.28s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.1038s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_16.params
epoch: 16, train time every whole data:1668.22s
epoch: 16, total time:40539.61s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.9798s
epoch: 17, train time every whole data:1668.16s
epoch: 17, total time:42922.75s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.7726s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_18.params
epoch: 18, train time every whole data:1668.04s
epoch: 18, total time:45305.57s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.8132s
epoch: 19, train time every whole data:1667.78s
epoch: 19, total time:47688.16s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.8886s
epoch: 20, train time every whole data:1667.76s
epoch: 20, total time:50070.81s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.8553s
epoch: 21, train time every whole data:1667.75s
epoch: 21, total time:52453.42s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.1036s
epoch: 22, train time every whole data:1667.69s
epoch: 22, total time:54836.21s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.8026s
epoch: 23, train time every whole data:1667.69s
epoch: 23, total time:57218.70s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.9595s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_24.params
epoch: 24, train time every whole data:1667.71s
epoch: 24, total time:59601.40s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.0042s
epoch: 25, train time every whole data:1667.72s
epoch: 25, total time:61984.13s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.0317s
epoch: 26, train time every whole data:1667.67s
epoch: 26, total time:64366.84s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.9970s
epoch: 27, train time every whole data:1667.62s
epoch: 27, total time:66749.46s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.9462s
epoch: 28, train time every whole data:1667.68s
epoch: 28, total time:69132.09s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.04
validation cost time: 714.8660s
epoch: 29, train time every whole data:1667.76s
epoch: 29, total time:71514.72s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.8510s
epoch: 30, train time every whole data:1667.68s
epoch: 30, total time:73897.25s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 715.0742s
epoch: 31, train time every whole data:1667.70s
epoch: 31, total time:76280.03s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.9132s
epoch: 32, train time every whole data:1667.76s
epoch: 32, total time:78662.70s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.8892s
epoch: 33, train time every whole data:1676.36s
epoch: 33, total time:81058.95s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.8242s
epoch: 34, train time every whole data:1676.49s
epoch: 34, total time:83455.27s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.1906s
epoch: 35, train time every whole data:1676.42s
epoch: 35, total time:85851.88s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.2231s
epoch: 36, train time every whole data:1676.49s
epoch: 36, total time:88248.60s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.9864s
epoch: 37, train time every whole data:1676.46s
epoch: 37, total time:90645.05s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.2985s
epoch: 38, train time every whole data:1676.51s
epoch: 38, total time:93041.86s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.9635s
epoch: 39, train time every whole data:1676.45s
epoch: 39, total time:95438.28s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.1826s
epoch: 40, train time every whole data:1676.42s
epoch: 40, total time:97834.89s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.9517s
epoch: 41, train time every whole data:1676.51s
epoch: 41, total time:100231.35s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.2307s
epoch: 42, train time every whole data:1676.34s
epoch: 42, total time:102627.92s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.7936s
epoch: 43, train time every whole data:1676.44s
epoch: 43, total time:105024.16s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.04
validation cost time: 720.1527s
epoch: 44, train time every whole data:1676.33s
epoch: 44, total time:107420.64s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.8419s
epoch: 45, train time every whole data:1676.58s
epoch: 45, total time:109817.06s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.0284s
epoch: 46, train time every whole data:1676.45s
epoch: 46, total time:112213.54s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.0999s
epoch: 47, train time every whole data:1676.53s
epoch: 47, total time:114610.17s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.8572s
epoch: 48, train time every whole data:1676.42s
epoch: 48, total time:117006.45s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.0169s
epoch: 49, train time every whole data:1676.57s
epoch: 49, total time:119403.04s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.8703s
epoch: 50, train time every whole data:1676.44s
epoch: 50, total time:121799.35s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.6160s
epoch: 51, train time every whole data:1676.33s
epoch: 51, total time:124196.30s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.0823s
epoch: 52, train time every whole data:1676.76s
epoch: 52, total time:126594.14s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.6116s
epoch: 53, train time every whole data:1676.50s
epoch: 53, total time:128991.25s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.9904s
epoch: 54, train time every whole data:1676.51s
epoch: 54, total time:131388.75s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.2132s
epoch: 55, train time every whole data:1676.87s
epoch: 55, total time:133785.84s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.0488s
epoch: 56, train time every whole data:1676.29s
epoch: 56, total time:136183.18s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.9108s
epoch: 57, train time every whole data:1676.83s
epoch: 57, total time:138580.92s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 719.7324s
epoch: 58, train time every whole data:1676.78s
epoch: 58, total time:140977.44s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.0999s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_59.params
epoch: 59, train time every whole data:1676.68s
epoch: 59, total time:143375.23s
best epoch: 59
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_59.params
predicting testing set batch 1 / 2620, time: 0.28s
predicting testing set batch 101 / 2620, time: 27.83s
predicting testing set batch 201 / 2620, time: 55.42s
predicting testing set batch 301 / 2620, time: 82.92s
predicting testing set batch 401 / 2620, time: 110.51s
predicting testing set batch 501 / 2620, time: 138.11s
predicting testing set batch 601 / 2620, time: 165.70s
predicting testing set batch 701 / 2620, time: 193.29s
predicting testing set batch 801 / 2620, time: 220.90s
predicting testing set batch 901 / 2620, time: 248.49s
predicting testing set batch 1001 / 2620, time: 276.08s
predicting testing set batch 1101 / 2620, time: 303.69s
predicting testing set batch 1201 / 2620, time: 331.29s
predicting testing set batch 1301 / 2620, time: 358.89s
predicting testing set batch 1401 / 2620, time: 386.46s
predicting testing set batch 1501 / 2620, time: 414.08s
predicting testing set batch 1601 / 2620, time: 441.71s
predicting testing set batch 1701 / 2620, time: 469.34s
predicting testing set batch 1801 / 2620, time: 496.97s
predicting testing set batch 1901 / 2620, time: 524.59s
predicting testing set batch 2001 / 2620, time: 552.23s
predicting testing set batch 2101 / 2620, time: 579.86s
predicting testing set batch 2201 / 2620, time: 607.51s
predicting testing set batch 2301 / 2620, time: 635.15s
predicting testing set batch 2401 / 2620, time: 662.78s
predicting testing set batch 2501 / 2620, time: 690.43s
predicting testing set batch 2601 / 2620, time: 718.07s
test time on whole data:723.32s
input: (5240, 883, 36, 1)
prediction: (5240, 883, 12, 1)
data_target_tensor: (5240, 883, 12)
current epoch: 59, predict 0 points
MAE: 15.18
RMSE: 25.52
MAPE: 6.58
current epoch: 59, predict 1 points
MAE: 16.48
RMSE: 28.01
MAPE: 7.24
current epoch: 59, predict 2 points
MAE: 17.26
RMSE: 29.50
MAPE: 7.69
current epoch: 59, predict 3 points
MAE: 17.80
RMSE: 30.52
MAPE: 7.96
current epoch: 59, predict 4 points
MAE: 18.21
RMSE: 31.30
MAPE: 8.15
current epoch: 59, predict 5 points
MAE: 18.58
RMSE: 31.96
MAPE: 8.32
current epoch: 59, predict 6 points
MAE: 18.93
RMSE: 32.55
MAPE: 8.48
current epoch: 59, predict 7 points
MAE: 19.27
RMSE: 33.08
MAPE: 8.65
current epoch: 59, predict 8 points
MAE: 19.58
RMSE: 33.59
MAPE: 8.86
current epoch: 59, predict 9 points
MAE: 19.89
RMSE: 34.04
MAPE: 9.11
current epoch: 59, predict 10 points
MAE: 20.20
RMSE: 34.49
MAPE: 9.37
current epoch: 59, predict 11 points
MAE: 20.66
RMSE: 35.00
MAPE: 9.92
all MAE: 18.50
all RMSE: 31.75
all MAPE: 8.36
[15.180911, 25.523404021251384, 6.576882302761078, 16.475685, 28.014011691905512, 7.243992388248444, 17.262455, 29.496493890892673, 7.693401724100113, 17.7966, 30.51751406243276, 7.956208288669586, 18.208502, 31.30043976203649, 8.145150542259216, 18.581541, 31.95852166376902, 8.324529975652695, 18.934965, 32.54582669039054, 8.483878523111343, 19.265028, 33.08323022424371, 8.649752289056778, 19.580477, 33.59108092594577, 8.857494592666626, 19.88593, 34.04402272644109, 9.105225652456284, 20.198126, 34.48596790611908, 9.365909546613693, 20.662859, 35.002104777672834, 9.917616844177246, 18.502747, 31.746640485709765, 8.359958976507187]
fine tune the model ... 
epoch: 60, train time every whole data:3375.52s
epoch: 60, total time:147481.60s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 722.3204s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_60.params
epoch: 61, train time every whole data:3375.93s
epoch: 61, total time:151579.87s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 722.2345s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_61.params
epoch: 62, train time every whole data:3375.76s
epoch: 62, total time:155677.88s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 722.0399s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_62.params
epoch: 63, train time every whole data:3375.36s
epoch: 63, total time:159775.30s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.8407s
epoch: 64, train time every whole data:3375.11s
epoch: 64, total time:163872.25s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 722.3918s
epoch: 65, train time every whole data:3375.79s
epoch: 65, total time:167970.44s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.7895s
epoch: 66, train time every whole data:3375.54s
epoch: 66, total time:172067.76s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.9976s
epoch: 67, train time every whole data:3375.72s
epoch: 67, total time:176164.48s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.0748s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_67.params
epoch: 68, train time every whole data:3374.89s
epoch: 68, total time:180260.46s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.3459s
epoch: 69, train time every whole data:3375.39s
epoch: 69, total time:184357.20s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.9728s
epoch: 70, train time every whole data:3375.16s
epoch: 70, total time:188453.34s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 720.8080s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_70.params
epoch: 71, train time every whole data:3374.06s
epoch: 71, total time:192548.22s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.6889s
epoch: 72, train time every whole data:3373.81s
epoch: 72, total time:196643.72s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.5452s
epoch: 73, train time every whole data:3374.00s
epoch: 73, total time:200739.27s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.7065s
epoch: 74, train time every whole data:3373.99s
epoch: 74, total time:204834.97s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.5264s
epoch: 75, train time every whole data:3373.97s
epoch: 75, total time:208930.46s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.6012s
epoch: 76, train time every whole data:3374.24s
epoch: 76, total time:213026.31s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.0002s
epoch: 77, train time every whole data:3374.59s
epoch: 77, total time:217121.90s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.1631s
epoch: 78, train time every whole data:3374.65s
epoch: 78, total time:221217.72s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.3709s
epoch: 79, train time every whole data:3374.46s
epoch: 79, total time:225313.56s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 721.3847s
epoch: 80, train time every whole data:5433.95s
epoch: 80, total time:231468.89s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1223.4687s
epoch: 81, train time every whole data:5573.63s
epoch: 81, total time:238265.99s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1224.1936s
epoch: 82, train time every whole data:5574.13s
epoch: 82, total time:245064.31s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1225.5462s
epoch: 83, train time every whole data:5574.33s
epoch: 83, total time:251864.19s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1224.8674s
epoch: 84, train time every whole data:5573.66s
epoch: 84, total time:258662.72s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1224.6252s
epoch: 85, train time every whole data:5574.32s
epoch: 85, total time:265461.67s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1216.0949s
epoch: 86, train time every whole data:5574.42s
epoch: 86, total time:272252.18s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1223.9747s
epoch: 87, train time every whole data:5573.98s
epoch: 87, total time:279050.14s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1223.9672s
epoch: 88, train time every whole data:5575.14s
epoch: 88, total time:285849.24s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1223.5813s
epoch: 89, train time every whole data:5574.30s
epoch: 89, total time:292647.13s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1223.5916s
epoch: 90, train time every whole data:5574.80s
epoch: 90, total time:299445.52s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1222.8769s
epoch: 91, train time every whole data:5567.07s
epoch: 91, total time:306235.47s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1223.2468s
epoch: 92, train time every whole data:3689.30s
epoch: 92, total time:311148.02s
C:\Users\developer\anaconda3\envs\ast\python.exe C:/Users/developer/Documents/GitHub/ASTGNN-0717/train_ASTGNN.py --config configurations/PEMS07_rdw.conf
CUDA: True cuda:0
Read configuration file: configurations/PEMS07_rdw.conf
total training epoch, fine tune epoch: 50 , 25
batch_size: 16
folder_dir: MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_16TcontextScaledSAtSE1TE
load file: data/PEMS07\PEMS07_r1_d1_w1.npz
ori length: 15718 , percent: 1.0 , scale: 15718
train: torch.Size([15718, 883, 1, 36]) torch.Size([15718, 883, 12]) torch.Size([15718, 883, 12])
val: torch.Size([5239, 883, 1, 36]) torch.Size([5239, 883, 12]) torch.Size([5239, 883, 12])
test: torch.Size([5240, 883, 1, 36]) torch.Size([5240, 883, 12]) torch.Size([5240, 883, 12])
TemporalPositionalEncoding max_len: 2016
w_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
d_index: [1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739]
h_index: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
load weight from: ../experiments\PEMS07\MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_16TcontextScaledSAtSE1TE\epoch_70.params
predicting testing set batch 1 / 328, time: 3.58s
predicting testing set batch 101 / 328, time: 165.48s
predicting testing set batch 201 / 328, time: 328.10s
predicting testing set batch 301 / 328, time: 491.64s
test time on whole data:535.29s
input: (5240, 883, 36, 1)
prediction: (5240, 883, 12, 1)
data_target_tensor: (5240, 883, 12)
current epoch: 70, predict 0 points
MAE: 15.04
RMSE: 25.46
MAPE: 6.32
current epoch: 70, predict 1 points
MAE: 16.26
RMSE: 27.92
MAPE: 6.81
current epoch: 70, predict 2 points
MAE: 16.97
RMSE: 29.36
MAPE: 7.09
current epoch: 70, predict 3 points
MAE: 17.46
RMSE: 30.34
MAPE: 7.26
current epoch: 70, predict 4 points
MAE: 17.83
RMSE: 31.11
MAPE: 7.40
current epoch: 70, predict 5 points
MAE: 18.15
RMSE: 31.75
MAPE: 7.52
current epoch: 70, predict 6 points
MAE: 18.43
RMSE: 32.32
MAPE: 7.62
current epoch: 70, predict 7 points
MAE: 18.68
RMSE: 32.82
MAPE: 7.72
current epoch: 70, predict 8 points
MAE: 18.92
RMSE: 33.30
MAPE: 7.82
current epoch: 70, predict 9 points
MAE: 19.14
RMSE: 33.75
MAPE: 7.91
current epoch: 70, predict 10 points
MAE: 19.37
RMSE: 34.17
MAPE: 8.01
current epoch: 70, predict 11 points
MAE: 19.66
RMSE: 34.64
MAPE: 8.14
all MAE: 17.99
all RMSE: 31.52
all MAPE: 7.47
[15.036142, 25.462004235094472, 6.318674236536026, 16.260681, 27.92153617939363, 6.813472509384155, 16.973213, 29.36058073628441, 7.09102600812912, 17.458624, 30.33791621315231, 7.256040722131729, 17.832935, 31.11038244695963, 7.397252321243286, 18.150503, 31.747624826574132, 7.517204433679581, 18.434128, 32.318083112922245, 7.624264806509018, 18.682642, 32.815284480068314, 7.718950510025024, 18.915262, 33.301800066892966, 7.8190043568611145, 19.14208, 33.74752233382289, 7.906559854745865, 19.366976, 34.16886882353128, 8.009732514619827, 19.655277, 34.64479881542015, 8.13850462436676, 17.992325, 31.52127954974425, 7.467518746852875]

Process finished with exit code 0
