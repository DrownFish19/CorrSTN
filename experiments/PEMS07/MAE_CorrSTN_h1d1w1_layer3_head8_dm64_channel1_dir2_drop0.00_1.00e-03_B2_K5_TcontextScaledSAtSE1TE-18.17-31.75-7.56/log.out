Tue Aug 24 18:19:22 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   31C    P0    39W / 250W |   1030MiB / 32480MiB |     29%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   27C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   27C    P0    24W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   55C    P0   155W / 250W |   5406MiB / 32480MiB |     96%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   60C    P0   146W / 250W |   9978MiB / 32480MiB |     99%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   63C    P0   138W / 250W |   5729MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   54C    P0    58W / 250W |  11008MiB / 32480MiB |     84%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   39C    P0    84W / 250W |   1477MiB / 32480MiB |     50%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     46453      C   python                                      1019MiB |
|    3     11857      C   python                                      5395MiB |
|    4      9029      C   python                                      9967MiB |
|    5     49212      C   python                                      1643MiB |
|    5     50876      C   python                                      4075MiB |
|    6     54094      C   python                                     10997MiB |
|    7     51054      C   python                                      1465MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u9272/ICASTGNN/configurations/PEMS07_rdw.conf
total training epoch, fine tune epoch: 60 , 40
batch_size: 2
folder_dir: MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE
load file: /data/home/u9272/ICASTGNN/data/PEMS07/PEMS07_r1_d1_w1.npz
ori length: 15718 , percent: 1.0 , scale: 15718
train: torch.Size([15718, 883, 1, 36]) torch.Size([15718, 883, 12]) torch.Size([15718, 883, 12])
val: torch.Size([5239, 883, 1, 36]) torch.Size([5239, 883, 12]) torch.Size([5239, 883, 12])
test: torch.Size([5240, 883, 1, 36]) torch.Size([5240, 883, 12]) torch.Size([5240, 883, 12])
TemporalPositionalEncoding max_len: 2016
w_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
d_index: [1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739]
h_index: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 2016, 64])
src_embed.2.embedding.weight 	 torch.Size([883, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([883, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 575313
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137]}]
validation batch 1 / 2620, loss: 0.84
validation batch 101 / 2620, loss: 0.42
validation batch 201 / 2620, loss: 0.47
validation batch 301 / 2620, loss: 0.84
validation batch 401 / 2620, loss: 0.59
validation batch 501 / 2620, loss: 0.31
validation batch 601 / 2620, loss: 0.83
validation batch 701 / 2620, loss: 0.79
validation batch 801 / 2620, loss: 0.31
validation batch 901 / 2620, loss: 0.72
validation batch 1001 / 2620, loss: 0.84
validation batch 1101 / 2620, loss: 0.32
validation batch 1201 / 2620, loss: 0.60
validation batch 1301 / 2620, loss: 0.83
validation batch 1401 / 2620, loss: 0.84
validation batch 1501 / 2620, loss: 0.37
validation batch 1601 / 2620, loss: 0.86
validation batch 1701 / 2620, loss: 0.80
validation batch 1801 / 2620, loss: 0.30
validation batch 1901 / 2620, loss: 0.82
validation batch 2001 / 2620, loss: 0.84
validation batch 2101 / 2620, loss: 0.28
validation batch 2201 / 2620, loss: 0.69
validation batch 2301 / 2620, loss: 0.81
validation batch 2401 / 2620, loss: 0.76
validation batch 2501 / 2620, loss: 0.49
validation batch 2601 / 2620, loss: 0.84
validation cost time: 714.2589s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:1663.19s
epoch: 0, total time:2377.47s
validation batch 1 / 2620, loss: 0.05
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.05
validation batch 701 / 2620, loss: 0.05
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.05
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.05
validation batch 1401 / 2620, loss: 0.10
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.05
validation batch 1701 / 2620, loss: 0.05
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.05
validation batch 2001 / 2620, loss: 0.05
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.05
validation batch 2401 / 2620, loss: 0.05
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.05
validation cost time: 714.2438s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:1663.86s
epoch: 1, total time:4755.59s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.05
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.07
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.04
validation cost time: 714.2161s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_2.params
epoch: 2, train time every whole data:1664.02s
epoch: 2, total time:7133.84s
validation batch 1 / 2620, loss: 0.06
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.05
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.06
validation batch 701 / 2620, loss: 0.05
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.06
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.06
validation batch 1401 / 2620, loss: 0.08
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.06
validation batch 1701 / 2620, loss: 0.06
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.06
validation batch 2001 / 2620, loss: 0.06
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.06
validation batch 2401 / 2620, loss: 0.05
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.06
validation cost time: 713.9016s
epoch: 3, train time every whole data:1663.15s
epoch: 3, total time:9510.89s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.05
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.05
validation batch 1401 / 2620, loss: 0.07
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.05
validation batch 1701 / 2620, loss: 0.05
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.05
validation batch 2001 / 2620, loss: 0.05
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.05
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.04
validation cost time: 712.9770s
epoch: 4, train time every whole data:1663.15s
epoch: 4, total time:11887.02s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.03
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 713.7061s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_5.params
epoch: 5, train time every whole data:1663.03s
epoch: 5, total time:14263.77s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.05
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.05
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.05
validation batch 1301 / 2620, loss: 0.05
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.05
validation cost time: 712.8893s
epoch: 6, train time every whole data:1663.16s
epoch: 6, total time:16639.82s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4862s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_7.params
epoch: 7, train time every whole data:1663.04s
epoch: 7, total time:19016.36s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.04
validation cost time: 712.8550s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_8.params
epoch: 8, train time every whole data:1662.98s
epoch: 8, total time:21392.21s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 713.3875s
epoch: 9, train time every whole data:1663.25s
epoch: 9, total time:23768.85s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4421s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_10.params
epoch: 10, train time every whole data:1663.23s
epoch: 10, total time:26145.53s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.04
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.03
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.06
validation batch 1501 / 2620, loss: 0.04
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.03
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.04
validation cost time: 712.9912s
epoch: 11, train time every whole data:1663.34s
epoch: 11, total time:28521.87s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.2953s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_12.params
epoch: 12, train time every whole data:1663.29s
epoch: 12, total time:30898.46s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.04
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.03
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.04
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.04
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.04
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.03
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.04
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.5053s
epoch: 13, train time every whole data:1663.37s
epoch: 13, total time:33275.34s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 712.8036s
epoch: 14, train time every whole data:1663.43s
epoch: 14, total time:35651.57s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.9574s
epoch: 15, train time every whole data:1663.46s
epoch: 15, total time:38027.99s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.04
validation cost time: 713.5233s
epoch: 16, train time every whole data:1663.48s
epoch: 16, total time:40405.00s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.5972s
epoch: 17, train time every whole data:1663.04s
epoch: 17, total time:42781.63s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.03
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.03
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 713.4720s
epoch: 18, train time every whole data:1662.53s
epoch: 18, total time:45157.63s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4288s
epoch: 19, train time every whole data:1662.62s
epoch: 19, total time:47533.68s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.3619s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_20.params
epoch: 20, train time every whole data:1662.04s
epoch: 20, total time:49909.10s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.5059s
epoch: 21, train time every whole data:1662.12s
epoch: 21, total time:52284.72s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4819s
epoch: 22, train time every whole data:1662.18s
epoch: 22, total time:54660.38s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.8755s
epoch: 23, train time every whole data:1662.06s
epoch: 23, total time:57035.33s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.9064s
epoch: 24, train time every whole data:1662.15s
epoch: 24, total time:59410.38s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.04
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.04
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 712.8530s
epoch: 25, train time every whole data:1662.09s
epoch: 25, total time:61785.33s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.04
validation cost time: 713.4732s
epoch: 26, train time every whole data:1661.74s
epoch: 26, total time:64160.54s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.6296s
epoch: 27, train time every whole data:1662.55s
epoch: 27, total time:66536.73s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.9212s
epoch: 28, train time every whole data:1662.50s
epoch: 28, total time:68912.15s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.5741s
epoch: 29, train time every whole data:1662.50s
epoch: 29, total time:71288.22s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.6130s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_30.params
epoch: 30, train time every whole data:1662.52s
epoch: 30, total time:73664.36s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4581s
epoch: 31, train time every whole data:1662.22s
epoch: 31, total time:76040.05s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7298s
epoch: 32, train time every whole data:1662.78s
epoch: 32, total time:78415.56s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.5440s
epoch: 33, train time every whole data:1666.05s
epoch: 33, total time:80795.15s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.7618s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_34.params
epoch: 34, train time every whole data:1666.09s
epoch: 34, total time:83175.02s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.6887s
epoch: 35, train time every whole data:1665.96s
epoch: 35, total time:85554.68s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.3280s
epoch: 36, train time every whole data:1666.02s
epoch: 36, total time:87935.03s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.8527s
epoch: 37, train time every whole data:1666.16s
epoch: 37, total time:90315.04s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.6835s
epoch: 38, train time every whole data:1666.15s
epoch: 38, total time:92694.88s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.5618s
epoch: 39, train time every whole data:1661.81s
epoch: 39, total time:95070.25s
validation batch 1 / 2620, loss: 0.04
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.05
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.05
validation batch 701 / 2620, loss: 0.04
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.05
validation batch 1001 / 2620, loss: 0.04
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.05
validation batch 1301 / 2620, loss: 0.05
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.04
validation batch 1701 / 2620, loss: 0.04
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.05
validation batch 2001 / 2620, loss: 0.04
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.04
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.04
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.05
validation cost time: 713.4732s
epoch: 40, train time every whole data:1661.50s
epoch: 40, total time:97445.22s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.3934s
epoch: 41, train time every whole data:1661.55s
epoch: 41, total time:99820.16s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.8762s
epoch: 42, train time every whole data:1661.52s
epoch: 42, total time:102194.56s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.04
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.8586s
epoch: 43, train time every whole data:1661.47s
epoch: 43, total time:104568.89s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7747s
epoch: 44, train time every whole data:1661.59s
epoch: 44, total time:106943.26s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.3895s
epoch: 45, train time every whole data:1661.54s
epoch: 45, total time:109318.19s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.04
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4357s
epoch: 46, train time every whole data:1661.57s
epoch: 46, total time:111693.19s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.9350s
epoch: 47, train time every whole data:1661.32s
epoch: 47, total time:114067.45s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7812s
epoch: 48, train time every whole data:1661.28s
epoch: 48, total time:116441.52s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.03
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.05
validation batch 501 / 2620, loss: 0.03
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.03
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.03
validation batch 1201 / 2620, loss: 0.04
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.03
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.03
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.04
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7514s
epoch: 49, train time every whole data:1661.29s
epoch: 49, total time:118815.56s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7280s
epoch: 50, train time every whole data:1661.24s
epoch: 50, total time:121189.53s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.8042s
epoch: 51, train time every whole data:1661.11s
epoch: 51, total time:123563.44s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.8078s
epoch: 52, train time every whole data:1661.22s
epoch: 52, total time:125937.47s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.02
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.8705s
epoch: 53, train time every whole data:1661.18s
epoch: 53, total time:128311.52s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.04
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.04
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7234s
epoch: 54, train time every whole data:1661.20s
epoch: 54, total time:130685.45s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7449s
epoch: 55, train time every whole data:1661.16s
epoch: 55, total time:133059.36s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7660s
epoch: 56, train time every whole data:1661.08s
epoch: 56, total time:135433.21s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7600s
epoch: 57, train time every whole data:1661.50s
epoch: 57, total time:137807.47s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.03
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.7777s
epoch: 58, train time every whole data:1661.17s
epoch: 58, total time:140181.42s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.02
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.02
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.02
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4342s
epoch: 59, train time every whole data:1661.05s
epoch: 59, total time:142555.90s
best epoch: 34
apply the best val model on the test data set ...
load weight from: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_34.params
predicting testing set batch 1 / 2620, time: 0.27s
predicting testing set batch 101 / 2620, time: 27.54s
predicting testing set batch 201 / 2620, time: 54.83s
predicting testing set batch 301 / 2620, time: 82.12s
predicting testing set batch 401 / 2620, time: 109.41s
predicting testing set batch 501 / 2620, time: 136.69s
predicting testing set batch 601 / 2620, time: 163.98s
predicting testing set batch 701 / 2620, time: 191.27s
predicting testing set batch 801 / 2620, time: 218.56s
predicting testing set batch 901 / 2620, time: 245.85s
predicting testing set batch 1001 / 2620, time: 273.14s
predicting testing set batch 1101 / 2620, time: 300.44s
predicting testing set batch 1201 / 2620, time: 327.73s
predicting testing set batch 1301 / 2620, time: 355.03s
predicting testing set batch 1401 / 2620, time: 382.33s
predicting testing set batch 1501 / 2620, time: 409.62s
predicting testing set batch 1601 / 2620, time: 436.92s
predicting testing set batch 1701 / 2620, time: 464.22s
predicting testing set batch 1801 / 2620, time: 491.53s
predicting testing set batch 1901 / 2620, time: 518.83s
predicting testing set batch 2001 / 2620, time: 546.14s
predicting testing set batch 2101 / 2620, time: 573.45s
predicting testing set batch 2201 / 2620, time: 600.76s
predicting testing set batch 2301 / 2620, time: 628.07s
predicting testing set batch 2401 / 2620, time: 655.38s
predicting testing set batch 2501 / 2620, time: 682.69s
predicting testing set batch 2601 / 2620, time: 710.00s
test time on whole data:715.19s
input: (5240, 883, 36, 1)
prediction: (5240, 883, 12, 1)
data_target_tensor: (5240, 883, 12)
current epoch: 34, predict 0 points
MAE: 15.34
RMSE: 25.81
MAPE: 6.57
current epoch: 34, predict 1 points
MAE: 16.60
RMSE: 28.37
MAPE: 7.00
current epoch: 34, predict 2 points
MAE: 17.44
RMSE: 29.92
MAPE: 7.37
current epoch: 34, predict 3 points
MAE: 18.04
RMSE: 31.00
MAPE: 7.73
current epoch: 34, predict 4 points
MAE: 18.51
RMSE: 31.88
MAPE: 8.00
current epoch: 34, predict 5 points
MAE: 18.88
RMSE: 32.59
MAPE: 8.16
current epoch: 34, predict 6 points
MAE: 19.19
RMSE: 33.21
MAPE: 8.24
current epoch: 34, predict 7 points
MAE: 19.49
RMSE: 33.76
MAPE: 8.38
current epoch: 34, predict 8 points
MAE: 19.76
RMSE: 34.27
MAPE: 8.45
current epoch: 34, predict 9 points
MAE: 20.05
RMSE: 34.74
MAPE: 8.59
current epoch: 34, predict 10 points
MAE: 20.38
RMSE: 35.24
MAPE: 8.79
current epoch: 34, predict 11 points
MAE: 20.87
RMSE: 35.79
MAPE: 9.14
all MAE: 18.71
all RMSE: 32.34
all MAPE: 8.03
[15.339492, 25.806893023745854, 6.56619593501091, 16.604113, 28.366237547781825, 6.996924430131912, 17.43925, 29.91726465787564, 7.369908690452576, 18.04154, 30.999018499772195, 7.7320396900177, 18.51232, 31.875714223003104, 7.99628272652626, 18.881998, 32.5871165649191, 8.156522363424301, 19.185572, 33.21124349276819, 8.24115201830864, 19.486038, 33.75959607501899, 8.381645381450653, 19.761122, 34.27315879901501, 8.450253307819366, 20.050388, 34.736418560554895, 8.586208522319794, 20.378849, 35.23524978126929, 8.792228996753693, 20.868258, 35.78612309883976, 9.144539386034012, 18.712423, 32.34029268961554, 8.034464716911316]
fine tune the model ... 
epoch: 60, train time every whole data:3329.17s
epoch: 60, total time:146607.62s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.0584s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_60.params
epoch: 61, train time every whole data:3327.94s
epoch: 61, total time:150649.63s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4148s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_61.params
epoch: 62, train time every whole data:3327.52s
epoch: 62, total time:154690.58s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.3124s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_62.params
epoch: 63, train time every whole data:3327.61s
epoch: 63, total time:158731.51s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.0652s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_63.params
epoch: 64, train time every whole data:3327.53s
epoch: 64, total time:162772.12s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 714.1168s
epoch: 65, train time every whole data:3327.23s
epoch: 65, total time:166813.47s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.0043s
epoch: 66, train time every whole data:3327.35s
epoch: 66, total time:170853.82s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.1202s
epoch: 67, train time every whole data:3327.28s
epoch: 67, total time:174894.22s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.2956s
epoch: 68, train time every whole data:3327.30s
epoch: 68, total time:178934.81s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.6898s
epoch: 69, train time every whole data:3327.37s
epoch: 69, total time:182975.87s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.7526s
save parameters to file: ../experiments/PEMS07/MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_2TcontextScaledSAtSE1TE/epoch_69.params
epoch: 70, train time every whole data:3327.30s
epoch: 70, total time:187016.93s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 712.9608s
epoch: 71, train time every whole data:3327.47s
epoch: 71, total time:191057.37s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.9142s
epoch: 72, train time every whole data:3327.50s
epoch: 72, total time:195098.79s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4261s
epoch: 73, train time every whole data:3327.58s
epoch: 73, total time:199139.79s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.3633s
epoch: 74, train time every whole data:3327.51s
epoch: 74, total time:203180.66s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.1949s
epoch: 75, train time every whole data:3327.65s
epoch: 75, total time:207221.51s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.5723s
epoch: 76, train time every whole data:3327.65s
epoch: 76, total time:211262.73s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.9193s
epoch: 77, train time every whole data:3327.72s
epoch: 77, total time:215304.38s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.2123s
epoch: 78, train time every whole data:3327.65s
epoch: 78, total time:219345.24s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.2014s
epoch: 79, train time every whole data:3327.62s
epoch: 79, total time:223386.06s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.5219s
epoch: 80, train time every whole data:4350.75s
epoch: 80, total time:228450.34s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1213.8806s
epoch: 81, train time every whole data:5511.30s
epoch: 81, total time:235175.52s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1213.2538s
epoch: 82, train time every whole data:5511.79s
epoch: 82, total time:241900.57s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1212.7863s
epoch: 83, train time every whole data:5511.34s
epoch: 83, total time:248624.70s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1212.7404s
epoch: 84, train time every whole data:5512.08s
epoch: 84, total time:255349.52s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1214.3206s
epoch: 85, train time every whole data:5511.84s
epoch: 85, total time:262075.68s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1213.1684s
epoch: 86, train time every whole data:5504.02s
epoch: 86, total time:268792.88s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1213.5981s
epoch: 87, train time every whole data:5509.76s
epoch: 87, total time:275516.23s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.05
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1213.0235s
epoch: 88, train time every whole data:5509.25s
epoch: 88, total time:282238.50s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1212.6215s
epoch: 89, train time every whole data:5509.18s
epoch: 89, total time:288960.31s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1212.8282s
epoch: 90, train time every whole data:5509.76s
epoch: 90, total time:295682.90s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.02
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.03
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.02
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1214.0173s
epoch: 91, train time every whole data:5507.66s
epoch: 91, total time:302404.57s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.03
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.03
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.03
validation batch 2601 / 2620, loss: 0.03
validation cost time: 1212.3228s
epoch: 92, train time every whole data:5042.67s
epoch: 92, total time:308659.57s
validation batch 1 / 2620, loss: 0.03
validation batch 101 / 2620, loss: 0.02
validation batch 201 / 2620, loss: 0.02
validation batch 301 / 2620, loss: 0.03
validation batch 401 / 2620, loss: 0.04
validation batch 501 / 2620, loss: 0.01
validation batch 601 / 2620, loss: 0.03
validation batch 701 / 2620, loss: 0.03
validation batch 801 / 2620, loss: 0.01
validation batch 901 / 2620, loss: 0.03
validation batch 1001 / 2620, loss: 0.03
validation batch 1101 / 2620, loss: 0.01
validation batch 1201 / 2620, loss: 0.02
validation batch 1301 / 2620, loss: 0.03
validation batch 1401 / 2620, loss: 0.04
validation batch 1501 / 2620, loss: 0.02
validation batch 1601 / 2620, loss: 0.03
validation batch 1701 / 2620, loss: 0.03
validation batch 1801 / 2620, loss: 0.01
validation batch 1901 / 2620, loss: 0.03
validation batch 2001 / 2620, loss: 0.03
validation batch 2101 / 2620, loss: 0.01
validation batch 2201 / 2620, loss: 0.02
validation batch 2301 / 2620, loss: 0.03
validation batch 2401 / 2620, loss: 0.03
validation batch 2501 / 2620, loss: 0.02
validation batch 2601 / 2620, loss: 0.03
validation cost time: 713.4048s
epoch: 93, train time every whole data:3334.15s
epoch: 93, total time:312707.13s
C:\Users\developer\anaconda3\envs\ast\python.exe C:/Users/developer/Documents/GitHub/ASTGNN-0717/train_ASTGNN.py --config configurations/PEMS07_rdw.conf
CUDA: True cuda:0
Read configuration file: configurations/PEMS07_rdw.conf
total training epoch, fine tune epoch: 50 , 25
batch_size: 16
folder_dir: MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_16TcontextScaledSAtSE1TE
load file: data/PEMS07\PEMS07_r1_d1_w1.npz
ori length: 15718 , percent: 1.0 , scale: 15718
train: torch.Size([15718, 883, 1, 36]) torch.Size([15718, 883, 12]) torch.Size([15718, 883, 12])
val: torch.Size([5239, 883, 1, 36]) torch.Size([5239, 883, 12]) torch.Size([5239, 883, 12])
test: torch.Size([5240, 883, 1, 36]) torch.Size([5240, 883, 12]) torch.Size([5240, 883, 12])
TemporalPositionalEncoding max_len: 2016
w_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
d_index: [1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739]
h_index: [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(883, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
load weight from: ../experiments\PEMS07\MAE_ASTGNN_h1d1w1_layer3_head8_dm64_channel1_dir2_drop0.00_1.00e-03_16TcontextScaledSAtSE1TE\epoch_69.params
predicting testing set batch 1 / 328, time: 3.52s
predicting testing set batch 101 / 328, time: 166.95s
predicting testing set batch 201 / 328, time: 334.38s
predicting testing set batch 301 / 328, time: 497.77s
test time on whole data:540.97s
input: (5240, 883, 36, 1)
prediction: (5240, 883, 12, 1)
data_target_tensor: (5240, 883, 12)
current epoch: 69, predict 0 points
MAE: 15.25
RMSE: 25.75
MAPE: 6.40
current epoch: 69, predict 1 points
MAE: 16.45
RMSE: 28.21
MAPE: 6.86
current epoch: 69, predict 2 points
MAE: 17.16
RMSE: 29.63
MAPE: 7.15
current epoch: 69, predict 3 points
MAE: 17.64
RMSE: 30.58
MAPE: 7.32
current epoch: 69, predict 4 points
MAE: 18.00
RMSE: 31.34
MAPE: 7.46
current epoch: 69, predict 5 points
MAE: 18.32
RMSE: 31.97
MAPE: 7.60
current epoch: 69, predict 6 points
MAE: 18.60
RMSE: 32.54
MAPE: 7.71
current epoch: 69, predict 7 points
MAE: 18.85
RMSE: 33.03
MAPE: 7.83
current epoch: 69, predict 8 points
MAE: 19.09
RMSE: 33.49
MAPE: 7.94
current epoch: 69, predict 9 points
MAE: 19.31
RMSE: 33.92
MAPE: 8.03
current epoch: 69, predict 10 points
MAE: 19.56
RMSE: 34.36
MAPE: 8.14
current epoch: 69, predict 11 points
MAE: 19.86
RMSE: 34.84
MAPE: 8.26
all MAE: 18.17
all RMSE: 31.75
all MAPE: 7.56
[15.252212, 25.74894638128119, 6.404631584882736, 16.446, 28.21101971394833, 6.863059848546982, 17.164892, 29.630708426157103, 7.1471452713012695, 17.637642, 30.58227847685547, 7.320201396942139, 18.000069, 31.340472551104945, 7.464763522148132, 18.318684, 31.971790277292204, 7.601633667945862, 18.602638, 32.53753278967326, 7.709119468927383, 18.85343, 33.03360586146989, 7.8258126974105835, 19.09203, 33.49350413641334, 7.937799394130707, 19.313168, 33.915238583903914, 8.031515777111053, 19.556635, 34.36099324008712, 8.14208984375, 19.860516, 34.84427024555562, 8.258665353059769, 18.174822, 31.74550807145546, 7.558847963809967]

Process finished with exit code 0
