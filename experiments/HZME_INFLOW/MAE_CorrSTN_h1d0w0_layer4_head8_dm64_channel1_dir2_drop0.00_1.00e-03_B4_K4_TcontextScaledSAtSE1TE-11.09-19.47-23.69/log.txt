C:\Users\developer\anaconda3\envs\pytorch1.7-python3.8\python.exe C:/Users/developer/Documents/GitHub/Research/ASTGNN-0717/train_ASTGNN.py --config configurations/HZME_INFLOW.conf
Wed Oct 27 18:49:10 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:17:00.0  On |                  N/A |
| 50%   47C    P2   188W / 350W |   8197MiB / 24576MiB |     82%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1412    C+G   Insufficient Permissions        N/A      |
|    0   N/A  N/A      1456    C+G   ...8\extracted\WeChatApp.exe    N/A      |
|    0   N/A  N/A      3468    C+G   ...��\EvernoteSubprocess.exe    N/A      |
|    0   N/A  N/A      4552    C+G   ...tracted\WechatBrowser.exe    N/A      |
|    0   N/A  N/A      5924    C+G   ...1\jbr\bin\jcef_helper.exe    N/A      |
|    0   N/A  N/A     10208    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     11716    C+G   ...m Files\Papers\Papers.exe    N/A      |
|    0   N/A  N/A     12012    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A     12208    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |
|    0   N/A  N/A     12452    C+G   ...nputApp\TextInputHost.exe    N/A      |
|    0   N/A  N/A     12876    C+G   ...perience\NVIDIA Share.exe    N/A      |
|    0   N/A  N/A     13376    C+G   ...perience\NVIDIA Share.exe    N/A      |
|    0   N/A  N/A     17276    C+G   ...b3d8bbwe\WinStore.App.exe    N/A      |
|    0   N/A  N/A     18004    C+G   ...\Common7\IDE\XDesProc.exe    N/A      |
|    0   N/A  N/A     18168    C+G   ...kyb3d8bbwe\Calculator.exe    N/A      |
|    0   N/A  N/A     19108    C+G   ...p-2.9.4\GitHubDesktop.exe    N/A      |
|    0   N/A  N/A     19380    C+G   ...ub.ThreadedWaitDialog.exe    N/A      |
|    0   N/A  N/A     20364      C   ...h1.7-python3.8\python.exe    N/A      |
|    0   N/A  N/A     21332    C+G   ...al\Common7\IDE\devenv.exe    N/A      |
|    0   N/A  N/A     21668    C+G   ...e\root\Office16\EXCEL.EXE    N/A      |
|    0   N/A  N/A     24488    C+G   ...s (x86)\ParaCloud\pcd.exe    N/A      |
|    0   N/A  N/A     25320    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     26148    C+G   ...\Common7\IDE\XDesProc.exe    N/A      |
|    0   N/A  N/A     26616    C+G   ...ub.ThreadedWaitDialog.exe    N/A      |
|    0   N/A  N/A     27596    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     28340      C   ...h1.7-python3.8\python.exe    N/A      |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: configurations/HZME_INFLOW.conf
total training epoch, fine tune epoch: 180 , 50
batch_size: 4
folder_dir: K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE
load file: data/HZME_INFLOW\HZME_INFLOW_r1_d0_w0.npz
ori length: 4479 , percent: 1.0 , scale: 4479
train: torch.Size([3323, 80, 1, 12]) torch.Size([3323, 80, 12]) torch.Size([3323, 80, 12])
val: torch.Size([1128, 80, 1, 12]) torch.Size([1128, 80, 12]) torch.Size([1128, 80, 12])
test: torch.Size([1128, 80, 1, 12]) torch.Size([1128, 80, 12]) torch.Size([1128, 80, 12])
TemporalPositionalEncoding max_len: 12
w_index: []
d_index: []
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 12, 64])
src_embed.2.embedding.weight 	 torch.Size([80, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([80, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 452181
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 282, loss: 1.60
validation batch 101 / 282, loss: 1.59
validation batch 201 / 282, loss: 1.59
validation cost time: 53.9210s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_0.params
epoch: 0, train time every whole data:90.73s
epoch: 0, total time:144.67s
validation batch 1 / 282, loss: 0.15
validation batch 101 / 282, loss: 0.15
validation batch 201 / 282, loss: 0.15
validation cost time: 54.4320s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_1.params
epoch: 1, train time every whole data:88.98s
epoch: 1, total time:288.11s
validation batch 1 / 282, loss: 0.05
validation batch 101 / 282, loss: 0.06
validation batch 201 / 282, loss: 0.06
validation cost time: 53.5110s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_2.params
epoch: 2, train time every whole data:88.72s
epoch: 2, total time:430.36s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 53.7070s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_3.params
epoch: 3, train time every whole data:89.23s
epoch: 3, total time:573.32s
validation batch 1 / 282, loss: 0.06
validation batch 101 / 282, loss: 0.07
validation batch 201 / 282, loss: 0.07
validation cost time: 53.1880s
epoch: 4, train time every whole data:89.72s
epoch: 4, total time:716.23s
validation batch 1 / 282, loss: 0.05
validation batch 101 / 282, loss: 0.06
validation batch 201 / 282, loss: 0.06
validation cost time: 55.2860s
epoch: 5, train time every whole data:89.26s
epoch: 5, total time:860.78s
validation batch 1 / 282, loss: 0.03
validation batch 101 / 282, loss: 0.03
validation batch 201 / 282, loss: 0.03
validation cost time: 54.8780s
epoch: 6, train time every whole data:88.57s
epoch: 6, total time:1004.23s
validation batch 1 / 282, loss: 0.10
validation batch 101 / 282, loss: 0.09
validation batch 201 / 282, loss: 0.09
validation cost time: 52.8310s
epoch: 7, train time every whole data:88.85s
epoch: 7, total time:1145.92s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 52.6360s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_8.params
epoch: 8, train time every whole data:89.04s
epoch: 8, total time:1287.61s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 52.0290s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_9.params
epoch: 9, train time every whole data:88.68s
epoch: 9, total time:1428.34s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 51.6600s
epoch: 10, train time every whole data:88.84s
epoch: 10, total time:1568.84s
validation batch 1 / 282, loss: 0.05
validation batch 101 / 282, loss: 0.05
validation batch 201 / 282, loss: 0.05
validation cost time: 52.0820s
epoch: 11, train time every whole data:89.37s
epoch: 11, total time:1710.30s
validation batch 1 / 282, loss: 0.08
validation batch 101 / 282, loss: 0.08
validation batch 201 / 282, loss: 0.08
validation cost time: 50.8500s
epoch: 12, train time every whole data:91.61s
epoch: 12, total time:1852.76s
validation batch 1 / 282, loss: 0.05
validation batch 101 / 282, loss: 0.05
validation batch 201 / 282, loss: 0.05
validation cost time: 49.3690s
epoch: 13, train time every whole data:90.73s
epoch: 13, total time:1992.86s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.03
validation cost time: 49.1620s
epoch: 14, train time every whole data:90.75s
epoch: 14, total time:2132.77s
validation batch 1 / 282, loss: 0.03
validation batch 101 / 282, loss: 0.03
validation batch 201 / 282, loss: 0.04
validation cost time: 49.1010s
epoch: 15, train time every whole data:91.34s
epoch: 15, total time:2273.22s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 49.5280s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_16.params
epoch: 16, train time every whole data:89.55s
epoch: 16, total time:2412.33s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 50.2770s
epoch: 17, train time every whole data:90.38s
epoch: 17, total time:2552.98s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.02
validation cost time: 53.2680s
epoch: 18, train time every whole data:90.37s
epoch: 18, total time:2696.62s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.5550s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_19.params
epoch: 19, train time every whole data:89.40s
epoch: 19, total time:2838.60s
validation batch 1 / 282, loss: 0.07
validation batch 101 / 282, loss: 0.06
validation batch 201 / 282, loss: 0.05
validation cost time: 50.6260s
epoch: 20, train time every whole data:89.45s
epoch: 20, total time:2978.68s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.02
validation cost time: 51.7230s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_21.params
epoch: 21, train time every whole data:89.70s
epoch: 21, total time:3120.13s
validation batch 1 / 282, loss: 0.04
validation batch 101 / 282, loss: 0.04
validation batch 201 / 282, loss: 0.05
validation cost time: 51.2870s
epoch: 22, train time every whole data:87.92s
epoch: 22, total time:3259.33s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.03
validation batch 201 / 282, loss: 0.03
validation cost time: 51.6580s
epoch: 23, train time every whole data:87.79s
epoch: 23, total time:3398.78s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.02
validation cost time: 50.3920s
epoch: 24, train time every whole data:89.12s
epoch: 24, total time:3538.30s
validation batch 1 / 282, loss: 0.03
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 52.9790s
epoch: 25, train time every whole data:88.75s
epoch: 25, total time:3680.03s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.02
validation cost time: 52.0700s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_26.params
epoch: 26, train time every whole data:88.80s
epoch: 26, total time:3820.92s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.02
validation cost time: 51.7210s
epoch: 27, train time every whole data:88.74s
epoch: 27, total time:3961.38s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 51.7620s
epoch: 28, train time every whole data:88.92s
epoch: 28, total time:4102.06s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.5930s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_29.params
epoch: 29, train time every whole data:88.70s
epoch: 29, total time:4243.39s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.8480s
epoch: 30, train time every whole data:87.88s
epoch: 30, total time:4384.12s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 54.7900s
epoch: 31, train time every whole data:88.96s
epoch: 31, total time:4527.87s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.02
validation cost time: 51.1660s
epoch: 32, train time every whole data:89.51s
epoch: 32, total time:4668.55s
validation batch 1 / 282, loss: 0.04
validation batch 101 / 282, loss: 0.04
validation batch 201 / 282, loss: 0.03
validation cost time: 52.7730s
epoch: 33, train time every whole data:88.76s
epoch: 33, total time:4810.08s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 51.7810s
epoch: 34, train time every whole data:89.45s
epoch: 34, total time:4951.32s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.2070s
epoch: 35, train time every whole data:89.31s
epoch: 35, total time:5092.84s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.7620s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_36.params
epoch: 36, train time every whole data:89.29s
epoch: 36, total time:5234.91s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 50.7320s
epoch: 37, train time every whole data:89.03s
epoch: 37, total time:5374.67s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.0640s
epoch: 38, train time every whole data:88.72s
epoch: 38, total time:5514.46s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 50.9240s
epoch: 39, train time every whole data:90.17s
epoch: 39, total time:5655.55s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.8670s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_40.params
epoch: 40, train time every whole data:91.19s
epoch: 40, total time:5798.63s
validation batch 1 / 282, loss: 0.03
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 51.1930s
epoch: 41, train time every whole data:89.27s
epoch: 41, total time:5939.10s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 52.6950s
epoch: 42, train time every whole data:89.75s
epoch: 42, total time:6081.55s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.5010s
epoch: 43, train time every whole data:90.03s
epoch: 43, total time:6224.08s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.4960s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_44.params
epoch: 44, train time every whole data:90.00s
epoch: 44, total time:6366.60s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.3550s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_45.params
epoch: 45, train time every whole data:88.73s
epoch: 45, total time:6508.70s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.9170s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_46.params
epoch: 46, train time every whole data:89.94s
epoch: 46, total time:6651.57s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.5370s
epoch: 47, train time every whole data:89.43s
epoch: 47, total time:6793.54s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 53.7810s
epoch: 48, train time every whole data:89.94s
epoch: 48, total time:6937.26s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.9360s
epoch: 49, train time every whole data:89.55s
epoch: 49, total time:7078.75s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.0100s
epoch: 50, train time every whole data:89.45s
epoch: 50, total time:7221.22s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.9910s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_51.params
epoch: 51, train time every whole data:89.92s
epoch: 51, total time:7365.15s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.3920s
epoch: 52, train time every whole data:89.58s
epoch: 52, total time:7508.12s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.6640s
epoch: 53, train time every whole data:89.98s
epoch: 53, total time:7651.77s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 53.5330s
epoch: 54, train time every whole data:90.40s
epoch: 54, total time:7795.71s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 54.6200s
epoch: 55, train time every whole data:89.30s
epoch: 55, total time:7939.63s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.3400s
epoch: 56, train time every whole data:90.11s
epoch: 56, total time:8083.08s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 55.5450s
epoch: 57, train time every whole data:88.15s
epoch: 57, total time:8226.77s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 56.2720s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_58.params
epoch: 58, train time every whole data:88.04s
epoch: 58, total time:8371.11s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 55.9280s
epoch: 59, train time every whole data:87.54s
epoch: 59, total time:8514.58s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 55.4060s
epoch: 60, train time every whole data:87.14s
epoch: 60, total time:8657.13s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 55.6100s
epoch: 61, train time every whole data:87.63s
epoch: 61, total time:8800.37s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 55.3450s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_62.params
epoch: 62, train time every whole data:87.40s
epoch: 62, total time:8943.14s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 55.7970s
epoch: 63, train time every whole data:88.04s
epoch: 63, total time:9086.99s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 54.9740s
epoch: 64, train time every whole data:88.24s
epoch: 64, total time:9230.21s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 56.7040s
epoch: 65, train time every whole data:87.26s
epoch: 65, total time:9374.18s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 54.7920s
epoch: 66, train time every whole data:88.19s
epoch: 66, total time:9517.16s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 56.2610s
epoch: 67, train time every whole data:88.31s
epoch: 67, total time:9661.73s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 56.8000s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_68.params
epoch: 68, train time every whole data:87.12s
epoch: 68, total time:9805.68s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 54.8550s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_69.params
epoch: 69, train time every whole data:87.14s
epoch: 69, total time:9947.70s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 56.6040s
epoch: 70, train time every whole data:88.82s
epoch: 70, total time:10093.13s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 54.8170s
epoch: 71, train time every whole data:88.65s
epoch: 71, total time:10236.60s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 55.5880s
epoch: 72, train time every whole data:89.45s
epoch: 72, total time:10381.64s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 54.5890s
epoch: 73, train time every whole data:88.68s
epoch: 73, total time:10524.92s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 54.4780s
epoch: 74, train time every whole data:89.36s
epoch: 74, total time:10668.76s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 55.2850s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_75.params
epoch: 75, train time every whole data:89.40s
epoch: 75, total time:10813.46s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 53.0210s
epoch: 76, train time every whole data:88.71s
epoch: 76, total time:10955.19s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 53.7220s
epoch: 77, train time every whole data:89.48s
epoch: 77, total time:11098.40s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 53.5300s
epoch: 78, train time every whole data:88.26s
epoch: 78, total time:11240.19s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.3330s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_79.params
epoch: 79, train time every whole data:88.19s
epoch: 79, total time:11380.73s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.5030s
epoch: 80, train time every whole data:89.29s
epoch: 80, total time:11522.53s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.8450s
epoch: 81, train time every whole data:89.56s
epoch: 81, total time:11664.94s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 53.1510s
epoch: 82, train time every whole data:88.79s
epoch: 82, total time:11806.88s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.4450s
epoch: 83, train time every whole data:87.43s
epoch: 83, total time:11947.76s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.4260s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_84.params
epoch: 84, train time every whole data:88.76s
epoch: 84, total time:12088.97s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 53.5190s
epoch: 85, train time every whole data:89.08s
epoch: 85, total time:12231.57s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.8060s
epoch: 86, train time every whole data:89.12s
epoch: 86, total time:12374.49s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.0910s
epoch: 87, train time every whole data:87.85s
epoch: 87, total time:12515.43s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.5700s
epoch: 88, train time every whole data:88.45s
epoch: 88, total time:12657.45s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 53.2410s
epoch: 89, train time every whole data:87.98s
epoch: 89, total time:12798.68s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.9130s
epoch: 90, train time every whole data:87.00s
epoch: 90, total time:12939.59s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 54.1000s
epoch: 91, train time every whole data:87.46s
epoch: 91, total time:13081.15s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 55.1550s
epoch: 92, train time every whole data:88.55s
epoch: 92, total time:13224.86s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.0810s
epoch: 93, train time every whole data:90.43s
epoch: 93, total time:13367.38s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 51.0430s
epoch: 94, train time every whole data:91.18s
epoch: 94, total time:13509.60s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.1050s
epoch: 95, train time every whole data:91.76s
epoch: 95, total time:13652.47s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 50.7420s
epoch: 96, train time every whole data:90.89s
epoch: 96, total time:13794.10s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.4150s
epoch: 97, train time every whole data:91.10s
epoch: 97, total time:13935.62s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 50.5200s
epoch: 98, train time every whole data:91.19s
epoch: 98, total time:14077.33s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 50.2240s
epoch: 99, train time every whole data:91.69s
epoch: 99, total time:14219.25s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 50.0540s
epoch: 100, train time every whole data:90.07s
epoch: 100, total time:14359.38s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.4390s
epoch: 101, train time every whole data:92.07s
epoch: 101, total time:14501.88s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.4010s
epoch: 102, train time every whole data:91.37s
epoch: 102, total time:14643.65s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.6690s
epoch: 103, train time every whole data:91.47s
epoch: 103, total time:14785.79s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.4660s
epoch: 104, train time every whole data:91.80s
epoch: 104, total time:14928.06s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.1330s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_105.params
epoch: 105, train time every whole data:91.63s
epoch: 105, total time:15070.86s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.7750s
epoch: 106, train time every whole data:91.58s
epoch: 106, total time:15213.21s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.9770s
epoch: 107, train time every whole data:91.02s
epoch: 107, total time:15355.22s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.0510s
epoch: 108, train time every whole data:91.06s
epoch: 108, total time:15497.33s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.7940s
epoch: 109, train time every whole data:91.79s
epoch: 109, total time:15639.92s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.0490s
epoch: 110, train time every whole data:91.14s
epoch: 110, total time:15782.11s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.9750s
epoch: 111, train time every whole data:91.38s
epoch: 111, total time:15924.47s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.0770s
epoch: 112, train time every whole data:91.93s
epoch: 112, total time:16067.48s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.3740s
epoch: 113, train time every whole data:92.21s
epoch: 113, total time:16211.07s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.0590s
epoch: 114, train time every whole data:91.51s
epoch: 114, total time:16354.64s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.3130s
epoch: 115, train time every whole data:92.29s
epoch: 115, total time:16498.24s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.7790s
epoch: 116, train time every whole data:93.03s
epoch: 116, total time:16643.06s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.6560s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_117.params
epoch: 117, train time every whole data:93.98s
epoch: 117, total time:16788.72s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.8930s
epoch: 118, train time every whole data:93.30s
epoch: 118, total time:16933.92s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 51.9580s
epoch: 119, train time every whole data:93.56s
epoch: 119, total time:17079.44s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.5120s
epoch: 120, train time every whole data:93.72s
epoch: 120, total time:17225.67s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 52.8640s
epoch: 121, train time every whole data:91.98s
epoch: 121, total time:17370.52s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 53.2030s
epoch: 122, train time every whole data:94.05s
epoch: 122, total time:17517.77s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.7330s
epoch: 123, train time every whole data:91.96s
epoch: 123, total time:17662.47s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.9950s
epoch: 124, train time every whole data:92.33s
epoch: 124, total time:17807.79s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.8540s
epoch: 125, train time every whole data:93.55s
epoch: 125, total time:17954.20s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.2620s
epoch: 126, train time every whole data:94.24s
epoch: 126, total time:18100.70s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.2370s
epoch: 127, train time every whole data:93.84s
epoch: 127, total time:18246.79s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.1620s
epoch: 128, train time every whole data:93.09s
epoch: 128, total time:18392.04s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.8660s
epoch: 129, train time every whole data:93.04s
epoch: 129, total time:18536.95s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.8630s
epoch: 130, train time every whole data:91.97s
epoch: 130, total time:18680.79s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.4260s
epoch: 131, train time every whole data:91.49s
epoch: 131, total time:18823.70s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 51.0460s
epoch: 132, train time every whole data:91.67s
epoch: 132, total time:18966.42s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.9840s
epoch: 133, train time every whole data:92.64s
epoch: 133, total time:19110.05s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.8830s
epoch: 134, train time every whole data:92.38s
epoch: 134, total time:19253.31s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.5910s
epoch: 135, train time every whole data:92.26s
epoch: 135, total time:19396.17s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.5860s
epoch: 136, train time every whole data:91.38s
epoch: 136, total time:19538.14s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.5520s
epoch: 137, train time every whole data:91.55s
epoch: 137, total time:19680.24s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.5760s
epoch: 138, train time every whole data:90.26s
epoch: 138, total time:19821.08s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.2090s
epoch: 139, train time every whole data:90.77s
epoch: 139, total time:19962.06s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.6180s
epoch: 140, train time every whole data:90.70s
epoch: 140, total time:20103.38s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.4590s
epoch: 141, train time every whole data:91.63s
epoch: 141, total time:20245.47s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.3360s
epoch: 142, train time every whole data:91.24s
epoch: 142, total time:20387.05s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.0050s
epoch: 143, train time every whole data:92.35s
epoch: 143, total time:20529.41s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 50.1400s
epoch: 144, train time every whole data:91.38s
epoch: 144, total time:20670.93s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 49.9560s
epoch: 145, train time every whole data:91.60s
epoch: 145, total time:20812.49s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.2740s
epoch: 146, train time every whole data:91.43s
epoch: 146, total time:20954.19s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 49.8140s
epoch: 147, train time every whole data:91.29s
epoch: 147, total time:21095.30s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.1320s
epoch: 148, train time every whole data:91.28s
epoch: 148, total time:21236.71s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 50.2940s
epoch: 149, train time every whole data:91.31s
epoch: 149, total time:21378.32s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.4550s
epoch: 150, train time every whole data:90.85s
epoch: 150, total time:21519.63s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.4130s
epoch: 151, train time every whole data:91.69s
epoch: 151, total time:21661.73s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 50.7570s
epoch: 152, train time every whole data:90.79s
epoch: 152, total time:21803.28s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.1800s
epoch: 153, train time every whole data:91.20s
epoch: 153, total time:21945.67s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.8470s
epoch: 154, train time every whole data:90.89s
epoch: 154, total time:22087.41s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 51.1330s
epoch: 155, train time every whole data:90.96s
epoch: 155, total time:22229.51s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.7800s
epoch: 156, train time every whole data:91.87s
epoch: 156, total time:22372.16s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.02
validation cost time: 50.9330s
epoch: 157, train time every whole data:91.97s
epoch: 157, total time:22515.06s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.2770s
epoch: 158, train time every whole data:92.55s
epoch: 158, total time:22658.90s
validation batch 1 / 282, loss: 0.02
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.8810s
epoch: 159, train time every whole data:93.17s
epoch: 159, total time:22802.95s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.3360s
epoch: 160, train time every whole data:90.85s
epoch: 160, total time:22945.14s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.5690s
epoch: 161, train time every whole data:92.08s
epoch: 161, total time:23088.79s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.1460s
epoch: 162, train time every whole data:91.52s
epoch: 162, total time:23231.46s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.1540s
epoch: 163, train time every whole data:92.57s
epoch: 163, total time:23375.19s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.5370s
epoch: 164, train time every whole data:92.80s
epoch: 164, total time:23519.53s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 51.7210s
epoch: 165, train time every whole data:93.06s
epoch: 165, total time:23664.31s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.8490s
epoch: 166, train time every whole data:93.43s
epoch: 166, total time:23809.59s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.3790s
epoch: 167, train time every whole data:94.84s
epoch: 167, total time:23956.81s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 52.7740s
epoch: 168, train time every whole data:93.26s
epoch: 168, total time:24102.85s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.1890s
epoch: 169, train time every whole data:93.22s
epoch: 169, total time:24249.26s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 53.1740s
epoch: 170, train time every whole data:91.63s
epoch: 170, total time:24394.06s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.9260s
epoch: 171, train time every whole data:92.74s
epoch: 171, total time:24539.73s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 52.9620s
epoch: 172, train time every whole data:93.98s
epoch: 172, total time:24686.68s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 52.8500s
epoch: 173, train time every whole data:94.21s
epoch: 173, total time:24833.74s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 52.7090s
epoch: 174, train time every whole data:94.93s
epoch: 174, total time:24981.39s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 52.3300s
epoch: 175, train time every whole data:94.71s
epoch: 175, total time:25128.43s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 51.8230s
epoch: 176, train time every whole data:94.68s
epoch: 176, total time:25274.93s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 51.5990s
epoch: 177, train time every whole data:92.24s
epoch: 177, total time:25418.77s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.02
validation batch 201 / 282, loss: 0.01
validation cost time: 51.0240s
epoch: 178, train time every whole data:92.38s
epoch: 178, total time:25562.18s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 50.9510s
epoch: 179, train time every whole data:91.92s
epoch: 179, total time:25705.05s
best epoch: 117
apply the best val model on the test data set ...
load weight from: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_117.params
predicting testing set batch 1 / 282, time: 0.16s
predicting testing set batch 101 / 282, time: 17.31s
predicting testing set batch 201 / 282, time: 34.58s
test time on whole data:48.39s
input: (1128, 80, 12, 1)
prediction: (1128, 80, 12, 1)
data_target_tensor: (1128, 80, 12)
current epoch: 117, predict 0 points
MAE: 10.10
RMSE: 16.65
MAPE: 23.55
current epoch: 117, predict 1 points
MAE: 10.60
RMSE: 18.28
MAPE: 24.38
current epoch: 117, predict 2 points
MAE: 10.89
RMSE: 19.01
MAPE: 24.79
current epoch: 117, predict 3 points
MAE: 11.09
RMSE: 19.51
MAPE: 24.71
current epoch: 117, predict 4 points
MAE: 11.21
RMSE: 19.83
MAPE: 24.55
current epoch: 117, predict 5 points
MAE: 11.37
RMSE: 20.12
MAPE: 24.63
current epoch: 117, predict 6 points
MAE: 11.60
RMSE: 20.66
MAPE: 25.10
current epoch: 117, predict 7 points
MAE: 11.91
RMSE: 21.58
MAPE: 25.47
current epoch: 117, predict 8 points
MAE: 12.10
RMSE: 22.02
MAPE: 25.74
current epoch: 117, predict 9 points
MAE: 12.16
RMSE: 22.05
MAPE: 26.08
current epoch: 117, predict 10 points
MAE: 12.26
RMSE: 22.26
MAPE: 26.48
current epoch: 117, predict 11 points
MAE: 12.54
RMSE: 23.20
MAPE: 27.15
all MAE: 11.49
all RMSE: 20.51
all MAPE: 25.20
[10.104522, 16.650615032395386, 23.554031550884247, 10.5960865, 18.283502632314722, 24.38023090362549, 10.885436, 19.01267823305675, 24.785859882831573, 11.091415, 19.506582860725345, 24.709665775299072, 11.209605, 19.825352877041528, 24.547582864761353, 11.372301, 20.120190916577588, 24.634546041488647, 11.604615, 20.657288587260037, 25.09969174861908, 11.910559, 21.575519025975584, 25.47003924846649, 12.098972, 22.022635865820323, 25.74421763420105, 12.159137, 22.046223718011422, 26.075124740600586, 12.263449, 22.25533205071315, 26.483267545700073, 12.538016, 23.195822977710474, 27.154424786567688, 11.486177, 20.510144212681684, 25.20468533039093]
fine tune the model ... 
epoch: 180, train time every whole data:237.34s
epoch: 180, total time:25991.03s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.7910s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_180.params
epoch: 181, train time every whole data:237.55s
epoch: 181, total time:26277.41s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5150s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_181.params
epoch: 182, train time every whole data:237.64s
epoch: 182, total time:26563.60s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.8500s
epoch: 183, train time every whole data:237.01s
epoch: 183, total time:26849.47s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6940s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_183.params
epoch: 184, train time every whole data:237.13s
epoch: 184, total time:27135.33s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5080s
epoch: 185, train time every whole data:236.92s
epoch: 185, total time:27420.75s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6580s
epoch: 186, train time every whole data:236.19s
epoch: 186, total time:27705.61s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5410s
epoch: 187, train time every whole data:237.24s
epoch: 187, total time:27991.39s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.4710s
epoch: 188, train time every whole data:236.44s
epoch: 188, total time:28276.30s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.8680s
epoch: 189, train time every whole data:237.13s
epoch: 189, total time:28562.30s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5200s
save parameters to file: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_189.params
epoch: 190, train time every whole data:236.89s
epoch: 190, total time:28847.76s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5420s
epoch: 191, train time every whole data:236.95s
epoch: 191, total time:29133.25s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.3710s
epoch: 192, train time every whole data:237.18s
epoch: 192, total time:29418.80s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.3840s
epoch: 193, train time every whole data:237.48s
epoch: 193, total time:29704.67s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5770s
epoch: 194, train time every whole data:237.24s
epoch: 194, total time:29990.49s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.8070s
epoch: 195, train time every whole data:237.51s
epoch: 195, total time:30276.81s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5400s
epoch: 196, train time every whole data:236.90s
epoch: 196, total time:30562.25s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5280s
epoch: 197, train time every whole data:236.47s
epoch: 197, total time:30847.26s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5560s
epoch: 198, train time every whole data:237.11s
epoch: 198, total time:31132.93s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6240s
epoch: 199, train time every whole data:237.14s
epoch: 199, total time:31418.70s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.4730s
epoch: 200, train time every whole data:237.02s
epoch: 200, total time:31704.20s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.4390s
epoch: 201, train time every whole data:236.53s
epoch: 201, total time:31989.16s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6940s
epoch: 202, train time every whole data:237.22s
epoch: 202, total time:32275.08s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.7910s
epoch: 203, train time every whole data:238.25s
epoch: 203, total time:32562.13s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5640s
epoch: 204, train time every whole data:237.24s
epoch: 204, total time:32847.94s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6650s
epoch: 205, train time every whole data:236.90s
epoch: 205, total time:33133.50s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5520s
epoch: 206, train time every whole data:237.64s
epoch: 206, total time:33419.70s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.2920s
epoch: 207, train time every whole data:236.97s
epoch: 207, total time:33704.96s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5140s
epoch: 208, train time every whole data:237.34s
epoch: 208, total time:33990.82s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.7090s
epoch: 209, train time every whole data:236.74s
epoch: 209, total time:34276.27s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.4690s
epoch: 210, train time every whole data:237.32s
epoch: 210, total time:34562.07s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.3690s
epoch: 211, train time every whole data:237.09s
epoch: 211, total time:34847.54s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.4730s
epoch: 212, train time every whole data:237.09s
epoch: 212, total time:35133.10s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.7870s
epoch: 213, train time every whole data:237.57s
epoch: 213, total time:35419.46s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6250s
epoch: 214, train time every whole data:237.38s
epoch: 214, total time:35705.47s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.4870s
epoch: 215, train time every whole data:237.12s
epoch: 215, total time:35991.08s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6020s
epoch: 216, train time every whole data:236.96s
epoch: 216, total time:36276.64s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6630s
epoch: 217, train time every whole data:236.95s
epoch: 217, total time:36562.25s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5540s
epoch: 218, train time every whole data:236.56s
epoch: 218, total time:36847.37s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.4990s
epoch: 219, train time every whole data:236.71s
epoch: 219, total time:37132.58s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.7460s
epoch: 220, train time every whole data:237.83s
epoch: 220, total time:37419.16s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6970s
epoch: 221, train time every whole data:236.93s
epoch: 221, total time:37704.79s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.9050s
epoch: 222, train time every whole data:236.74s
epoch: 222, total time:37990.44s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.5260s
epoch: 223, train time every whole data:238.08s
epoch: 223, total time:38277.05s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6850s
epoch: 224, train time every whole data:237.43s
epoch: 224, total time:38563.16s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.8660s
epoch: 225, train time every whole data:236.91s
epoch: 225, total time:38848.95s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.7080s
epoch: 226, train time every whole data:236.93s
epoch: 226, total time:39134.59s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6430s
epoch: 227, train time every whole data:237.18s
epoch: 227, total time:39420.41s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.6460s
epoch: 228, train time every whole data:237.67s
epoch: 228, total time:39706.73s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 48.7090s
epoch: 229, train time every whole data:229.15s
epoch: 229, total time:39984.59s
validation batch 1 / 282, loss: 0.01
validation batch 101 / 282, loss: 0.01
validation batch 201 / 282, loss: 0.01
validation cost time: 39.6240s
best epoch: 189
apply the best val model on the test data set ...
load weight from: ../experiments\HZME_INFLOW\K4_MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_4TcontextScaledSAtSE1TE\epoch_189.params
predicting testing set batch 1 / 282, time: 0.16s
predicting testing set batch 101 / 282, time: 14.21s
predicting testing set batch 201 / 282, time: 28.28s
test time on whole data:39.72s
input: (1128, 80, 12, 1)
prediction: (1128, 80, 12, 1)
data_target_tensor: (1128, 80, 12)
current epoch: 189, predict 0 points
MAE: 10.06
RMSE: 16.74
MAPE: 22.34
current epoch: 189, predict 1 points
MAE: 10.50
RMSE: 18.21
MAPE: 22.59
current epoch: 189, predict 2 points
MAE: 10.73
RMSE: 18.73
MAPE: 22.79
current epoch: 189, predict 3 points
MAE: 10.82
RMSE: 18.90
MAPE: 22.96
current epoch: 189, predict 4 points
MAE: 10.84
RMSE: 18.85
MAPE: 23.00
current epoch: 189, predict 5 points
MAE: 10.95
RMSE: 18.97
MAPE: 23.19
current epoch: 189, predict 6 points
MAE: 11.15
RMSE: 19.51
MAPE: 23.55
current epoch: 189, predict 7 points
MAE: 11.31
RMSE: 19.84
MAPE: 23.96
current epoch: 189, predict 8 points
MAE: 11.47
RMSE: 20.36
MAPE: 24.42
current epoch: 189, predict 9 points
MAE: 11.64
RMSE: 20.76
MAPE: 24.77
current epoch: 189, predict 10 points
MAE: 11.74
RMSE: 20.93
MAPE: 25.17
current epoch: 189, predict 11 points
MAE: 11.91
RMSE: 21.40
MAPE: 25.77
all MAE: 11.09
all RMSE: 19.47
all MAPE: 23.69
[10.06117, 16.73926895058443, 22.338679432868958, 10.497882, 18.20603349886292, 22.592970728874207, 10.725988, 18.73156808882818, 22.79135137796402, 10.822684, 18.903420178200392, 22.958257794380188, 10.836755, 18.851499365310772, 22.995761036872864, 10.946398, 18.972805438084432, 23.18689227104187, 11.149897, 19.509748320801467, 23.55363965034485, 11.314145, 19.841303829297217, 23.960284888744354, 11.47215, 20.360182165743645, 24.418123066425323, 11.643648, 20.7591084171597, 24.77046251296997, 11.741137, 20.925346285963823, 25.170186161994934, 11.914413, 21.3981244557697, 25.76708495616913, 11.093854, 19.473818687094028, 23.69154691696167]

Process finished with exit code 0
