Thu Nov  4 14:19:53 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   40C    P0    43W / 250W |  17086MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   58C    P0   130W / 250W |  14376MiB / 32480MiB |     88%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   59C    P0   187W / 250W |  14376MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   57C    P0   119W / 250W |  14377MiB / 32480MiB |     89%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   53C    P0   126W / 250W |  14376MiB / 32480MiB |     80%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   60C    P0   146W / 250W |  14376MiB / 32480MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   29C    P0    24W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   27C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     10414      C   python                                     17075MiB |
|    1     13876      C   python                                     14365MiB |
|    2     14233      C   python                                     14365MiB |
|    3     14433      C   python                                     14365MiB |
|    4     14662      C   python                                     14365MiB |
|    5     14863      C   python                                     14365MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u18112042/CorrSTN/configurations/HZME_INFLOW_rdw.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 5
folder_dir: MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE
load file: /data/home/u18112042/CorrSTN/data/HZME_INFLOW/HZME_INFLOW_r1_d1_w0.npz
ori length: 4313 , percent: 1.0 , scale: 4313
train: torch.Size([3218, 80, 1, 24]) torch.Size([3218, 80, 12]) torch.Size([3218, 80, 12])
val: torch.Size([1073, 80, 1, 24]) torch.Size([1073, 80, 12]) torch.Size([1073, 80, 12])
test: torch.Size([1073, 80, 1, 24]) torch.Size([1073, 80, 12]) torch.Size([1073, 80, 12])
TemporalPositionalEncoding max_len: 288
w_index: []
d_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
h_index: [276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 288, 64])
src_embed.2.embedding.weight 	 torch.Size([80, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([80, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 469845
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 3, loss: 2.00
validation cost time: 9.2920s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:24.29s
epoch: 0, total time:33.60s
validation batch 1 / 3, loss: 0.07
validation cost time: 8.7886s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:24.27s
epoch: 1, total time:66.67s
validation batch 1 / 3, loss: 0.07
validation cost time: 8.7888s
epoch: 2, train time every whole data:24.25s
epoch: 2, total time:99.70s
validation batch 1 / 3, loss: 0.09
validation cost time: 8.7885s
epoch: 3, train time every whole data:24.25s
epoch: 3, total time:132.75s
validation batch 1 / 3, loss: 0.06
validation cost time: 8.7899s
epoch: 4, train time every whole data:24.26s
epoch: 4, total time:165.80s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7885s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_5.params
epoch: 5, train time every whole data:24.25s
epoch: 5, total time:198.85s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7903s
epoch: 6, train time every whole data:24.28s
epoch: 6, total time:231.92s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7890s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_7.params
epoch: 7, train time every whole data:24.26s
epoch: 7, total time:264.98s
validation batch 1 / 3, loss: 0.06
validation cost time: 8.7897s
epoch: 8, train time every whole data:24.26s
epoch: 8, total time:298.03s
validation batch 1 / 3, loss: 0.10
validation cost time: 8.7896s
epoch: 9, train time every whole data:24.27s
epoch: 9, total time:331.09s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.7902s
epoch: 10, train time every whole data:24.26s
epoch: 10, total time:364.14s
validation batch 1 / 3, loss: 0.16
validation cost time: 8.7890s
epoch: 11, train time every whole data:24.26s
epoch: 11, total time:397.20s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7896s
epoch: 12, train time every whole data:24.26s
epoch: 12, total time:430.25s
validation batch 1 / 3, loss: 0.09
validation cost time: 8.7888s
epoch: 13, train time every whole data:24.25s
epoch: 13, total time:463.30s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7895s
epoch: 14, train time every whole data:24.26s
epoch: 14, total time:496.35s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7895s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_15.params
epoch: 15, train time every whole data:24.25s
epoch: 15, total time:529.40s
validation batch 1 / 3, loss: 0.06
validation cost time: 8.7887s
epoch: 16, train time every whole data:24.25s
epoch: 16, total time:562.44s
validation batch 1 / 3, loss: 0.05
validation cost time: 8.7889s
epoch: 17, train time every whole data:24.27s
epoch: 17, total time:595.50s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7911s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_18.params
epoch: 18, train time every whole data:24.27s
epoch: 18, total time:628.57s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7938s
epoch: 19, train time every whole data:24.28s
epoch: 19, total time:661.65s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.8295s
epoch: 20, train time every whole data:24.26s
epoch: 20, total time:694.74s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7931s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_21.params
epoch: 21, train time every whole data:24.25s
epoch: 21, total time:727.81s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7893s
epoch: 22, train time every whole data:24.26s
epoch: 22, total time:760.85s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7885s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_23.params
epoch: 23, train time every whole data:24.27s
epoch: 23, total time:793.92s
validation batch 1 / 3, loss: 0.06
validation cost time: 8.7893s
epoch: 24, train time every whole data:24.27s
epoch: 24, total time:826.98s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7893s
epoch: 25, train time every whole data:24.27s
epoch: 25, total time:860.04s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7895s
epoch: 26, train time every whole data:24.27s
epoch: 26, total time:893.10s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7891s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_27.params
epoch: 27, train time every whole data:24.27s
epoch: 27, total time:926.17s
validation batch 1 / 3, loss: 0.05
validation cost time: 8.7891s
epoch: 28, train time every whole data:24.26s
epoch: 28, total time:959.22s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.7900s
epoch: 29, train time every whole data:24.25s
epoch: 29, total time:992.27s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.7889s
epoch: 30, train time every whole data:24.26s
epoch: 30, total time:1025.32s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7896s
epoch: 31, train time every whole data:24.26s
epoch: 31, total time:1058.37s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.7920s
epoch: 32, train time every whole data:24.26s
epoch: 32, total time:1091.42s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7912s
epoch: 33, train time every whole data:24.29s
epoch: 33, total time:1124.51s
validation batch 1 / 3, loss: 0.05
validation cost time: 8.7944s
epoch: 34, train time every whole data:24.28s
epoch: 34, total time:1157.59s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7933s
epoch: 35, train time every whole data:24.29s
epoch: 35, total time:1190.67s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7932s
epoch: 36, train time every whole data:24.29s
epoch: 36, total time:1223.76s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7940s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_37.params
epoch: 37, train time every whole data:24.28s
epoch: 37, total time:1256.85s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.7934s
epoch: 38, train time every whole data:24.28s
epoch: 38, total time:1289.92s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7893s
epoch: 39, train time every whole data:24.27s
epoch: 39, total time:1322.98s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7892s
epoch: 40, train time every whole data:24.27s
epoch: 40, total time:1356.05s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7919s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_41.params
epoch: 41, train time every whole data:24.27s
epoch: 41, total time:1389.12s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7895s
epoch: 42, train time every whole data:24.28s
epoch: 42, total time:1422.20s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7913s
epoch: 43, train time every whole data:24.28s
epoch: 43, total time:1455.27s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7890s
epoch: 44, train time every whole data:24.27s
epoch: 44, total time:1488.34s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7881s
epoch: 45, train time every whole data:24.27s
epoch: 45, total time:1521.40s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7893s
epoch: 46, train time every whole data:24.28s
epoch: 46, total time:1554.47s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7938s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_47.params
epoch: 47, train time every whole data:24.28s
epoch: 47, total time:1587.56s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7895s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_48.params
epoch: 48, train time every whole data:24.28s
epoch: 48, total time:1620.64s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7918s
epoch: 49, train time every whole data:24.29s
epoch: 49, total time:1653.72s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7889s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_50.params
epoch: 50, train time every whole data:24.27s
epoch: 50, total time:1686.80s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7889s
epoch: 51, train time every whole data:24.28s
epoch: 51, total time:1719.87s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.7896s
epoch: 52, train time every whole data:24.28s
epoch: 52, total time:1752.94s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7885s
epoch: 53, train time every whole data:24.26s
epoch: 53, total time:1785.99s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7891s
epoch: 54, train time every whole data:24.26s
epoch: 54, total time:1819.04s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7894s
epoch: 55, train time every whole data:24.28s
epoch: 55, total time:1852.12s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7896s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_56.params
epoch: 56, train time every whole data:24.28s
epoch: 56, total time:1885.20s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7903s
epoch: 57, train time every whole data:24.30s
epoch: 57, total time:1918.30s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.8215s
epoch: 58, train time every whole data:24.28s
epoch: 58, total time:1951.40s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7895s
epoch: 59, train time every whole data:24.28s
epoch: 59, total time:1984.47s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7886s
epoch: 60, train time every whole data:24.26s
epoch: 60, total time:2017.52s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7886s
epoch: 61, train time every whole data:24.26s
epoch: 61, total time:2050.57s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7887s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_62.params
epoch: 62, train time every whole data:24.27s
epoch: 62, total time:2083.65s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7891s
epoch: 63, train time every whole data:24.26s
epoch: 63, total time:2116.70s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7886s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_64.params
epoch: 64, train time every whole data:24.27s
epoch: 64, total time:2149.77s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7914s
epoch: 65, train time every whole data:24.30s
epoch: 65, total time:2182.86s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7889s
epoch: 66, train time every whole data:24.27s
epoch: 66, total time:2215.92s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7891s
epoch: 67, train time every whole data:24.29s
epoch: 67, total time:2249.00s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7887s
epoch: 68, train time every whole data:24.27s
epoch: 68, total time:2282.07s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7889s
epoch: 69, train time every whole data:24.27s
epoch: 69, total time:2315.12s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7883s
epoch: 70, train time every whole data:24.27s
epoch: 70, total time:2348.19s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7885s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_71.params
epoch: 71, train time every whole data:24.27s
epoch: 71, total time:2381.26s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7899s
epoch: 72, train time every whole data:24.28s
epoch: 72, total time:2414.33s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7934s
epoch: 73, train time every whole data:24.29s
epoch: 73, total time:2447.42s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7890s
epoch: 74, train time every whole data:24.28s
epoch: 74, total time:2480.49s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7889s
epoch: 75, train time every whole data:24.27s
epoch: 75, total time:2513.55s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7893s
epoch: 76, train time every whole data:24.27s
epoch: 76, total time:2546.61s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7889s
epoch: 77, train time every whole data:24.27s
epoch: 77, total time:2579.68s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7891s
epoch: 78, train time every whole data:24.26s
epoch: 78, total time:2612.73s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7904s
epoch: 79, train time every whole data:24.30s
epoch: 79, total time:2645.82s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7940s
epoch: 80, train time every whole data:24.29s
epoch: 80, total time:2678.91s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7890s
epoch: 81, train time every whole data:24.26s
epoch: 81, total time:2711.96s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7890s
epoch: 82, train time every whole data:24.27s
epoch: 82, total time:2745.02s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7899s
epoch: 83, train time every whole data:24.27s
epoch: 83, total time:2778.08s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7889s
epoch: 84, train time every whole data:24.26s
epoch: 84, total time:2811.13s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7885s
epoch: 85, train time every whole data:24.28s
epoch: 85, total time:2844.20s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7903s
epoch: 86, train time every whole data:24.27s
epoch: 86, total time:2877.27s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7902s
epoch: 87, train time every whole data:24.29s
epoch: 87, total time:2910.35s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7936s
epoch: 88, train time every whole data:24.29s
epoch: 88, total time:2943.43s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7890s
epoch: 89, train time every whole data:24.28s
epoch: 89, total time:2976.50s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7899s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_90.params
epoch: 90, train time every whole data:24.28s
epoch: 90, total time:3009.58s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7895s
epoch: 91, train time every whole data:24.29s
epoch: 91, total time:3042.66s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7901s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_92.params
epoch: 92, train time every whole data:24.29s
epoch: 92, total time:3075.76s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.8263s
epoch: 93, train time every whole data:24.32s
epoch: 93, total time:3108.90s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7907s
epoch: 94, train time every whole data:24.28s
epoch: 94, total time:3141.97s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7893s
epoch: 95, train time every whole data:24.31s
epoch: 95, total time:3175.07s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.7927s
epoch: 96, train time every whole data:24.29s
epoch: 96, total time:3208.16s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7945s
epoch: 97, train time every whole data:24.33s
epoch: 97, total time:3241.28s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7899s
epoch: 98, train time every whole data:24.27s
epoch: 98, total time:3274.34s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7901s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_99.params
epoch: 99, train time every whole data:24.32s
epoch: 99, total time:3307.47s
best epoch: 99
apply the best val model on the test data set ...
load weight from: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_99.params
predicting testing set batch 1 / 3, time: 4.18s
test time on whole data:8.80s
input: (1073, 80, 24, 1)
prediction: (1073, 80, 12, 1)
data_target_tensor: (1073, 80, 12)
current epoch: 99, predict 0 points
MAE: 10.04
RMSE: 15.88
MAPE: 23.41
current epoch: 99, predict 1 points
MAE: 10.28
RMSE: 16.62
MAPE: 23.56
current epoch: 99, predict 2 points
MAE: 10.37
RMSE: 16.75
MAPE: 23.59
current epoch: 99, predict 3 points
MAE: 10.47
RMSE: 16.95
MAPE: 23.83
current epoch: 99, predict 4 points
MAE: 10.55
RMSE: 17.14
MAPE: 24.08
current epoch: 99, predict 5 points
MAE: 10.70
RMSE: 17.35
MAPE: 24.35
current epoch: 99, predict 6 points
MAE: 10.84
RMSE: 17.60
MAPE: 24.41
current epoch: 99, predict 7 points
MAE: 11.03
RMSE: 18.05
MAPE: 24.51
current epoch: 99, predict 8 points
MAE: 11.15
RMSE: 18.45
MAPE: 24.63
current epoch: 99, predict 9 points
MAE: 11.30
RMSE: 18.78
MAPE: 24.90
current epoch: 99, predict 10 points
MAE: 11.43
RMSE: 19.14
MAPE: 25.35
current epoch: 99, predict 11 points
MAE: 11.73
RMSE: 19.70
MAPE: 26.07
all MAE: 10.82
all RMSE: 17.74
all MAPE: 24.38
[10.039647, 15.878024556548569, 23.406553268432617, 10.276197, 16.621561483577402, 23.557263612747192, 10.370802, 16.747504660501384, 23.59158545732498, 10.468344, 16.950971925489995, 23.829269409179688, 10.548346, 17.13799244112651, 24.076354503631592, 10.7025585, 17.3528551550242, 24.35273826122284, 10.839592, 17.604189010094302, 24.41350966691971, 11.034121, 18.052272765553735, 24.5070219039917, 11.1481085, 18.452509780176246, 24.6349036693573, 11.302104, 18.78249950346482, 24.90105628967285, 11.434, 19.13567936744122, 25.34846067428589, 11.731176, 19.70479848290647, 26.070481538772583, 10.824586, 17.735456499237007, 24.380020797252655]
fine tune the model ... 
epoch: 100, train time every whole data:56.65s
epoch: 100, total time:3373.04s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7898s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_100.params
epoch: 101, train time every whole data:56.60s
epoch: 101, total time:3438.45s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7901s
epoch: 102, train time every whole data:56.59s
epoch: 102, total time:3503.83s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7898s
epoch: 103, train time every whole data:56.57s
epoch: 103, total time:3569.19s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7895s
epoch: 104, train time every whole data:56.60s
epoch: 104, total time:3634.58s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7898s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_104.params
epoch: 105, train time every whole data:56.59s
epoch: 105, total time:3699.98s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7906s
epoch: 106, train time every whole data:56.59s
epoch: 106, total time:3765.36s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7900s
epoch: 107, train time every whole data:56.56s
epoch: 107, total time:3830.71s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7907s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_107.params
epoch: 108, train time every whole data:56.60s
epoch: 108, total time:3896.12s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7904s
epoch: 109, train time every whole data:56.53s
epoch: 109, total time:3961.44s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7983s
epoch: 110, train time every whole data:56.58s
epoch: 110, total time:4026.82s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7900s
epoch: 111, train time every whole data:56.57s
epoch: 111, total time:4092.18s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7903s
epoch: 112, train time every whole data:56.55s
epoch: 112, total time:4157.52s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7898s
epoch: 113, train time every whole data:56.49s
epoch: 113, total time:4222.80s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7912s
epoch: 114, train time every whole data:56.61s
epoch: 114, total time:4288.20s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7907s
epoch: 115, train time every whole data:56.62s
epoch: 115, total time:4353.61s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7898s
epoch: 116, train time every whole data:56.57s
epoch: 116, total time:4418.98s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7898s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_116.params
epoch: 117, train time every whole data:56.61s
epoch: 117, total time:4484.39s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7893s
epoch: 118, train time every whole data:56.55s
epoch: 118, total time:4549.73s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7904s
epoch: 119, train time every whole data:56.58s
epoch: 119, total time:4615.10s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7901s
epoch: 120, train time every whole data:56.51s
epoch: 120, total time:4680.40s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7903s
epoch: 121, train time every whole data:56.61s
epoch: 121, total time:4745.80s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7904s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_121.params
epoch: 122, train time every whole data:56.58s
epoch: 122, total time:4811.19s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7944s
epoch: 123, train time every whole data:56.61s
epoch: 123, total time:4876.59s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7951s
epoch: 124, train time every whole data:56.47s
epoch: 124, total time:4941.86s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7897s
epoch: 125, train time every whole data:56.57s
epoch: 125, total time:5007.23s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7894s
epoch: 126, train time every whole data:56.58s
epoch: 126, total time:5072.60s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.8235s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_126.params
epoch: 127, train time every whole data:56.55s
epoch: 127, total time:5137.98s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7906s
epoch: 128, train time every whole data:56.54s
epoch: 128, total time:5203.32s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7952s
epoch: 129, train time every whole data:56.54s
epoch: 129, total time:5268.66s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7904s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_129.params
epoch: 130, train time every whole data:56.54s
epoch: 130, total time:5334.00s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7905s
epoch: 131, train time every whole data:56.52s
epoch: 131, total time:5399.32s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7907s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_131.params
epoch: 132, train time every whole data:56.62s
epoch: 132, total time:5464.74s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7917s
epoch: 133, train time every whole data:56.52s
epoch: 133, total time:5530.05s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7923s
epoch: 134, train time every whole data:56.57s
epoch: 134, total time:5595.41s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7901s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_134.params
epoch: 135, train time every whole data:56.60s
epoch: 135, total time:5660.82s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7899s
epoch: 136, train time every whole data:56.57s
epoch: 136, total time:5726.19s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7898s
epoch: 137, train time every whole data:56.54s
epoch: 137, total time:5791.52s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7901s
epoch: 138, train time every whole data:56.49s
epoch: 138, total time:5856.80s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7910s
epoch: 139, train time every whole data:56.52s
epoch: 139, total time:5922.11s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7936s
epoch: 140, train time every whole data:56.54s
epoch: 140, total time:5987.44s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7908s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_140.params
epoch: 141, train time every whole data:56.60s
epoch: 141, total time:6052.85s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7901s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_141.params
epoch: 142, train time every whole data:56.58s
epoch: 142, total time:6118.23s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7899s
save parameters to file: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_142.params
epoch: 143, train time every whole data:56.59s
epoch: 143, total time:6183.62s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7902s
epoch: 144, train time every whole data:56.70s
epoch: 144, total time:6249.12s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7926s
epoch: 145, train time every whole data:56.48s
epoch: 145, total time:6314.39s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7919s
epoch: 146, train time every whole data:56.61s
epoch: 146, total time:6379.80s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7885s
epoch: 147, train time every whole data:56.33s
epoch: 147, total time:6444.92s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7911s
epoch: 148, train time every whole data:56.50s
epoch: 148, total time:6510.21s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7944s
epoch: 149, train time every whole data:56.32s
epoch: 149, total time:6575.32s
validation batch 1 / 3, loss: 0.01
validation cost time: 8.7888s
best epoch: 142
apply the best val model on the test data set ...
load weight from: ../experiments/HZME_INFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_142.params
predicting testing set batch 1 / 3, time: 4.17s
test time on whole data:8.79s
input: (1073, 80, 24, 1)
prediction: (1073, 80, 12, 1)
data_target_tensor: (1073, 80, 12)
current epoch: 142, predict 0 points
MAE: 9.88
RMSE: 15.69
MAPE: 22.23
current epoch: 142, predict 1 points
MAE: 10.11
RMSE: 16.41
MAPE: 22.18
current epoch: 142, predict 2 points
MAE: 10.14
RMSE: 16.50
MAPE: 22.24
current epoch: 142, predict 3 points
MAE: 10.20
RMSE: 16.63
MAPE: 22.29
current epoch: 142, predict 4 points
MAE: 10.20
RMSE: 16.65
MAPE: 22.43
current epoch: 142, predict 5 points
MAE: 10.25
RMSE: 16.74
MAPE: 22.55
current epoch: 142, predict 6 points
MAE: 10.33
RMSE: 16.92
MAPE: 22.76
current epoch: 142, predict 7 points
MAE: 10.42
RMSE: 17.13
MAPE: 23.05
current epoch: 142, predict 8 points
MAE: 10.47
RMSE: 17.25
MAPE: 23.43
current epoch: 142, predict 9 points
MAE: 10.54
RMSE: 17.42
MAPE: 23.73
current epoch: 142, predict 10 points
MAE: 10.64
RMSE: 17.76
MAPE: 24.27
current epoch: 142, predict 11 points
MAE: 10.85
RMSE: 18.12
MAPE: 24.98
all MAE: 10.34
all RMSE: 16.95
all MAPE: 23.00
[9.881243, 15.692793359620026, 22.22732901573181, 10.11043, 16.414179631305064, 22.181436419487, 10.142047, 16.497227291299023, 22.241415083408356, 10.196175, 16.632711586706556, 22.293899953365326, 10.200056, 16.645057911610262, 22.42918759584427, 10.25208, 16.7391468015706, 22.55096435546875, 10.333946, 16.92292366758724, 22.758755087852478, 10.418476, 17.133283630378166, 23.052246868610382, 10.470678, 17.252863107333535, 23.432034254074097, 10.543315, 17.417730904268662, 23.72543066740036, 10.643859, 17.76225352106214, 24.26801770925522, 10.852928, 18.11804149830701, 24.9838724732399, 10.337105, 16.947188080560462, 22.999970614910126]
