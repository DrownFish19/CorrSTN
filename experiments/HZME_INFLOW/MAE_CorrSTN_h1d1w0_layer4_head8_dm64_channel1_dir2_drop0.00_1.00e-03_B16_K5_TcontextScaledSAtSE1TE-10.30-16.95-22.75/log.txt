C:\Users\developer\anaconda3\envs\pytorch1.7-python3.8\python.exe C:/Users/developer/Documents/GitHub/Research/CorrSTN/train_ASTGNN.py --config configurations/HZME_INFLOW_rdw.conf
Wed Nov  3 22:28:58 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:17:00.0  On |                  N/A |
| 50%   58C    P2   240W / 350W |  15534MiB / 24576MiB |     84%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1500    C+G   C:\Windows\System32\dwm.exe     N/A      |
|    0   N/A  N/A      2920    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |
|    0   N/A  N/A      4488    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     10044    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     10088    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A     10280    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     12320    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |
|    0   N/A  N/A     12616    C+G   ...perience\NVIDIA Share.exe    N/A      |
|    0   N/A  N/A     13660    C+G   ...nputApp\TextInputHost.exe    N/A      |
|    0   N/A  N/A     15604    C+G   ...ropbox\Client\Dropbox.exe    N/A      |
|    0   N/A  N/A     17644    C+G   ...b3d8bbwe\WinStore.App.exe    N/A      |
|    0   N/A  N/A     20532    C+G   ...kyb3d8bbwe\Calculator.exe    N/A      |
|    0   N/A  N/A     30548      C   ...h1.7-python3.8\python.exe    N/A      |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: configurations/HZME_INFLOW_rdw.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 5
folder_dir: MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE
load file: data/HZME_INFLOW\HZME_INFLOW_r1_d1_w0.npz
ori length: 4313 , percent: 1.0 , scale: 4313
train: torch.Size([3218, 80, 1, 24]) torch.Size([3218, 80, 12]) torch.Size([3218, 80, 12])
val: torch.Size([1073, 80, 1, 24]) torch.Size([1073, 80, 12]) torch.Size([1073, 80, 12])
test: torch.Size([1073, 80, 1, 24]) torch.Size([1073, 80, 12]) torch.Size([1073, 80, 12])
TemporalPositionalEncoding max_len: 288
w_index: []
d_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
h_index: [276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
delete the old one and create params directory ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 288, 64])
src_embed.2.embedding.weight 	 torch.Size([80, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([80, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 469845
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 5, loss: 1.98
validation cost time: 14.2180s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_0.params
epoch: 0, train time every whole data:55.26s
epoch: 0, total time:69.51s
validation batch 1 / 5, loss: 0.18
validation cost time: 11.3626s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_1.params
epoch: 1, train time every whole data:54.40s
epoch: 1, total time:135.30s
validation batch 1 / 5, loss: 0.08
validation cost time: 11.3120s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_2.params
epoch: 2, train time every whole data:54.78s
epoch: 2, total time:201.42s
validation batch 1 / 5, loss: 0.08
validation cost time: 11.5202s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_3.params
epoch: 3, train time every whole data:55.62s
epoch: 3, total time:268.59s
validation batch 1 / 5, loss: 0.04
validation cost time: 11.4474s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_4.params
epoch: 4, train time every whole data:56.11s
epoch: 4, total time:336.16s
validation batch 1 / 5, loss: 0.09
validation cost time: 11.0534s
epoch: 5, train time every whole data:55.28s
epoch: 5, total time:402.50s
validation batch 1 / 5, loss: 0.10
validation cost time: 10.9697s
epoch: 6, train time every whole data:55.10s
epoch: 6, total time:468.57s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.8290s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_7.params
epoch: 7, train time every whole data:55.21s
epoch: 7, total time:534.63s
validation batch 1 / 5, loss: 0.11
validation cost time: 10.9437s
epoch: 8, train time every whole data:54.78s
epoch: 8, total time:600.36s
validation batch 1 / 5, loss: 0.12
validation cost time: 10.9846s
epoch: 9, train time every whole data:54.77s
epoch: 9, total time:666.12s
validation batch 1 / 5, loss: 0.04
validation cost time: 10.9118s
epoch: 10, train time every whole data:55.01s
epoch: 10, total time:732.04s
validation batch 1 / 5, loss: 0.09
validation cost time: 10.8919s
epoch: 11, train time every whole data:54.69s
epoch: 11, total time:797.63s
validation batch 1 / 5, loss: 0.08
validation cost time: 10.9687s
epoch: 12, train time every whole data:54.64s
epoch: 12, total time:863.24s
validation batch 1 / 5, loss: 0.07
validation cost time: 11.6349s
epoch: 13, train time every whole data:54.27s
epoch: 13, total time:929.14s
validation batch 1 / 5, loss: 0.02
validation cost time: 13.1443s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_14.params
epoch: 14, train time every whole data:52.67s
epoch: 14, total time:995.03s
validation batch 1 / 5, loss: 0.05
validation cost time: 14.3646s
epoch: 15, train time every whole data:51.62s
epoch: 15, total time:1061.01s
validation batch 1 / 5, loss: 0.05
validation cost time: 15.8287s
epoch: 16, train time every whole data:50.13s
epoch: 16, total time:1126.98s
validation batch 1 / 5, loss: 0.04
validation cost time: 16.0202s
epoch: 17, train time every whole data:50.09s
epoch: 17, total time:1193.09s
validation batch 1 / 5, loss: 0.06
validation cost time: 16.0600s
epoch: 18, train time every whole data:50.95s
epoch: 18, total time:1260.10s
validation batch 1 / 5, loss: 0.03
validation cost time: 15.2103s
epoch: 19, train time every whole data:51.59s
epoch: 19, total time:1326.91s
validation batch 1 / 5, loss: 0.05
validation cost time: 14.2439s
epoch: 20, train time every whole data:52.84s
epoch: 20, total time:1393.99s
validation batch 1 / 5, loss: 0.03
validation cost time: 12.9484s
epoch: 21, train time every whole data:53.82s
epoch: 21, total time:1460.76s
validation batch 1 / 5, loss: 0.03
validation cost time: 11.9111s
epoch: 22, train time every whole data:55.08s
epoch: 22, total time:1527.76s
validation batch 1 / 5, loss: 0.03
validation cost time: 11.0903s
epoch: 23, train time every whole data:54.60s
epoch: 23, total time:1593.45s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0744s
epoch: 24, train time every whole data:54.73s
epoch: 24, total time:1659.26s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.1362s
epoch: 25, train time every whole data:54.58s
epoch: 25, total time:1724.98s
validation batch 1 / 5, loss: 0.05
validation cost time: 11.0096s
epoch: 26, train time every whole data:55.44s
epoch: 26, total time:1791.43s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.9746s
epoch: 27, train time every whole data:55.03s
epoch: 27, total time:1857.43s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8370s
epoch: 28, train time every whole data:54.95s
epoch: 28, total time:1923.22s
validation batch 1 / 5, loss: 0.07
validation cost time: 10.8101s
epoch: 29, train time every whole data:55.27s
epoch: 29, total time:1989.30s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.9158s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_30.params
epoch: 30, train time every whole data:55.66s
epoch: 30, total time:2055.90s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.8799s
epoch: 31, train time every whole data:55.16s
epoch: 31, total time:2121.94s
validation batch 1 / 5, loss: 0.05
validation cost time: 10.8330s
epoch: 32, train time every whole data:55.43s
epoch: 32, total time:2188.21s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8789s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_33.params
epoch: 33, train time every whole data:55.27s
epoch: 33, total time:2254.39s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8719s
epoch: 34, train time every whole data:55.52s
epoch: 34, total time:2320.79s
validation batch 1 / 5, loss: 0.06
validation cost time: 10.8699s
epoch: 35, train time every whole data:55.29s
epoch: 35, total time:2386.95s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.8330s
epoch: 36, train time every whole data:55.16s
epoch: 36, total time:2452.95s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0854s
epoch: 37, train time every whole data:55.39s
epoch: 37, total time:2519.43s
validation batch 1 / 5, loss: 0.04
validation cost time: 11.0026s
epoch: 38, train time every whole data:55.21s
epoch: 38, total time:2585.64s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.9896s
epoch: 39, train time every whole data:55.24s
epoch: 39, total time:2651.88s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0225s
epoch: 40, train time every whole data:55.22s
epoch: 40, total time:2718.12s
validation batch 1 / 5, loss: 0.03
validation cost time: 11.0475s
epoch: 41, train time every whole data:55.51s
epoch: 41, total time:2784.68s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0325s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_42.params
epoch: 42, train time every whole data:55.12s
epoch: 42, total time:2850.88s
validation batch 1 / 5, loss: 0.03
validation cost time: 11.0066s
epoch: 43, train time every whole data:55.29s
epoch: 43, total time:2917.18s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8899s
epoch: 44, train time every whole data:55.30s
epoch: 44, total time:2983.37s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.8440s
epoch: 45, train time every whole data:55.38s
epoch: 45, total time:3049.60s
validation batch 1 / 5, loss: 0.05
validation cost time: 10.9018s
epoch: 46, train time every whole data:55.30s
epoch: 46, total time:3115.80s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8799s
epoch: 47, train time every whole data:55.24s
epoch: 47, total time:3181.92s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8540s
epoch: 48, train time every whole data:55.14s
epoch: 48, total time:3247.92s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.8430s
epoch: 49, train time every whole data:55.05s
epoch: 49, total time:3313.82s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.8480s
epoch: 50, train time every whole data:55.61s
epoch: 50, total time:3380.27s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8530s
epoch: 51, train time every whole data:55.26s
epoch: 51, total time:3446.39s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0116s
epoch: 52, train time every whole data:55.27s
epoch: 52, total time:3512.68s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0046s
epoch: 53, train time every whole data:55.39s
epoch: 53, total time:3579.07s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0475s
epoch: 54, train time every whole data:55.04s
epoch: 54, total time:3645.16s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0953s
epoch: 55, train time every whole data:54.90s
epoch: 55, total time:3711.16s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0195s
epoch: 56, train time every whole data:55.09s
epoch: 56, total time:3777.28s
validation batch 1 / 5, loss: 0.04
validation cost time: 10.9756s
epoch: 57, train time every whole data:55.02s
epoch: 57, total time:3843.28s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0115s
epoch: 58, train time every whole data:54.95s
epoch: 58, total time:3909.24s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8799s
epoch: 59, train time every whole data:55.19s
epoch: 59, total time:3975.32s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8580s
epoch: 60, train time every whole data:55.15s
epoch: 60, total time:4041.33s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.2350s
epoch: 61, train time every whole data:54.67s
epoch: 61, total time:4107.24s
validation batch 1 / 5, loss: 0.01
validation cost time: 12.5066s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_62.params
epoch: 62, train time every whole data:53.57s
epoch: 62, total time:4173.36s
validation batch 1 / 5, loss: 0.02
validation cost time: 13.8909s
epoch: 63, train time every whole data:52.40s
epoch: 63, total time:4239.66s
validation batch 1 / 5, loss: 0.02
validation cost time: 15.0178s
epoch: 64, train time every whole data:51.27s
epoch: 64, total time:4305.94s
validation batch 1 / 5, loss: 0.01
validation cost time: 15.9843s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_65.params
epoch: 65, train time every whole data:50.33s
epoch: 65, total time:4372.30s
validation batch 1 / 5, loss: 0.02
validation cost time: 15.8536s
epoch: 66, train time every whole data:51.23s
epoch: 66, total time:4439.38s
validation batch 1 / 5, loss: 0.02
validation cost time: 15.1894s
epoch: 67, train time every whole data:51.94s
epoch: 67, total time:4506.51s
validation batch 1 / 5, loss: 0.02
validation cost time: 14.1502s
epoch: 68, train time every whole data:53.06s
epoch: 68, total time:4573.73s
validation batch 1 / 5, loss: 0.02
validation cost time: 13.1069s
epoch: 69, train time every whole data:54.10s
epoch: 69, total time:4640.93s
validation batch 1 / 5, loss: 0.02
validation cost time: 12.0837s
epoch: 70, train time every whole data:55.00s
epoch: 70, total time:4708.02s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.1532s
epoch: 71, train time every whole data:55.20s
epoch: 71, total time:4774.37s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0245s
epoch: 72, train time every whole data:55.02s
epoch: 72, total time:4840.42s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0046s
epoch: 73, train time every whole data:55.23s
epoch: 73, total time:4906.66s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.1063s
epoch: 74, train time every whole data:55.08s
epoch: 74, total time:4972.85s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0375s
epoch: 75, train time every whole data:55.21s
epoch: 75, total time:5039.10s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0604s
epoch: 76, train time every whole data:55.22s
epoch: 76, total time:5105.38s
validation batch 1 / 5, loss: 0.05
validation cost time: 10.8330s
epoch: 77, train time every whole data:55.10s
epoch: 77, total time:5171.31s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8619s
epoch: 78, train time every whole data:55.24s
epoch: 78, total time:5237.41s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8430s
epoch: 79, train time every whole data:55.11s
epoch: 79, total time:5303.37s
validation batch 1 / 5, loss: 0.01
validation cost time: 10.8679s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_80.params
epoch: 80, train time every whole data:55.52s
epoch: 80, total time:5369.79s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8959s
epoch: 81, train time every whole data:55.22s
epoch: 81, total time:5435.90s
validation batch 1 / 5, loss: 0.04
validation cost time: 10.8590s
epoch: 82, train time every whole data:55.16s
epoch: 82, total time:5501.93s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.8679s
epoch: 83, train time every whole data:55.87s
epoch: 83, total time:5568.67s
validation batch 1 / 5, loss: 0.01
validation cost time: 10.8689s
epoch: 84, train time every whole data:55.34s
epoch: 84, total time:5634.87s
validation batch 1 / 5, loss: 0.02
validation cost time: 10.8610s
epoch: 85, train time every whole data:55.02s
epoch: 85, total time:5700.76s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0265s
epoch: 86, train time every whole data:55.14s
epoch: 86, total time:5766.93s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0923s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_87.params
epoch: 87, train time every whole data:55.05s
epoch: 87, total time:5833.11s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0275s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_88.params
epoch: 88, train time every whole data:55.35s
epoch: 88, total time:5899.52s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0514s
epoch: 89, train time every whole data:55.15s
epoch: 89, total time:5965.73s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0056s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_90.params
epoch: 90, train time every whole data:55.16s
epoch: 90, total time:6031.92s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0345s
epoch: 91, train time every whole data:55.26s
epoch: 91, total time:6098.22s
validation batch 1 / 5, loss: 0.02
validation cost time: 11.0165s
epoch: 92, train time every whole data:55.08s
epoch: 92, total time:6164.31s
validation batch 1 / 5, loss: 0.01
validation cost time: 11.0145s
epoch: 93, train time every whole data:55.31s
epoch: 93, total time:6230.64s
validation batch 1 / 5, loss: 0.03
validation cost time: 10.9118s
epoch: 94, train time every whole data:53.43s
epoch: 94, total time:6294.98s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.8138s
epoch: 95, train time every whole data:47.02s
epoch: 95, total time:6351.82s
validation batch 1 / 5, loss: 0.02
validation cost time: 9.9574s
epoch: 96, train time every whole data:51.90s
epoch: 96, total time:6413.68s
validation batch 1 / 5, loss: 0.02
validation cost time: 9.9783s
epoch: 97, train time every whole data:47.16s
epoch: 97, total time:6470.82s
validation batch 1 / 5, loss: 0.02
validation cost time: 9.7609s
epoch: 98, train time every whole data:52.53s
epoch: 98, total time:6533.12s
validation batch 1 / 5, loss: 0.03
validation cost time: 9.9314s
epoch: 99, train time every whole data:47.29s
epoch: 99, total time:6590.34s
best epoch: 90
apply the best val model on the test data set ...
load weight from: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_90.params
predicting testing set batch 1 / 5, time: 2.29s
test time on whole data:9.79s
input: (1073, 80, 24, 1)
prediction: (1073, 80, 12, 1)
data_target_tensor: (1073, 80, 12)
current epoch: 90, predict 0 points
MAE: 10.07
RMSE: 16.04
MAPE: 22.49
current epoch: 90, predict 1 points
MAE: 10.29
RMSE: 16.72
MAPE: 22.59
current epoch: 90, predict 2 points
MAE: 10.33
RMSE: 16.73
MAPE: 22.54
current epoch: 90, predict 3 points
MAE: 10.38
RMSE: 16.88
MAPE: 22.50
current epoch: 90, predict 4 points
MAE: 10.48
RMSE: 17.14
MAPE: 22.40
current epoch: 90, predict 5 points
MAE: 10.58
RMSE: 17.41
MAPE: 22.59
current epoch: 90, predict 6 points
MAE: 10.73
RMSE: 17.68
MAPE: 22.74
current epoch: 90, predict 7 points
MAE: 10.85
RMSE: 17.98
MAPE: 23.14
current epoch: 90, predict 8 points
MAE: 10.94
RMSE: 18.28
MAPE: 23.40
current epoch: 90, predict 9 points
MAE: 11.03
RMSE: 18.48
MAPE: 23.48
current epoch: 90, predict 10 points
MAE: 11.19
RMSE: 18.94
MAPE: 23.77
current epoch: 90, predict 11 points
MAE: 11.47
RMSE: 19.45
MAPE: 24.35
all MAE: 10.70
all RMSE: 17.67
all MAPE: 22.99
[10.072819, 16.038870325809263, 22.49409109354019, 10.288005, 16.71591407752701, 22.593241930007935, 10.329896, 16.72523512511063, 22.540241479873657, 10.377893, 16.87877290722035, 22.502678632736206, 10.483046, 17.13557675488795, 22.39910215139389, 10.583067, 17.40581255762039, 22.59019762277603, 10.732333, 17.679662470983168, 22.742019593715668, 10.852983, 17.979268320915306, 23.142825067043304, 10.938158, 18.282950141296688, 23.39513897895813, 11.032004, 18.480604572397738, 23.483194410800934, 11.194646, 18.940596223528296, 23.768725991249084, 11.471158, 19.450567728499276, 24.352096021175385, 10.696333, 17.6696005250499, 22.99230843782425]
fine tune the model ...
epoch: 100, train time every whole data:106.65s
epoch: 100, total time:6707.05s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.8267s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_100.params
epoch: 101, train time every whole data:106.73s
epoch: 101, total time:6823.65s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.8417s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_101.params
epoch: 102, train time every whole data:106.70s
epoch: 102, total time:6940.22s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.8965s
epoch: 103, train time every whole data:106.56s
epoch: 103, total time:7056.67s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9703s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_103.params
epoch: 104, train time every whole data:106.55s
epoch: 104, total time:7173.23s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9145s
epoch: 105, train time every whole data:106.59s
epoch: 105, total time:7289.73s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9284s
epoch: 106, train time every whole data:106.31s
epoch: 106, total time:7405.97s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9354s
epoch: 107, train time every whole data:106.57s
epoch: 107, total time:7522.48s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7788s
epoch: 108, train time every whole data:106.41s
epoch: 108, total time:7638.67s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7808s
epoch: 109, train time every whole data:106.63s
epoch: 109, total time:7755.08s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7609s
epoch: 110, train time every whole data:106.59s
epoch: 110, total time:7871.44s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7759s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_110.params
epoch: 111, train time every whole data:106.48s
epoch: 111, total time:7987.72s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7579s
epoch: 112, train time every whole data:106.30s
epoch: 112, total time:8103.78s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7719s
epoch: 113, train time every whole data:106.61s
epoch: 113, total time:8220.17s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7579s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_113.params
epoch: 114, train time every whole data:106.49s
epoch: 114, total time:8336.46s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7818s
epoch: 115, train time every whole data:106.58s
epoch: 115, total time:8452.82s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9215s
epoch: 116, train time every whole data:106.17s
epoch: 116, total time:8568.92s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9284s
epoch: 117, train time every whole data:106.43s
epoch: 117, total time:8685.28s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9035s
epoch: 118, train time every whole data:106.54s
epoch: 118, total time:8801.72s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9185s
epoch: 119, train time every whole data:106.64s
epoch: 119, total time:8918.28s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9015s
epoch: 120, train time every whole data:106.36s
epoch: 120, total time:9034.55s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9125s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_120.params
epoch: 121, train time every whole data:106.45s
epoch: 121, total time:9150.96s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9374s
epoch: 122, train time every whole data:106.61s
epoch: 122, total time:9267.50s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7350s
epoch: 123, train time every whole data:106.54s
epoch: 123, total time:9383.79s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7320s
epoch: 124, train time every whole data:106.56s
epoch: 124, total time:9500.08s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7639s
save parameters to file: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_124.params
epoch: 125, train time every whole data:106.21s
epoch: 125, total time:9616.09s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7739s
epoch: 126, train time every whole data:106.50s
epoch: 126, total time:9732.37s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7918s
epoch: 127, train time every whole data:106.37s
epoch: 127, total time:9848.53s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7669s
epoch: 128, train time every whole data:106.56s
epoch: 128, total time:9964.86s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7559s
epoch: 129, train time every whole data:106.49s
epoch: 129, total time:10081.12s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7469s
epoch: 130, train time every whole data:106.54s
epoch: 130, total time:10197.40s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7639s
epoch: 131, train time every whole data:106.68s
epoch: 131, total time:10313.85s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9215s
epoch: 132, train time every whole data:106.40s
epoch: 132, total time:10430.17s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9175s
epoch: 133, train time every whole data:106.46s
epoch: 133, total time:10546.55s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9175s
epoch: 134, train time every whole data:106.48s
epoch: 134, total time:10662.94s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9185s
epoch: 135, train time every whole data:106.43s
epoch: 135, total time:10779.30s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9394s
epoch: 136, train time every whole data:106.51s
epoch: 136, total time:10895.75s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.9105s
epoch: 137, train time every whole data:106.53s
epoch: 137, total time:11012.19s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7569s
epoch: 138, train time every whole data:106.70s
epoch: 138, total time:11128.65s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7559s
epoch: 139, train time every whole data:106.25s
epoch: 139, total time:11244.65s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7539s
epoch: 140, train time every whole data:106.62s
epoch: 140, total time:11361.03s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7639s
epoch: 141, train time every whole data:106.67s
epoch: 141, total time:11477.46s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7699s
epoch: 142, train time every whole data:106.60s
epoch: 142, total time:11593.83s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7390s
epoch: 143, train time every whole data:106.65s
epoch: 143, total time:11710.22s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7559s
epoch: 144, train time every whole data:106.67s
epoch: 144, total time:11826.65s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7419s
epoch: 145, train time every whole data:106.53s
epoch: 145, total time:11942.92s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.7509s
epoch: 146, train time every whole data:106.38s
epoch: 146, total time:12059.06s
validation batch 1 / 5, loss: 0.01
validation cost time: 9.8806s
epoch: 147, train time every whole data:82.63s
epoch: 147, total time:12151.57s
validation batch 1 / 5, loss: 0.01
validation cost time: 7.7592s
epoch: 148, train time every whole data:68.62s
epoch: 148, total time:12227.95s
validation batch 1 / 5, loss: 0.01
validation cost time: 7.8959s
epoch: 149, train time every whole data:68.57s
epoch: 149, total time:12304.42s
validation batch 1 / 5, loss: 0.01
validation cost time: 7.7433s
best epoch: 124
apply the best val model on the test data set ...
load weight from: ./experiments\HZME_INFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_124.params
predicting testing set batch 1 / 5, time: 1.83s
test time on whole data:7.75s
input: (1073, 80, 24, 1)
prediction: (1073, 80, 12, 1)
data_target_tensor: (1073, 80, 12)
current epoch: 124, predict 0 points
MAE: 9.86
RMSE: 15.72
MAPE: 22.14
current epoch: 124, predict 1 points
MAE: 10.07
RMSE: 16.33
MAPE: 22.30
current epoch: 124, predict 2 points
MAE: 10.08
RMSE: 16.37
MAPE: 22.17
current epoch: 124, predict 3 points
MAE: 10.13
RMSE: 16.52
MAPE: 22.41
current epoch: 124, predict 4 points
MAE: 10.18
RMSE: 16.70
MAPE: 22.42
current epoch: 124, predict 5 points
MAE: 10.26
RMSE: 16.88
MAPE: 22.70
current epoch: 124, predict 6 points
MAE: 10.33
RMSE: 16.96
MAPE: 22.66
current epoch: 124, predict 7 points
MAE: 10.41
RMSE: 17.14
MAPE: 22.88
current epoch: 124, predict 8 points
MAE: 10.44
RMSE: 17.31
MAPE: 22.96
current epoch: 124, predict 9 points
MAE: 10.48
RMSE: 17.41
MAPE: 23.03
current epoch: 124, predict 10 points
MAE: 10.60
RMSE: 17.73
MAPE: 23.37
current epoch: 124, predict 11 points
MAE: 10.81
RMSE: 18.18
MAPE: 23.99
all MAE: 10.30
all RMSE: 16.95
all MAPE: 22.75
[9.859732, 15.718141820832534, 22.143542766571045, 10.065653, 16.328324983823165, 22.295856475830078, 10.081297, 16.374557372655623, 22.16741591691971, 10.132583, 16.51951785660961, 22.41472899913788, 10.182363, 16.696332444277807, 22.419023513793945, 10.2597475, 16.877139259656648, 22.695262730121613, 10.327724, 16.961856010654373, 22.661155462265015, 10.41073, 17.144818275536544, 22.882845997810364, 10.4437475, 17.309949115433767, 22.961145639419556, 10.481003, 17.405553944229087, 23.028865456581116, 10.596861, 17.728959628852575, 23.373787105083466, 10.8098135, 18.18076265218911, 23.989126086235046, 10.30427, 16.949417257278437, 22.745533287525177]

Process finished with exit code 0
