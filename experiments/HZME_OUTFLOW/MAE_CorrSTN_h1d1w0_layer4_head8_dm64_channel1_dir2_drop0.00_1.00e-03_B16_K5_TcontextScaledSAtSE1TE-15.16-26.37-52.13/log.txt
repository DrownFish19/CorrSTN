C:\Users\developer\anaconda3\envs\pytorch1.7-python3.8\python.exe C:/Users/developer/Documents/GitHub/Research/CorrSTN/train_ASTGNN.py --config configurations/HZME_OUTFLOW_rdw-011.conf
Wed Nov  3 18:43:05 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 472.12       Driver Version: 472.12       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:17:00.0  On |                  N/A |
|  0%   31C    P8    23W / 350W |   1656MiB / 24576MiB |     35%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1500    C+G   Insufficient Permissions        N/A      |
|    0   N/A  N/A      2920    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |
|    0   N/A  N/A      3908    C+G   ...tracted\WechatBrowser.exe    N/A      |
|    0   N/A  N/A      4488    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A     10016    C+G   ...m Files\Papers\Papers.exe    N/A      |
|    0   N/A  N/A     10044    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A     10088    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A     10280    C+G   ...lPanel\SystemSettings.exe    N/A      |
|    0   N/A  N/A     11296    C+G   ...8\extracted\WeChatApp.exe    N/A      |
|    0   N/A  N/A     12152    C+G   ...1\jbr\bin\jcef_helper.exe    N/A      |
|    0   N/A  N/A     12320    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |
|    0   N/A  N/A     12616    C+G   ...perience\NVIDIA Share.exe    N/A      |
|    0   N/A  N/A     13660    C+G   ...nputApp\TextInputHost.exe    N/A      |
|    0   N/A  N/A     15604    C+G   ...ropbox\Client\Dropbox.exe    N/A      |
|    0   N/A  N/A     16956    C+G   ...nruo_x64\TianruoOCR64.exe    N/A      |
|    0   N/A  N/A     17644    C+G   ...b3d8bbwe\WinStore.App.exe    N/A      |
|    0   N/A  N/A     20532    C+G   ...kyb3d8bbwe\Calculator.exe    N/A      |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: configurations/HZME_OUTFLOW_rdw-011.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 5
folder_dir: MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE
load file: data/HZME_OUTFLOW\HZME_OUTFLOW_r1_d1_w0.npz
ori length: 4313 , percent: 1.0 , scale: 4313
train: torch.Size([3218, 80, 1, 24]) torch.Size([3218, 80, 12]) torch.Size([3218, 80, 12])
val: torch.Size([1073, 80, 1, 24]) torch.Size([1073, 80, 12]) torch.Size([1073, 80, 12])
test: torch.Size([1073, 80, 1, 24]) torch.Size([1073, 80, 12]) torch.Size([1073, 80, 12])
TemporalPositionalEncoding max_len: 288
w_index: []
d_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
h_index: [276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 288, 64])
src_embed.2.embedding.weight 	 torch.Size([80, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([80, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 469845
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 3, loss: 1.77
validation cost time: 9.7559s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_0.params
epoch: 0, train time every whole data:30.74s
epoch: 0, total time:40.54s
validation batch 1 / 3, loss: 0.05
validation cost time: 7.9487s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_1.params
epoch: 1, train time every whole data:30.51s
epoch: 1, total time:79.03s
validation batch 1 / 3, loss: 0.07
validation cost time: 7.7014s
epoch: 2, train time every whole data:30.52s
epoch: 2, total time:117.25s
validation batch 1 / 3, loss: 0.04
validation cost time: 7.6904s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_3.params
epoch: 3, train time every whole data:30.45s
epoch: 3, total time:155.42s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.2579s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_4.params
epoch: 4, train time every whole data:30.80s
epoch: 4, total time:194.51s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8879s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_5.params
epoch: 5, train time every whole data:30.89s
epoch: 5, total time:233.32s
validation batch 1 / 3, loss: 0.07
validation cost time: 7.8231s
epoch: 6, train time every whole data:31.81s
epoch: 6, total time:272.96s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8490s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_7.params
epoch: 7, train time every whole data:30.31s
epoch: 7, total time:311.15s
validation batch 1 / 3, loss: 0.04
validation cost time: 7.7852s
epoch: 8, train time every whole data:32.59s
epoch: 8, total time:351.52s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.9617s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_9.params
epoch: 9, train time every whole data:33.83s
epoch: 9, total time:393.35s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.9617s
epoch: 10, train time every whole data:32.18s
epoch: 10, total time:433.49s
validation batch 1 / 3, loss: 0.06
validation cost time: 7.7533s
epoch: 11, train time every whole data:34.25s
epoch: 11, total time:475.50s
validation batch 1 / 3, loss: 0.04
validation cost time: 7.7652s
epoch: 12, train time every whole data:34.48s
epoch: 12, total time:517.74s
validation batch 1 / 3, loss: 0.14
validation cost time: 7.7563s
epoch: 13, train time every whole data:34.10s
epoch: 13, total time:559.61s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7416s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_14.params
epoch: 14, train time every whole data:33.29s
epoch: 14, total time:600.66s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7313s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_15.params
epoch: 15, train time every whole data:34.39s
epoch: 15, total time:642.82s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7373s
epoch: 16, train time every whole data:34.00s
epoch: 16, total time:684.56s
validation batch 1 / 3, loss: 0.08
validation cost time: 7.7433s
epoch: 17, train time every whole data:34.82s
epoch: 17, total time:727.13s
validation batch 1 / 3, loss: 0.05
validation cost time: 7.7961s
epoch: 18, train time every whole data:33.69s
epoch: 18, total time:768.62s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7852s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_19.params
epoch: 19, train time every whole data:34.85s
epoch: 19, total time:811.29s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9487s
epoch: 20, train time every whole data:34.47s
epoch: 20, total time:853.72s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.3926s
epoch: 21, train time every whole data:34.07s
epoch: 21, total time:896.18s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8630s
epoch: 22, train time every whole data:33.66s
epoch: 22, total time:937.70s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9288s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_23.params
epoch: 23, train time every whole data:34.77s
epoch: 23, total time:980.43s
validation batch 1 / 3, loss: 0.07
validation cost time: 8.1303s
epoch: 24, train time every whole data:32.80s
epoch: 24, total time:1021.36s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8520s
epoch: 25, train time every whole data:30.80s
epoch: 25, total time:1060.02s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.9657s
epoch: 26, train time every whole data:30.71s
epoch: 26, total time:1098.70s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8301s
epoch: 27, train time every whole data:30.60s
epoch: 27, total time:1137.13s
validation batch 1 / 3, loss: 0.04
validation cost time: 7.9797s
epoch: 28, train time every whole data:30.70s
epoch: 28, total time:1175.81s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7772s
epoch: 29, train time every whole data:30.65s
epoch: 29, total time:1214.24s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.0215s
epoch: 30, train time every whole data:30.24s
epoch: 30, total time:1252.51s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.0714s
epoch: 31, train time every whole data:30.93s
epoch: 31, total time:1291.51s
validation batch 1 / 3, loss: 0.05
validation cost time: 8.8186s
epoch: 32, train time every whole data:31.04s
epoch: 32, total time:1331.37s
validation batch 1 / 3, loss: 0.03
validation cost time: 8.0325s
epoch: 33, train time every whole data:30.78s
epoch: 33, total time:1370.18s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8101s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_34.params
epoch: 34, train time every whole data:30.17s
epoch: 34, total time:1408.18s
validation batch 1 / 3, loss: 0.04
validation cost time: 8.0983s
epoch: 35, train time every whole data:30.74s
epoch: 35, total time:1447.03s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.0884s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_36.params
epoch: 36, train time every whole data:30.27s
epoch: 36, total time:1485.42s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8829s
epoch: 37, train time every whole data:30.58s
epoch: 37, total time:1523.89s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7583s
epoch: 38, train time every whole data:30.07s
epoch: 38, total time:1561.72s
validation batch 1 / 3, loss: 0.04
validation cost time: 7.7443s
epoch: 39, train time every whole data:30.22s
epoch: 39, total time:1599.69s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7373s
epoch: 40, train time every whole data:30.49s
epoch: 40, total time:1637.91s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7952s
epoch: 41, train time every whole data:30.41s
epoch: 41, total time:1676.12s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7233s
epoch: 42, train time every whole data:30.72s
epoch: 42, total time:1714.56s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7403s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_43.params
epoch: 43, train time every whole data:30.61s
epoch: 43, total time:1752.94s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8301s
epoch: 44, train time every whole data:30.60s
epoch: 44, total time:1791.38s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8959s
epoch: 45, train time every whole data:30.36s
epoch: 45, total time:1829.64s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7353s
epoch: 46, train time every whole data:30.40s
epoch: 46, total time:1867.77s
validation batch 1 / 3, loss: 0.04
validation cost time: 7.6994s
epoch: 47, train time every whole data:30.36s
epoch: 47, total time:1905.83s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.0006s
epoch: 48, train time every whole data:30.05s
epoch: 48, total time:1943.88s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7094s
epoch: 49, train time every whole data:30.33s
epoch: 49, total time:1981.92s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7054s
epoch: 50, train time every whole data:30.41s
epoch: 50, total time:2020.04s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7463s
epoch: 51, train time every whole data:30.15s
epoch: 51, total time:2057.94s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7473s
epoch: 52, train time every whole data:30.22s
epoch: 52, total time:2095.91s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8331s
epoch: 53, train time every whole data:30.17s
epoch: 53, total time:2133.91s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9428s
epoch: 54, train time every whole data:30.09s
epoch: 54, total time:2171.96s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7463s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_55.params
epoch: 55, train time every whole data:30.33s
epoch: 55, total time:2210.06s
validation batch 1 / 3, loss: 0.04
validation cost time: 7.7044s
epoch: 56, train time every whole data:30.54s
epoch: 56, total time:2248.31s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.9467s
epoch: 57, train time every whole data:30.60s
epoch: 57, total time:2286.86s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8081s
epoch: 58, train time every whole data:30.35s
epoch: 58, total time:2325.03s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8350s
epoch: 59, train time every whole data:30.31s
epoch: 59, total time:2363.17s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9477s
epoch: 60, train time every whole data:30.27s
epoch: 60, total time:2401.39s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7383s
epoch: 61, train time every whole data:30.86s
epoch: 61, total time:2439.99s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.3566s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_62.params
epoch: 62, train time every whole data:31.11s
epoch: 62, total time:2479.48s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.0575s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_63.params
epoch: 63, train time every whole data:30.21s
epoch: 63, total time:2517.79s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9309s
epoch: 64, train time every whole data:30.60s
epoch: 64, total time:2556.31s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7473s
epoch: 65, train time every whole data:30.17s
epoch: 65, total time:2594.23s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8101s
epoch: 66, train time every whole data:30.38s
epoch: 66, total time:2632.42s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8869s
epoch: 67, train time every whole data:30.89s
epoch: 67, total time:2671.20s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.0425s
epoch: 68, train time every whole data:30.69s
epoch: 68, total time:2709.94s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8091s
epoch: 69, train time every whole data:30.42s
epoch: 69, total time:2748.17s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9029s
epoch: 70, train time every whole data:30.16s
epoch: 70, total time:2786.23s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9318s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_71.params
epoch: 71, train time every whole data:30.55s
epoch: 71, total time:2824.74s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.8919s
epoch: 72, train time every whole data:30.47s
epoch: 72, total time:2863.11s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8440s
epoch: 73, train time every whole data:30.30s
epoch: 73, total time:2901.26s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8759s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_74.params
epoch: 74, train time every whole data:30.47s
epoch: 74, total time:2939.63s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8091s
epoch: 75, train time every whole data:30.39s
epoch: 75, total time:2977.84s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7243s
epoch: 76, train time every whole data:30.26s
epoch: 76, total time:3015.82s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7104s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_77.params
epoch: 77, train time every whole data:30.34s
epoch: 77, total time:3053.90s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7004s
epoch: 78, train time every whole data:30.33s
epoch: 78, total time:3091.93s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7154s
epoch: 79, train time every whole data:30.13s
epoch: 79, total time:3129.78s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7403s
epoch: 80, train time every whole data:30.03s
epoch: 80, total time:3167.55s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9338s
epoch: 81, train time every whole data:30.28s
epoch: 81, total time:3205.77s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7084s
epoch: 82, train time every whole data:30.19s
epoch: 82, total time:3243.68s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7253s
epoch: 83, train time every whole data:30.49s
epoch: 83, total time:3281.89s
validation batch 1 / 3, loss: 0.03
validation cost time: 7.7273s
epoch: 84, train time every whole data:30.18s
epoch: 84, total time:3319.80s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7363s
epoch: 85, train time every whole data:30.54s
epoch: 85, total time:3358.08s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7204s
epoch: 86, train time every whole data:30.45s
epoch: 86, total time:3396.25s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7722s
epoch: 87, train time every whole data:30.52s
epoch: 87, total time:3434.55s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7194s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_88.params
epoch: 88, train time every whole data:30.08s
epoch: 88, total time:3472.38s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7573s
epoch: 89, train time every whole data:30.03s
epoch: 89, total time:3510.17s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7463s
epoch: 90, train time every whole data:30.10s
epoch: 90, total time:3548.02s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7233s
epoch: 91, train time every whole data:30.53s
epoch: 91, total time:3586.28s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7493s
epoch: 92, train time every whole data:30.46s
epoch: 92, total time:3624.49s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7483s
epoch: 93, train time every whole data:30.61s
epoch: 93, total time:3662.85s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7104s
epoch: 94, train time every whole data:30.31s
epoch: 94, total time:3700.87s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7263s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_95.params
epoch: 95, train time every whole data:30.41s
epoch: 95, total time:3739.03s
validation batch 1 / 3, loss: 0.04
validation cost time: 7.7333s
epoch: 96, train time every whole data:30.45s
epoch: 96, total time:3777.21s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9019s
epoch: 97, train time every whole data:30.46s
epoch: 97, total time:3815.58s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7154s
epoch: 98, train time every whole data:30.42s
epoch: 98, total time:3853.71s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.0225s
epoch: 99, train time every whole data:30.10s
epoch: 99, total time:3891.84s
best epoch: 95
apply the best val model on the test data set ...
load weight from: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_95.params
predicting testing set batch 1 / 3, time: 3.68s
test time on whole data:7.80s
input: (1073, 80, 24, 1)
prediction: (1073, 80, 12, 1)
data_target_tensor: (1073, 80, 12)
current epoch: 95, predict 0 points
MAE: 13.98
RMSE: 23.29
MAPE: 43.67
current epoch: 95, predict 1 points
MAE: 14.75
RMSE: 24.90
MAPE: 52.72
current epoch: 95, predict 2 points
MAE: 14.80
RMSE: 24.98
MAPE: 58.36
current epoch: 95, predict 3 points
MAE: 15.03
RMSE: 25.60
MAPE: 52.33
current epoch: 95, predict 4 points
MAE: 15.73
RMSE: 27.07
MAPE: 60.18
current epoch: 95, predict 5 points
MAE: 15.92
RMSE: 27.55
MAPE: 61.58
current epoch: 95, predict 6 points
MAE: 15.90
RMSE: 27.52
MAPE: 57.32
current epoch: 95, predict 7 points
MAE: 16.05
RMSE: 27.64
MAPE: 59.52
current epoch: 95, predict 8 points
MAE: 16.17
RMSE: 27.62
MAPE: 61.92
current epoch: 95, predict 9 points
MAE: 16.20
RMSE: 27.58
MAPE: 59.83
current epoch: 95, predict 10 points
MAE: 16.40
RMSE: 27.97
MAPE: 60.56
current epoch: 95, predict 11 points
MAE: 16.84
RMSE: 28.84
MAPE: 62.16
all MAE: 15.65
all RMSE: 26.76
all MAPE: 57.48
[13.9837675, 23.29158747005021, 43.66917610168457, 14.753232, 24.90140960257697, 52.72030234336853, 14.79837, 24.982551283893, 58.36461782455444, 15.034231, 25.596897700209688, 52.33260989189148, 15.726221, 27.073103662020934, 60.17725467681885, 15.917895, 27.54686946079071, 61.582934856414795, 15.8989105, 27.517533694801415, 57.324016094207764, 16.04686, 27.635499365767078, 59.5165491104126, 16.166067, 27.623087683033035, 61.92100644111633, 16.195091, 27.5767328791694, 59.832996129989624, 16.403662, 27.96578851290336, 60.5584979057312, 16.842276, 28.840067817513017, 62.16028928756714, 15.647217, 26.75813298165742, 57.47638940811157]
fine tune the model ...
epoch: 100, train time every whole data:63.97s
epoch: 100, total time:3963.84s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8400s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_100.params
epoch: 101, train time every whole data:64.20s
epoch: 101, total time:4035.90s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7433s
epoch: 102, train time every whole data:63.90s
epoch: 102, total time:4107.55s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8949s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_102.params
epoch: 103, train time every whole data:64.13s
epoch: 103, total time:4179.60s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7862s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_103.params
epoch: 104, train time every whole data:64.00s
epoch: 104, total time:4251.41s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9108s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_104.params
epoch: 105, train time every whole data:64.05s
epoch: 105, total time:4323.39s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8340s
epoch: 106, train time every whole data:64.15s
epoch: 106, total time:4395.38s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7144s
epoch: 107, train time every whole data:64.39s
epoch: 107, total time:4467.49s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9557s
epoch: 108, train time every whole data:64.70s
epoch: 108, total time:4540.14s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9118s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_108.params
epoch: 109, train time every whole data:64.62s
epoch: 109, total time:4612.70s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8909s
epoch: 110, train time every whole data:64.38s
epoch: 110, total time:4684.97s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8729s
epoch: 111, train time every whole data:64.53s
epoch: 111, total time:4757.38s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8999s
epoch: 112, train time every whole data:64.65s
epoch: 112, total time:4829.93s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8370s
epoch: 113, train time every whole data:63.38s
epoch: 113, total time:4901.15s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7902s
epoch: 114, train time every whole data:63.78s
epoch: 114, total time:4972.72s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7433s
epoch: 115, train time every whole data:65.70s
epoch: 115, total time:5046.16s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9318s
epoch: 116, train time every whole data:65.22s
epoch: 116, total time:5119.32s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.0754s
epoch: 117, train time every whole data:67.93s
epoch: 117, total time:5195.33s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7523s
epoch: 118, train time every whole data:65.84s
epoch: 118, total time:5268.93s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7592s
epoch: 119, train time every whole data:64.69s
epoch: 119, total time:5341.38s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8400s
epoch: 120, train time every whole data:64.66s
epoch: 120, total time:5413.88s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7223s
epoch: 121, train time every whole data:63.79s
epoch: 121, total time:5485.40s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8460s
epoch: 122, train time every whole data:64.88s
epoch: 122, total time:5558.13s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7154s
epoch: 123, train time every whole data:64.56s
epoch: 123, total time:5630.40s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7722s
epoch: 124, train time every whole data:64.18s
epoch: 124, total time:5702.36s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7054s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_124.params
epoch: 125, train time every whole data:63.65s
epoch: 125, total time:5773.75s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7313s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_125.params
epoch: 126, train time every whole data:66.15s
epoch: 126, total time:5847.66s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.5820s
epoch: 127, train time every whole data:65.23s
epoch: 127, total time:5921.47s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.1881s
epoch: 128, train time every whole data:65.78s
epoch: 128, total time:5995.44s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8271s
epoch: 129, train time every whole data:64.35s
epoch: 129, total time:6067.63s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7932s
epoch: 130, train time every whole data:64.03s
epoch: 130, total time:6139.46s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.9258s
epoch: 131, train time every whole data:65.27s
epoch: 131, total time:6212.65s
validation batch 1 / 3, loss: 0.02
validation cost time: 8.7396s
epoch: 132, train time every whole data:65.01s
epoch: 132, total time:6286.40s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.8652s
epoch: 133, train time every whole data:68.62s
epoch: 133, total time:6362.88s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7513s
epoch: 134, train time every whole data:67.85s
epoch: 134, total time:6438.49s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7213s
epoch: 135, train time every whole data:68.59s
epoch: 135, total time:6514.80s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7174s
epoch: 136, train time every whole data:68.36s
epoch: 136, total time:6590.88s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7403s
epoch: 137, train time every whole data:68.28s
epoch: 137, total time:6666.91s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7094s
epoch: 138, train time every whole data:68.36s
epoch: 138, total time:6742.99s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7014s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_138.params
epoch: 139, train time every whole data:68.34s
epoch: 139, total time:6819.07s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7104s
epoch: 140, train time every whole data:68.34s
epoch: 140, total time:6895.13s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7164s
save parameters to file: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_140.params
epoch: 141, train time every whole data:67.64s
epoch: 141, total time:6970.51s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7144s
epoch: 142, train time every whole data:68.30s
epoch: 142, total time:7046.53s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7114s
epoch: 143, train time every whole data:68.29s
epoch: 143, total time:7122.54s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7313s
epoch: 144, train time every whole data:67.84s
epoch: 144, total time:7198.11s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7134s
epoch: 145, train time every whole data:67.81s
epoch: 145, total time:7273.64s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7104s
epoch: 146, train time every whole data:68.23s
epoch: 146, total time:7349.58s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7144s
epoch: 147, train time every whole data:68.14s
epoch: 147, total time:7425.44s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7074s
epoch: 148, train time every whole data:68.04s
epoch: 148, total time:7501.20s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7134s
epoch: 149, train time every whole data:68.52s
epoch: 149, total time:7577.43s
validation batch 1 / 3, loss: 0.02
validation cost time: 7.7144s
best epoch: 140
apply the best val model on the test data set ...
load weight from: ./experiments\HZME_OUTFLOW\MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE\epoch_140.params
predicting testing set batch 1 / 3, time: 3.66s
test time on whole data:7.73s
input: (1073, 80, 24, 1)
prediction: (1073, 80, 12, 1)
data_target_tensor: (1073, 80, 12)
current epoch: 140, predict 0 points
MAE: 13.71
RMSE: 23.00
MAPE: 40.04
current epoch: 140, predict 1 points
MAE: 14.58
RMSE: 24.93
MAPE: 47.54
current epoch: 140, predict 2 points
MAE: 14.56
RMSE: 25.11
MAPE: 49.87
current epoch: 140, predict 3 points
MAE: 14.75
RMSE: 25.48
MAPE: 46.37
current epoch: 140, predict 4 points
MAE: 15.18
RMSE: 26.37
MAPE: 52.97
current epoch: 140, predict 5 points
MAE: 15.38
RMSE: 26.92
MAPE: 55.97
current epoch: 140, predict 6 points
MAE: 15.39
RMSE: 26.96
MAPE: 52.20
current epoch: 140, predict 7 points
MAE: 15.47
RMSE: 27.05
MAPE: 54.91
current epoch: 140, predict 8 points
MAE: 15.54
RMSE: 27.18
MAPE: 55.90
current epoch: 140, predict 9 points
MAE: 15.50
RMSE: 27.03
MAPE: 54.52
current epoch: 140, predict 10 points
MAE: 15.70
RMSE: 27.44
MAPE: 57.70
current epoch: 140, predict 11 points
MAE: 16.10
RMSE: 28.49
MAPE: 58.16
all MAE: 15.16
all RMSE: 26.37
all MAPE: 52.13
[13.708307, 22.998467435777787, 40.03831148147583, 14.579242, 24.927852437553412, 47.539326548576355, 14.564435, 25.106629293329206, 49.8734325170517, 14.751386, 25.482015812780833, 46.36790156364441, 15.1822815, 26.374243268913187, 52.96590328216553, 15.384262, 26.92239303223306, 55.96756935119629, 15.386823, 26.964208545411985, 52.201467752456665, 15.46727, 27.046975420451936, 54.914963245391846, 15.544731, 27.17818626387254, 55.90384602546692, 15.501318, 27.030457448641386, 54.520368576049805, 15.699194, 27.440304971229136, 57.69638419151306, 16.097227, 28.494119673312092, 58.1550657749176, 15.155538, 26.367929387710983, 52.133846282958984]

Process finished with exit code 0
