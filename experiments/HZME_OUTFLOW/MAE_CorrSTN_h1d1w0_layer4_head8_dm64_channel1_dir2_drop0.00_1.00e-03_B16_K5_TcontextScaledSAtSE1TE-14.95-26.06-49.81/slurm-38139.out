Wed Nov  3 22:31:41 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:5A:00.0 Off |                    0 |
| N/A   39C    P0    40W / 250W |  17086MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |
| N/A   27C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:62:00.0 Off |                    0 |
| N/A   55C    P0    79W / 250W |   3654MiB / 32480MiB |     61%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:66:00.0 Off |                    0 |
| N/A   56C    P0    84W / 250W |   3655MiB / 32480MiB |     60%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   46C    P0    66W / 250W |   3598MiB / 32480MiB |     51%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B9:00.0 Off |                    0 |
| N/A   55C    P0    71W / 250W |   3358MiB / 32480MiB |     52%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:BD:00.0 Off |                    0 |
| N/A   30C    P0    24W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:C1:00.0 Off |                    0 |
| N/A   27C    P0    23W / 250W |     11MiB / 32480MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     10414      C   python                                     17075MiB |
|    2      6126      C   python                                      3643MiB |
|    3      6462      C   python                                      3643MiB |
|    4      6280      C   python                                      3587MiB |
|    5      7159      C   python                                      3347MiB |
+-----------------------------------------------------------------------------+
CUDA: True cuda:0
Read configuration file: /data/home/u18112042/CorrSTN/configurations/HZME_OUTFLOW_rdw-011.conf
total training epoch, fine tune epoch: 100 , 50
batch_size: 16
attention_top_k: 5
folder_dir: MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE
load file: /data/home/u18112042/CorrSTN/data/HZME_OUTFLOW/HZME_OUTFLOW_r1_d1_w0.npz
ori length: 4313 , percent: 1.0 , scale: 4313
train: torch.Size([3218, 80, 1, 24]) torch.Size([3218, 80, 12]) torch.Size([3218, 80, 12])
val: torch.Size([1073, 80, 1, 24]) torch.Size([1073, 80, 12]) torch.Size([1073, 80, 12])
test: torch.Size([1073, 80, 1, 24]) torch.Size([1073, 80, 12]) torch.Size([1073, 80, 12])
TemporalPositionalEncoding max_len: 288
w_index: []
d_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
h_index: [276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]
EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
          (attn_ic): Attention_IC(
            (DAT): DynamicAttentionLayer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(80, 64)
      (gcn_smooth_layers): ModuleList(
        (0): GCN(
          (Theta): Linear(in_features=64, out_features=64, bias=False)
        )
      )
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
create params directory ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE
Net's state_dict:
encoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
encoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
encoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
encoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
encoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
encoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
encoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
encoder.norm.weight 	 torch.Size([64])
encoder.norm.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.0.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.0.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.0.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.0.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.0.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.0.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.0.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.1.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.1.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.1.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.1.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.1.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.1.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.1.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.2.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.2.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.2.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.2.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.2.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.2.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.2.sublayer.2.norm.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.self_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.0.bias 	 torch.Size([64])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.self_attn.conv1Ds_aware_temporal_context.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.0.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.0.bias 	 torch.Size([64])
decoder.layers.3.src_attn.linears.1.weight 	 torch.Size([64, 64])
decoder.layers.3.src_attn.linears.1.bias 	 torch.Size([64])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.query_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.weight 	 torch.Size([64, 64, 1, 3])
decoder.layers.3.src_attn.key_conv1Ds_aware_temporal_context.bias 	 torch.Size([64])
decoder.layers.3.feed_forward_gcn.gcn.alpha 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.beta 	 torch.Size([1])
decoder.layers.3.feed_forward_gcn.gcn.Theta.weight 	 torch.Size([64, 64])
decoder.layers.3.sublayer.0.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.0.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.1.norm.bias 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.weight 	 torch.Size([64])
decoder.layers.3.sublayer.2.norm.bias 	 torch.Size([64])
decoder.norm.weight 	 torch.Size([64])
decoder.norm.bias 	 torch.Size([64])
src_embed.0.weight 	 torch.Size([64, 1])
src_embed.0.bias 	 torch.Size([64])
src_embed.1.pe 	 torch.Size([1, 1, 288, 64])
src_embed.2.embedding.weight 	 torch.Size([80, 64])
src_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
src_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
trg_embed.0.weight 	 torch.Size([64, 1])
trg_embed.0.bias 	 torch.Size([64])
trg_embed.1.pe 	 torch.Size([1, 1, 12, 64])
trg_embed.2.embedding.weight 	 torch.Size([80, 64])
trg_embed.2.gcn_smooth_layers.0.alpha 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.beta 	 torch.Size([1])
trg_embed.2.gcn_smooth_layers.0.Theta.weight 	 torch.Size([64, 64])
prediction_generator.weight 	 torch.Size([1, 64])
prediction_generator.bias 	 torch.Size([1])
Net's total params: 469845
Optimizer's state_dict:
state 	 {}
param_groups 	 [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177]}]
validation batch 1 / 3, loss: 0.10
validation cost time: 14.9109s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_0.params
epoch: 0, train time every whole data:36.92s
epoch: 0, total time:51.84s
validation batch 1 / 3, loss: 0.06
validation cost time: 14.2436s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_1.params
epoch: 1, train time every whole data:41.81s
epoch: 1, total time:107.91s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2392s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_2.params
epoch: 2, train time every whole data:36.84s
epoch: 2, total time:159.00s
validation batch 1 / 3, loss: 0.06
validation cost time: 14.2394s
epoch: 3, train time every whole data:41.79s
epoch: 3, total time:215.04s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2370s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_4.params
epoch: 4, train time every whole data:36.88s
epoch: 4, total time:266.17s
validation batch 1 / 3, loss: 0.07
validation cost time: 14.2353s
epoch: 5, train time every whole data:41.40s
epoch: 5, total time:321.80s
validation batch 1 / 3, loss: 0.06
validation cost time: 14.6166s
epoch: 6, train time every whole data:36.92s
epoch: 6, total time:373.33s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2380s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_7.params
epoch: 7, train time every whole data:36.83s
epoch: 7, total time:424.42s
validation batch 1 / 3, loss: 0.05
validation cost time: 19.0187s
epoch: 8, train time every whole data:36.90s
epoch: 8, total time:480.34s
validation batch 1 / 3, loss: 0.06
validation cost time: 14.2422s
epoch: 9, train time every whole data:36.87s
epoch: 9, total time:531.46s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2429s
epoch: 10, train time every whole data:41.74s
epoch: 10, total time:587.45s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2431s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_11.params
epoch: 11, train time every whole data:36.84s
epoch: 11, total time:638.55s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2376s
epoch: 12, train time every whole data:41.79s
epoch: 12, total time:694.58s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2417s
epoch: 13, train time every whole data:36.83s
epoch: 13, total time:745.66s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2455s
epoch: 14, train time every whole data:41.78s
epoch: 14, total time:801.69s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2487s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_15.params
epoch: 15, train time every whole data:36.84s
epoch: 15, total time:852.79s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2449s
epoch: 16, train time every whole data:39.43s
epoch: 16, total time:906.47s
validation batch 1 / 3, loss: 0.03
validation cost time: 16.5457s
epoch: 17, train time every whole data:36.91s
epoch: 17, total time:959.92s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2752s
epoch: 18, train time every whole data:36.89s
epoch: 18, total time:1011.09s
validation batch 1 / 3, loss: 0.03
validation cost time: 17.1644s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_19.params
epoch: 19, train time every whole data:38.77s
epoch: 19, total time:1067.04s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2373s
epoch: 20, train time every whole data:36.84s
epoch: 20, total time:1118.12s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2517s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_21.params
epoch: 21, train time every whole data:41.79s
epoch: 21, total time:1174.18s
validation batch 1 / 3, loss: 0.05
validation cost time: 14.2364s
epoch: 22, train time every whole data:36.83s
epoch: 22, total time:1225.24s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2495s
epoch: 23, train time every whole data:41.82s
epoch: 23, total time:1281.31s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2453s
epoch: 24, train time every whole data:36.85s
epoch: 24, total time:1332.41s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2407s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_25.params
epoch: 25, train time every whole data:41.75s
epoch: 25, total time:1388.42s
validation batch 1 / 3, loss: 0.05
validation cost time: 14.2340s
epoch: 26, train time every whole data:36.83s
epoch: 26, total time:1439.49s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2416s
epoch: 27, train time every whole data:37.50s
epoch: 27, total time:1491.24s
validation batch 1 / 3, loss: 0.08
validation cost time: 18.4437s
epoch: 28, train time every whole data:36.81s
epoch: 28, total time:1546.49s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2409s
epoch: 29, train time every whole data:36.87s
epoch: 29, total time:1597.61s
validation batch 1 / 3, loss: 0.02
validation cost time: 15.1457s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_30.params
epoch: 30, train time every whole data:40.89s
epoch: 30, total time:1653.65s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2342s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_31.params
epoch: 31, train time every whole data:36.87s
epoch: 31, total time:1704.77s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2394s
epoch: 32, train time every whole data:41.75s
epoch: 32, total time:1760.76s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2453s
epoch: 33, train time every whole data:36.84s
epoch: 33, total time:1811.85s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2458s
epoch: 34, train time every whole data:41.79s
epoch: 34, total time:1867.90s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2361s
epoch: 35, train time every whole data:36.89s
epoch: 35, total time:1919.02s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2398s
epoch: 36, train time every whole data:40.91s
epoch: 36, total time:1974.17s
validation batch 1 / 3, loss: 0.03
validation cost time: 15.1106s
epoch: 37, train time every whole data:36.86s
epoch: 37, total time:2026.15s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2358s
epoch: 38, train time every whole data:36.85s
epoch: 38, total time:2077.24s
validation batch 1 / 3, loss: 0.02
validation cost time: 18.5126s
epoch: 39, train time every whole data:37.42s
epoch: 39, total time:2133.18s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2388s
epoch: 40, train time every whole data:36.90s
epoch: 40, total time:2184.31s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2251s
epoch: 41, train time every whole data:41.87s
epoch: 41, total time:2240.41s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2467s
epoch: 42, train time every whole data:36.83s
epoch: 42, total time:2291.49s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2469s
epoch: 43, train time every whole data:41.78s
epoch: 43, total time:2347.51s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2410s
epoch: 44, train time every whole data:36.84s
epoch: 44, total time:2398.59s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2357s
epoch: 45, train time every whole data:41.80s
epoch: 45, total time:2454.63s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2348s
epoch: 46, train time every whole data:36.84s
epoch: 46, total time:2505.71s
validation batch 1 / 3, loss: 0.04
validation cost time: 14.2359s
epoch: 47, train time every whole data:38.96s
epoch: 47, total time:2558.91s
validation batch 1 / 3, loss: 0.02
validation cost time: 16.9794s
epoch: 48, train time every whole data:36.82s
epoch: 48, total time:2612.72s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2465s
epoch: 49, train time every whole data:36.88s
epoch: 49, total time:2663.84s
validation batch 1 / 3, loss: 0.05
validation cost time: 16.6014s
epoch: 50, train time every whole data:39.41s
epoch: 50, total time:2719.86s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2395s
epoch: 51, train time every whole data:36.81s
epoch: 51, total time:2770.91s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2387s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_52.params
epoch: 52, train time every whole data:41.82s
epoch: 52, total time:2826.98s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2374s
epoch: 53, train time every whole data:36.86s
epoch: 53, total time:2878.08s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2360s
epoch: 54, train time every whole data:41.80s
epoch: 54, total time:2934.13s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2448s
epoch: 55, train time every whole data:36.84s
epoch: 55, total time:2985.22s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2394s
epoch: 56, train time every whole data:41.79s
epoch: 56, total time:3041.25s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2427s
epoch: 57, train time every whole data:36.87s
epoch: 57, total time:3092.36s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2360s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_58.params
epoch: 58, train time every whole data:37.07s
epoch: 58, total time:3143.67s
validation batch 1 / 3, loss: 0.02
validation cost time: 18.9006s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_59.params
epoch: 59, train time every whole data:36.92s
epoch: 59, total time:3199.51s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2422s
epoch: 60, train time every whole data:36.90s
epoch: 60, total time:3250.66s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.8736s
epoch: 61, train time every whole data:41.20s
epoch: 61, total time:3306.74s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2468s
epoch: 62, train time every whole data:36.88s
epoch: 62, total time:3357.86s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2370s
epoch: 63, train time every whole data:41.82s
epoch: 63, total time:3413.92s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2399s
epoch: 64, train time every whole data:36.85s
epoch: 64, total time:3465.01s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2569s
epoch: 65, train time every whole data:41.85s
epoch: 65, total time:3521.13s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2364s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_66.params
epoch: 66, train time every whole data:36.85s
epoch: 66, total time:3572.23s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2398s
epoch: 67, train time every whole data:40.81s
epoch: 67, total time:3627.28s
validation batch 1 / 3, loss: 0.03
validation cost time: 15.2276s
epoch: 68, train time every whole data:36.86s
epoch: 68, total time:3679.37s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2465s
epoch: 69, train time every whole data:36.88s
epoch: 69, total time:3730.50s
validation batch 1 / 3, loss: 0.02
validation cost time: 18.4186s
epoch: 70, train time every whole data:37.57s
epoch: 70, total time:3786.50s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2447s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_71.params
epoch: 71, train time every whole data:36.88s
epoch: 71, total time:3837.64s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2332s
epoch: 72, train time every whole data:41.81s
epoch: 72, total time:3893.68s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2439s
epoch: 73, train time every whole data:36.88s
epoch: 73, total time:3944.82s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2459s
epoch: 74, train time every whole data:41.80s
epoch: 74, total time:4000.86s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2464s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_75.params
epoch: 75, train time every whole data:36.91s
epoch: 75, total time:4052.03s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2352s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_76.params
epoch: 76, train time every whole data:41.75s
epoch: 76, total time:4108.03s
validation batch 1 / 3, loss: 0.03
validation cost time: 14.2367s
epoch: 77, train time every whole data:36.85s
epoch: 77, total time:4159.13s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2397s
epoch: 78, train time every whole data:39.09s
epoch: 78, total time:4212.46s
validation batch 1 / 3, loss: 0.02
validation cost time: 16.9024s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_79.params
epoch: 79, train time every whole data:36.80s
epoch: 79, total time:4266.17s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2386s
epoch: 80, train time every whole data:36.84s
epoch: 80, total time:4317.25s
validation batch 1 / 3, loss: 0.03
validation cost time: 16.5451s
epoch: 81, train time every whole data:39.44s
epoch: 81, total time:4373.24s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2439s
epoch: 82, train time every whole data:36.87s
epoch: 82, total time:4424.35s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2434s
epoch: 83, train time every whole data:41.76s
epoch: 83, total time:4480.36s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2405s
epoch: 84, train time every whole data:36.84s
epoch: 84, total time:4531.44s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2409s
epoch: 85, train time every whole data:41.80s
epoch: 85, total time:4587.48s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2510s
epoch: 86, train time every whole data:36.81s
epoch: 86, total time:4638.55s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2397s
epoch: 87, train time every whole data:41.82s
epoch: 87, total time:4694.61s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2448s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_88.params
epoch: 88, train time every whole data:36.85s
epoch: 88, total time:4745.73s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2345s
epoch: 89, train time every whole data:36.82s
epoch: 89, total time:4796.78s
validation batch 1 / 3, loss: 0.02
validation cost time: 19.0831s
epoch: 90, train time every whole data:36.88s
epoch: 90, total time:4852.74s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2432s
epoch: 91, train time every whole data:36.82s
epoch: 91, total time:4903.80s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.3366s
epoch: 92, train time every whole data:41.67s
epoch: 92, total time:4959.82s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2370s
epoch: 93, train time every whole data:36.84s
epoch: 93, total time:5010.90s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2451s
epoch: 94, train time every whole data:41.79s
epoch: 94, total time:5066.94s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2441s
epoch: 95, train time every whole data:36.81s
epoch: 95, total time:5118.00s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2417s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_96.params
epoch: 96, train time every whole data:41.79s
epoch: 96, total time:5174.05s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2433s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_97.params
epoch: 97, train time every whole data:36.86s
epoch: 97, total time:5225.16s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2398s
epoch: 98, train time every whole data:40.06s
epoch: 98, total time:5279.46s
validation batch 1 / 3, loss: 0.02
validation cost time: 15.9492s
epoch: 99, train time every whole data:36.84s
epoch: 99, total time:5332.26s
best epoch: 97
apply the best val model on the test data set ...
load weight from: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_97.params
predicting testing set batch 1 / 3, time: 6.76s
test time on whole data:14.25s
input: (1073, 80, 24, 1)
prediction: (1073, 80, 12, 1)
data_target_tensor: (1073, 80, 12)
current epoch: 97, predict 0 points
MAE: 13.97
RMSE: 23.24
MAPE: 40.05
current epoch: 97, predict 1 points
MAE: 14.35
RMSE: 23.89
MAPE: 46.97
current epoch: 97, predict 2 points
MAE: 14.02
RMSE: 23.76
MAPE: 40.38
current epoch: 97, predict 3 points
MAE: 14.62
RMSE: 24.95
MAPE: 41.51
current epoch: 97, predict 4 points
MAE: 14.94
RMSE: 25.65
MAPE: 47.97
current epoch: 97, predict 5 points
MAE: 14.86
RMSE: 25.49
MAPE: 46.18
current epoch: 97, predict 6 points
MAE: 15.25
RMSE: 26.37
MAPE: 45.93
current epoch: 97, predict 7 points
MAE: 15.43
RMSE: 26.84
MAPE: 51.00
current epoch: 97, predict 8 points
MAE: 15.52
RMSE: 26.95
MAPE: 49.46
current epoch: 97, predict 9 points
MAE: 15.70
RMSE: 27.39
MAPE: 50.39
current epoch: 97, predict 10 points
MAE: 15.94
RMSE: 28.17
MAPE: 54.04
current epoch: 97, predict 11 points
MAE: 16.19
RMSE: 28.71
MAPE: 55.45
all MAE: 15.07
all RMSE: 26.00
all MAPE: 47.40
[13.966537, 23.235649027780298, 40.05330204963684, 14.347101, 23.888052203684115, 46.973249316215515, 14.024219, 23.755766158582425, 40.38111865520477, 14.617691, 24.947883910686567, 41.51477813720703, 14.940447, 25.64836253986914, 47.96602427959442, 14.862358, 25.488386314438632, 46.176061034202576, 15.24614, 26.367035879193693, 45.93115746974945, 15.432864, 26.838958845567436, 51.003122329711914, 15.521795, 26.95114009901471, 49.46040213108063, 15.695432, 27.38608218709996, 50.39030909538269, 15.941177, 28.167003140518318, 54.04491424560547, 16.194601, 28.708560700939557, 55.44946789741516, 15.065867, 26.003586741784297, 47.398895025253296]
fine tune the model ... 
epoch: 100, train time every whole data:89.05s
epoch: 100, total time:5435.73s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2526s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_100.params
epoch: 101, train time every whole data:88.78s
epoch: 101, total time:5538.78s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2351s
epoch: 102, train time every whole data:88.74s
epoch: 102, total time:5641.75s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2325s
epoch: 103, train time every whole data:88.74s
epoch: 103, total time:5744.72s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2339s
epoch: 104, train time every whole data:88.70s
epoch: 104, total time:5847.66s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2394s
epoch: 105, train time every whole data:88.74s
epoch: 105, total time:5950.64s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2422s
epoch: 106, train time every whole data:88.75s
epoch: 106, total time:6053.64s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2411s
epoch: 107, train time every whole data:88.70s
epoch: 107, total time:6156.58s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2293s
epoch: 108, train time every whole data:86.57s
epoch: 108, total time:6257.39s
validation batch 1 / 3, loss: 0.02
validation cost time: 16.2461s
epoch: 109, train time every whole data:83.58s
epoch: 109, total time:6357.22s
validation batch 1 / 3, loss: 0.02
validation cost time: 19.1344s
epoch: 110, train time every whole data:83.54s
epoch: 110, total time:6459.89s
validation batch 1 / 3, loss: 0.02
validation cost time: 15.9331s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_110.params
epoch: 111, train time every whole data:86.85s
epoch: 111, total time:6562.68s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2367s
epoch: 112, train time every whole data:88.64s
epoch: 112, total time:6665.56s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2457s
epoch: 113, train time every whole data:88.61s
epoch: 113, total time:6768.42s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2423s
epoch: 114, train time every whole data:88.63s
epoch: 114, total time:6871.29s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2403s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_114.params
epoch: 115, train time every whole data:88.69s
epoch: 115, total time:6974.23s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2386s
epoch: 116, train time every whole data:88.63s
epoch: 116, total time:7077.10s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2390s
epoch: 117, train time every whole data:88.63s
epoch: 117, total time:7179.98s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2286s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_117.params
epoch: 118, train time every whole data:88.65s
epoch: 118, total time:7282.88s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2422s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_118.params
epoch: 119, train time every whole data:88.66s
epoch: 119, total time:7385.79s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2418s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_119.params
epoch: 120, train time every whole data:88.65s
epoch: 120, total time:7488.69s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2385s
epoch: 121, train time every whole data:88.69s
epoch: 121, total time:7591.62s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2404s
epoch: 122, train time every whole data:88.63s
epoch: 122, total time:7694.49s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2438s
epoch: 123, train time every whole data:88.62s
epoch: 123, total time:7797.36s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2476s
epoch: 124, train time every whole data:88.59s
epoch: 124, total time:7900.20s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2508s
epoch: 125, train time every whole data:88.66s
epoch: 125, total time:8003.11s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2329s
epoch: 126, train time every whole data:87.49s
epoch: 126, total time:8104.83s
validation batch 1 / 3, loss: 0.02
validation cost time: 15.2825s
epoch: 127, train time every whole data:83.95s
epoch: 127, total time:8204.06s
validation batch 1 / 3, loss: 0.02
validation cost time: 18.6188s
epoch: 128, train time every whole data:83.53s
epoch: 128, total time:8306.22s
validation batch 1 / 3, loss: 0.02
validation cost time: 16.8340s
epoch: 129, train time every whole data:85.87s
epoch: 129, total time:8408.93s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2436s
epoch: 130, train time every whole data:88.64s
epoch: 130, total time:8511.81s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2352s
epoch: 131, train time every whole data:88.64s
epoch: 131, total time:8614.69s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2329s
epoch: 132, train time every whole data:88.64s
epoch: 132, total time:8717.57s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2453s
epoch: 133, train time every whole data:88.66s
epoch: 133, total time:8820.48s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2405s
epoch: 134, train time every whole data:88.67s
epoch: 134, total time:8923.39s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2380s
epoch: 135, train time every whole data:88.66s
epoch: 135, total time:9026.29s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2362s
epoch: 136, train time every whole data:88.64s
epoch: 136, total time:9129.17s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2442s
epoch: 137, train time every whole data:88.67s
epoch: 137, total time:9232.08s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2402s
epoch: 138, train time every whole data:88.65s
epoch: 138, total time:9334.98s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2405s
epoch: 139, train time every whole data:88.64s
epoch: 139, total time:9437.86s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2348s
epoch: 140, train time every whole data:88.66s
epoch: 140, total time:9540.76s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2451s
epoch: 141, train time every whole data:88.63s
epoch: 141, total time:9643.63s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2432s
epoch: 142, train time every whole data:88.61s
epoch: 142, total time:9746.49s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2357s
epoch: 143, train time every whole data:88.67s
epoch: 143, total time:9849.40s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2404s
epoch: 144, train time every whole data:88.33s
epoch: 144, total time:9951.97s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.4541s
epoch: 145, train time every whole data:85.02s
epoch: 145, total time:10051.45s
validation batch 1 / 3, loss: 0.02
validation cost time: 17.6258s
epoch: 146, train time every whole data:83.52s
epoch: 146, total time:10152.60s
validation batch 1 / 3, loss: 0.02
validation cost time: 17.8304s
epoch: 147, train time every whole data:84.85s
epoch: 147, total time:10255.28s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.6680s
epoch: 148, train time every whole data:88.18s
epoch: 148, total time:10358.13s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2327s
save parameters to file: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_148.params
epoch: 149, train time every whole data:88.67s
epoch: 149, total time:10461.05s
validation batch 1 / 3, loss: 0.02
validation cost time: 14.2369s
best epoch: 148
apply the best val model on the test data set ...
load weight from: ../experiments/HZME_OUTFLOW/MAE_ASTGNN_h1d1w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03_B16_K5_TcontextScaledSAtSE1TE/epoch_148.params
predicting testing set batch 1 / 3, time: 6.77s
test time on whole data:15.05s
input: (1073, 80, 24, 1)
prediction: (1073, 80, 12, 1)
data_target_tensor: (1073, 80, 12)
current epoch: 148, predict 0 points
MAE: 13.72
RMSE: 23.01
MAPE: 39.20
current epoch: 148, predict 1 points
MAE: 14.31
RMSE: 24.33
MAPE: 47.06
current epoch: 148, predict 2 points
MAE: 14.12
RMSE: 24.13
MAPE: 45.38
current epoch: 148, predict 3 points
MAE: 14.45
RMSE: 24.79
MAPE: 44.65
current epoch: 148, predict 4 points
MAE: 14.88
RMSE: 25.88
MAPE: 49.65
current epoch: 148, predict 5 points
MAE: 14.95
RMSE: 26.17
MAPE: 50.55
current epoch: 148, predict 6 points
MAE: 15.09
RMSE: 26.36
MAPE: 49.52
current epoch: 148, predict 7 points
MAE: 15.31
RMSE: 26.92
MAPE: 53.03
current epoch: 148, predict 8 points
MAE: 15.43
RMSE: 27.25
MAPE: 52.04
current epoch: 148, predict 9 points
MAE: 15.45
RMSE: 27.20
MAPE: 53.50
current epoch: 148, predict 10 points
MAE: 15.63
RMSE: 27.64
MAPE: 55.19
current epoch: 148, predict 11 points
MAE: 16.04
RMSE: 28.46
MAPE: 58.54
all MAE: 14.95
all RMSE: 26.06
all MAPE: 49.81
[13.722148, 23.012805856705903, 39.197203516960144, 14.308179, 24.332431027479174, 47.063738107681274, 14.118793, 24.132672132563616, 45.38373649120331, 14.453959, 24.79379559092733, 44.65323984622955, 14.878189, 25.8806193418846, 49.65418875217438, 14.947769, 26.172993213052052, 50.54908990859985, 15.088371, 26.360146050518907, 49.52211380004883, 15.312384, 26.92046708085806, 53.03047299385071, 15.429965, 27.254954036491238, 52.038997411727905, 15.447916, 27.202501249310696, 53.50059270858765, 15.63468, 27.637947466060478, 55.18673062324524, 16.041225, 28.459958508073232, 58.537834882736206, 14.948632, 26.060522210253072, 49.81071949005127]
